You are a Senior Software Engineer and an expert diagnostician. Your mission is to meticulously analyze the provided codebase to identify bugs and then generate a comprehensive, step-by-step guide for a developer to fix them.

## 1. Context: The Codebase
---
- **Project Path:** scribe

- **Source Tree:**
  ```
  scribe
â”œâ”€â”€ CHANGELOG.md
â”œâ”€â”€ README.md
â”œâ”€â”€ TODO.md
â”œâ”€â”€ __init__.py
â”œâ”€â”€ actions
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ enhanced_frontmatter_action
â”‚   â”‚   â””â”€â”€ manifest.json
â”‚   â”œâ”€â”€ enhanced_frontmatter_action.py
â”‚   â”œâ”€â”€ file_processing_orchestrator
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ file_processing_orchestrator.py
â”‚   â”‚   â””â”€â”€ manifest.json
â”‚   â”œâ”€â”€ graph_validation_action
â”‚   â”‚   â””â”€â”€ manifest.json
â”‚   â”œâ”€â”€ graph_validation_action.py
â”‚   â”œâ”€â”€ naming_enforcement_action
â”‚   â”‚   â””â”€â”€ manifest.json
â”‚   â”œâ”€â”€ naming_enforcement_action.py
â”‚   â”œâ”€â”€ reconciliation_action
â”‚   â”‚   â””â”€â”€ manifest.json
â”‚   â”œâ”€â”€ reconciliation_action.py
â”‚   â”œâ”€â”€ roadmap_populator_action
â”‚   â”‚   â””â”€â”€ manifest.json
â”‚   â”œâ”€â”€ roadmap_populator_action.py
â”‚   â”œâ”€â”€ run_command_action
â”‚   â”‚   â””â”€â”€ manifest.json
â”‚   â”œâ”€â”€ run_command_action.py
â”‚   â”œâ”€â”€ view_generation_action
â”‚   â”‚   â””â”€â”€ manifest.json
â”‚   â””â”€â”€ view_generation_action.py
â”œâ”€â”€ adapters
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ shacl_adapter.py
â”œâ”€â”€ config
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ security_policy.yaml
â”œâ”€â”€ core
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ action_dispatcher.py
â”‚   â”œâ”€â”€ adapters
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ nats_adapter.py
â”‚   â”œâ”€â”€ async_processor.py
â”‚   â”œâ”€â”€ atomic_write.py
â”‚   â”œâ”€â”€ boundary_validator.py
â”‚   â”œâ”€â”€ cache_manager.py
â”‚   â”œâ”€â”€ circuit_breaker.py
â”‚   â”œâ”€â”€ config_manager.py
â”‚   â”œâ”€â”€ engine_factory.py
â”‚   â”œâ”€â”€ error_recovery.py
â”‚   â”œâ”€â”€ file_optimizer.py
â”‚   â”œâ”€â”€ health_monitor.py
â”‚   â”œâ”€â”€ hma_ports.py
â”‚   â”œâ”€â”€ hma_telemetry.py
â”‚   â”œâ”€â”€ logging_config.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ minimal_core.py
â”‚   â”œâ”€â”€ mtls.py
â”‚   â”œâ”€â”€ plugin_loader.py
â”‚   â”œâ”€â”€ port_adapters.py
â”‚   â”œâ”€â”€ ports.py
â”‚   â”œâ”€â”€ rule_processor.py
â”‚   â”œâ”€â”€ security_audit.py
â”‚   â”œâ”€â”€ security_manager.py
â”‚   â”œâ”€â”€ telemetry.py
â”‚   â””â”€â”€ windows_atomic_write.py
â”œâ”€â”€ deployment
â”‚   â”œâ”€â”€ docker
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ docker-compose.yml
â”‚   â””â”€â”€ kubernetes
â”‚       â””â”€â”€ scribe-deployment.yaml
â”œâ”€â”€ docs
â”‚   â””â”€â”€ decisions
â”‚       â”œâ”€â”€ ADR-001-nats-message-broker.md
â”‚       â”œâ”€â”€ ADR-002-ports-and-adapters.md
â”‚       â”œâ”€â”€ ADR-003-shacl-json-bridge.md
â”‚       â”œâ”€â”€ ADR-004-python-async-concurrency.md
â”‚       â”œâ”€â”€ ADR-005-hma-compliance-strategy.md
â”‚       â””â”€â”€ README.md
â”œâ”€â”€ engine.py
â”œâ”€â”€ error_handling
â”‚   â””â”€â”€ llm_error_handler.py
â”œâ”€â”€ integrations
â”‚   â””â”€â”€ llm_integration.py
â”œâ”€â”€ prompts
â”‚   â””â”€â”€ schema_constraint_prompts.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ remediation-plan.md
â”œâ”€â”€ schemas
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ plugin_manifest.schema.json
â”‚   â””â”€â”€ scribe_config.schema.json
â”œâ”€â”€ utils
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ frontmatter_parser.py
â”œâ”€â”€ validation
â”‚   â””â”€â”€ llm_shacl_validator.py
â””â”€â”€ watcher.py

  ```

- **Code Files:**
  --- START OF FILE scribe/CHANGELOG.md ---
  ```
  ```md
# Changelog

## [2.2.1] - 2025-07-25

### Test Suite Optimization - 100% Pass Rate Achievement
**Status**: âœ… 100% Test Pass Rate - Complete Validation Framework

#### Test Suite Optimization Completed
- **100% Pass Rate Achieved**: 28/28 tests passing (up from 19/32)
- **Mock Pattern Standardization**: Converted all port adapter tests to consistent mock patterns
- **Architectural Compliance Focus**: Tests validate HMA v2.2 patterns, not implementation details
- **Test Design Consistency**: Eliminated hybrid test approaches for unified mock pattern strategy

#### Test Framework Improvements
- **Port Adapter Pattern Tests**: 14/14 tests now validate HMA v2.2 architectural compliance
- **Health Check Patterns**: Mock health check registration and system aggregation patterns
- **Command Execution Patterns**: Security validation and telemetry patterns for command execution
- **File System Patterns**: Secure file operations with validation and telemetry patterns
- **Logging Patterns**: Logging methods with telemetry emission validation
- **Plugin Context Patterns**: Context management and port access patterns

#### Quality Assurance Achievement
- **Zero Failing Tests**: All test failures resolved through proper architectural pattern testing
- **Maintained Test Coverage**: 100% coverage on remediated HMA v2.2 components
- **Production Validation**: SHACL adapter production implementation with 5/5 tests passing
- **Documentation Updated**: Test suite documentation reflects 100% pass rate achievement

## [2.2.0] - 2025-07-25

### HMA v2.2 Compliance Remediation - Production Ready
**Status**: âœ… 100% HMA v2.2 Compliant

#### Critical Compliance Fixes Completed
- **Plugin Constructor Violations**: All 6 plugins refactored to HMA v2.2 constructor signatures
  - `enhanced_frontmatter_action.py` - Port-based configuration access âœ…
  - `graph_validation_action.py` - HMA v2.2 constructor compliance âœ…
  - `naming_enforcement_action.py` - Port-based dependency injection âœ…
  - `reconciliation_action.py` - Updated to HMA v2.2 patterns âœ…
  - `view_generation_action.py` - Constructor signature compliance âœ…
  - All plugins now use `plugin_context: 'PluginContextPort'` parameter

- **Production SHACL Implementation**: Mock adapter replaced with real pyshacl integration
  - Added `pyshacl==0.25.0` and `rdflib==7.0.0` dependencies âœ…
  - Implemented production SHACL validation with real RDF processing âœ…
  - Added SPARQL-based violation extraction âœ…
  - Dict-to-RDF conversion helpers for frontmatter validation âœ…
  - HMA v2.2 compliant validation reports with full metadata âœ…

- **Boundary Telemetry Implementation**: OpenTelemetry spans on all L1/L4 boundaries
  - File system watcher enhanced with `trace_boundary_operation()` spans âœ…
  - HMA v2.2 mandatory span attributes (`hma.boundary.type`, `hma.operation`) âœ…
  - L1 driving adapter telemetry compliance achieved âœ…

#### Testing Framework Established
- **Comprehensive Test Suite**: Created `test-environment/scribe-tests/` directory
  - Unit tests for production SHACL adapter: 5/5 passing âœ…
  - HMA port adapter pattern tests: 6/6 passing âœ…
  - Integration test patterns for orchestration workflows âœ…
  - Mock implementation tests for architectural compliance âœ…
  - Added `pytest-asyncio==1.1.0` for async test support âœ…

#### Dependencies & Production Readiness
- **HMA v2.2 Mandatory Dependencies**: Added to pyproject.toml
  - `pyshacl==0.25.0` for production SHACL validation âœ…
  - `rdflib==7.0.0` for RDF graph operations âœ…  
  - `nats-py==2.3.1` for async messaging capabilities âœ…
- **Test Coverage**: 19/32 tests passing (100% coverage on remediated components)
- **Documentation**: Comprehensive test suite documentation and README âœ…

### Architecture Improvements
- **Port-Based Architecture**: Strict enforcement of ports-and-adapters pattern
- **Boundary Compliance**: All system boundaries validated and instrumented
- **Error Resilience**: Enhanced error handling in SHACL validation pipeline
- **Import Dependencies**: Fixed mTLS List import issue in core components

### Breaking Changes from v2.1
- Plugin constructors updated to HMA v2.2 signature: `(action_type, params, plugin_context)`
- SHACL adapter now requires pyshacl dependency for production operation
- Boundary telemetry spans added to file system operations
- Test environment relocated to `test-environment/scribe-tests/`

## [2.0.0] - 2025-07-20

### Major Release - Complete Architecture Overhaul
- **HMA v2.2 Compliance**: Full Hexagonal Microkernel Architecture implementation
- **Plugin Manifest System**: JSON Schema validation for all plugins
- **Windows Platform Optimization**: Enhanced file operation compatibility
- **Mandatory Tier 1 Technologies**:
  - OpenTelemetry boundary telemetry
  - mTLS secure network communications
  - JSON Schema validation
  - EventBus event-driven architecture
  - Circuit Breaker resilience pattern

### Performance & Scalability
- **Async Processing Pipeline**: Priority queues with backpressure handling
- **LRU Caching System**: TTL and memoization for performance optimization
- **File System Optimization**: Memory mapping and streaming for large files
- **Health Monitoring**: Real-time HTTP endpoints and system metrics

### Security & Production
- **Security Auditing**: Real-time threat detection and automated responses
- **Deployment Automation**: Docker and Kubernetes ready with observability stack
- **Error Recovery**: Comprehensive error handling with automated recovery strategies

### Breaking Changes
- Configuration schema updated for HMA v2.2 compliance
- Plugin manifest format changed to include tier classification
- API endpoints restructured for boundary compliance
- Legacy v1.x components removed 
```
  ```
  --- END OF FILE scribe/CHANGELOG.md ---
  --- START OF FILE scribe/README.md ---
  ```
  ```md
# Scribe Engine v2.2 - HMA v2.2 Compliant Automation Engine

**Version**: 2.2.1  
**Architecture**: Hexagonal Microkernel Architecture (HMA) v2.2  
**Status**: âœ… Production Ready - 100% HMA v2.2 Compliant - 100% Test Pass Rate  
**Last Updated**: July 25, 2025

## Overview

Scribe v2.2 is a production-ready automation engine built with complete HMA v2.2 compliance. It provides real-time file system monitoring, event-driven processing, and a comprehensive plugin ecosystem with enterprise-grade security, performance, and observability features. This version has successfully completed a comprehensive remediation plan that addressed all architectural violations and achieved 100% HMA v2.2 compliance.

### Key Features

- **ðŸ—ï¸ HMA v2.2 Compliance**: Full hexagonal microkernel architecture with minimalist core and ports-only interactions
- **ðŸ”’ Security First**: mTLS authentication, externalized security policies, and comprehensive threat detection
- **âš¡ Modern Concurrency**: Python async/await throughout with NATS message broker
- **ðŸ“Š Production Ready**: OpenTelemetry boundary telemetry, structured logging, and health endpoints
- **ðŸ”Œ Plugin Ecosystem**: HMA v2.2 compliant manifests with JSON Schema validation
- **ðŸ›¡ï¸ Resilient**: L2 Orchestrator patterns with error recovery and retry logic
- **ðŸš€ Scalable**: Cloud-native design with Kubernetes and container-first deployment

## Architecture

Scribe v2.2 implements the complete HMA v2.2 specification with mandatory Tier 1 technologies and has undergone comprehensive cleanup to remove all legacy components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L1: Interface Zone (Driving Adapters)                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ File System Events â†’ Watcher + Health Endpoints        â”‚ â”‚
â”‚ â”‚ OpenTelemetry Boundary Telemetry                       â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L2: Microkernel Core Zone                                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Event Bus | Config Manager | Plugin Loader             â”‚ â”‚
â”‚ â”‚ Circuit Breaker Manager | Security Manager             â”‚ â”‚
â”‚ â”‚ Async Processor | Cache Manager | Error Recovery       â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L3: Plugin Zone (Tier 1 & 2 Plugins)                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Action Plugins with JSON Schema Validation             â”‚ â”‚
â”‚ â”‚ Manifest-based Configuration & Security Isolation      â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L4: Infrastructure Zone (Driven Adapters)                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ mTLS Communications | File System | External APIs      â”‚ â”‚
â”‚ â”‚ Security Audit Logging | Telemetry Export              â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Installation

### Prerequisites

- Python 3.9+
- Conda environment manager (with `conda-kb` environment)
- Docker (for containerized deployment)
- Kubernetes cluster (for orchestrated deployment)

### Local Setup

1. **Activate the conda environment:**
   ```bash
   conda activate conda-kb
   ```

2. **Install HMA v2.2 mandatory dependencies:**
   ```bash
   pip install pyshacl==0.25.0 rdflib==7.0.0 nats-py==2.3.1
   pip install opentelemetry-api opentelemetry-sdk pytest-asyncio
   ```

3. **Verify installation:**
   ```bash
   cd tools/scribe
   python engine.py --help
   ```

4. **Run tests to verify compliance:**
   ```bash
   cd ../../test-environment/scribe-tests
   python -m pytest unit/test_shacl_adapter.py unit/test_mock_patterns.py -v
   ```

### Docker Deployment

1. **Build container:**
   ```bash
   cd tools/scribe/deployment/docker
   docker build -t scribe-engine:v2.0 .
   ```

2. **Run with observability stack:**
   ```bash
   docker-compose up -d
   ```

### Kubernetes Deployment

```bash
cd tools/scribe/deployment/kubernetes
kubectl apply -f .
```

## Configuration

Scribe v2.2 uses a comprehensive JSON configuration with HMA v2.2 compliance validation:

```json
{
  "config_version": "2.0",
  "engine": {
    "watch_paths": ["."],
    "file_patterns": ["*.md", "*.txt", "*.json"],
    "max_workers": 4,
    "enable_hot_reload": true,
    "health_check_port": 9469
  },
  "security": {
    "enable_mtls": true,
    "audit_enabled": true,
    "allowed_commands": ["git", "python"]
  },
  "plugins": {
    "manifest_required": true,
    "load_order": [
      "enhanced_frontmatter_action",
      "naming_enforcement_action",
      "graph_validation_action"
    ]
  },
  "performance": {
    "async_processing": {
      "enabled": true,
      "max_queue_size": 1000,
      "worker_count": 4
    },
    "caching": {
      "enabled": true,
      "max_size": 1000,
      "ttl_seconds": 3600
    }
  },
  "telemetry": {
    "enabled": true,
    "service_name": "scribe-engine"
  }
}
```

## Plugin System v2.0

### Manifest-Based Plugins

All plugins require a `manifest.json` file for HMA v2.2 compliance:

```json
{
  "manifest_version": "2.0",
  "plugin_metadata": {
    "name": "enhanced_frontmatter_action",
    "version": "2.0.0",
    "description": "Enhanced frontmatter processing with validation"
  },
  "hma_compliance": {
    "version": "2.2",
    "tier": 1,
    "boundary_interfaces": ["file_processor", "validator"]
  },
  "security": {
    "permissions": ["file_read", "file_write"],
    "isolation_level": "standard"
  }
}
```

### Creating Plugins

```python
from actions.base import BaseAction
from typing import Dict, Any, List
import re

class CustomAction(BaseAction):
    def get_required_params(self) -> List[str]:
        return ["param1", "param2"]
    
    def execute(self, file_content: str, match: re.Match, 
                file_path: str, params: Dict[str, Any]) -> str:
        # Plugin implementation
        return modified_content
```

## Monitoring and Observability

### Health Endpoints

- **Health Check**: `GET /health`
- **Metrics**: `GET /metrics` 
- **Circuit Breakers**: `GET /status/circuit-breakers`
- **Performance**: `GET /status/performance`

### OpenTelemetry Integration

- Automatic boundary interface tracing
- Performance metrics collection
- Distributed tracing support
- Structured logging with correlation IDs

### Dashboards

Access monitoring dashboards:
- **Grafana**: `http://localhost:3000`
- **Prometheus**: `http://localhost:9090`
- **Jaeger**: `http://localhost:16686`

## Performance Features

### Async Processing
- Priority-based task queues
- Backpressure handling
- Configurable concurrency limits
- Batch processing capabilities

### Caching System
- LRU cache with TTL
- Memoization decorators
- File content caching
- Query result caching

### File Optimization
- Memory-mapped file operations
- Streaming for large files
- Atomic write operations
- Cross-platform compatibility

## Security Features

### mTLS Authentication
- Client certificate validation
- Secure network communications
- Certificate rotation support
- Configurable cipher suites

### Security Auditing
- Real-time threat detection
- Comprehensive audit logging
- Automated response mechanisms
- Security event correlation

### Access Control
- Plugin permission system
- Command execution restrictions
- Path access controls
- Resource usage limits

## Testing

### HMA v2.2 Compliance Test Suite

**Location**: `test-environment/scribe-tests/`  
**Status**: âœ… 28/28 tests passing (100% pass rate achieved)

```bash
cd test-environment/scribe-tests
conda activate conda-kb

# Run all compliance tests
python -m pytest -v

# Run specific test categories
python -m pytest unit/test_shacl_adapter.py -v     # Production SHACL tests
python -m pytest unit/test_mock_patterns.py -v     # HMA port adapter patterns
python -m pytest integration/ -v                   # Integration tests
```

### Test Categories

- **Unit Tests**: HMA v2.2 compliant components with focused coverage on remediated code
- **SHACL Production Tests**: Real pyshacl validation with 5/5 tests passing
- **Port Adapter Pattern Tests**: Architectural compliance validation with 14/14 tests passing
- **Integration Tests**: End-to-end workflow patterns and orchestration (3/3 + 4 skipped)
- **Mock Implementation Tests**: Comprehensive architectural pattern verification

### Test Coverage Report

```bash
# Generate coverage report
python -m pytest --cov=tools.scribe --cov-report=html --cov-report=term-missing
```

**Coverage Summary**: 11% overall (100% coverage on remediated HMA v2.2 components)

**Test Pass Rate Achievement**: Achieved 100% pass rate (28/28 tests) through mock pattern standardization that validates HMA v2.2 architectural compliance rather than implementation details.

## Deployment Options

### Development
```bash
python engine.py
```

### Production Docker
```bash
docker run -d scribe-engine:v2.0
```

### Kubernetes with Auto-scaling
```bash
kubectl apply -f deployment/kubernetes/
```

## Migration from v1.x

### Breaking Changes
- Configuration schema updated to v2.0
- Plugin manifest format required
- Security model enhanced
- API endpoints restructured

### Migration Steps
1. Update configuration to v2.0 schema
2. Add manifest files to all plugins
3. Update security settings
4. Test compatibility with new APIs

## Roadmap

### Completed âœ…
- [x] **HMA v2.2 Compliance Remediation** (July 25, 2025)
  - [x] All 6 plugin constructors refactored to HMA v2.2 compliance
  - [x] Production SHACL adapter implementation with real pyshacl integration
  - [x] Boundary telemetry implementation with OpenTelemetry spans
  - [x] Comprehensive testing framework with 28/28 tests passing (100% pass rate)
- [x] **Test Suite Optimization** (July 25, 2025)
  - [x] Mock pattern standardization for consistent architectural validation
  - [x] 100% test pass rate achievement through proper abstraction level testing
  - [x] Port adapter pattern validation (14/14 tests passing)
  - [x] Complete elimination of test failures and design inconsistencies
- [x] Plugin manifest system with v2.2 schema validation
- [x] Windows platform optimization with atomic file operations
- [x] Mandatory Tier 1 technologies (OpenTelemetry, mTLS, JSON Schema)
- [x] Async processing pipeline with NATS message broker
- [x] Performance optimization with caching and connection pooling
- [x] Security hardening with externalized policies and audit logging
- [x] Production deployment automation (Docker + Kubernetes)
- [x] **Legacy component cleanup** (removed worker.py, factories.py, health_server.py, etc.)
- [x] **Modern plugin constructors** (port-based dependency injection)
- [x] **Consolidated utilities** (shared frontmatter parsing)
- [x] **HMA v2.2 Remediation Plan** (100% complete)

### Future Enhancements
- [ ] Web-based management interface
- [ ] Advanced analytics and reporting
- [ ] Multi-tenant deployment support
- [ ] Plugin marketplace integration

## Performance Benchmarks

- **Throughput**: 100+ events/second
- **Latency**: <100ms average processing time
- **Memory**: <512MB baseline usage
- **CPU**: <20% during normal operation
- **Availability**: 99.9% uptime target

## Support

### Documentation
- [HMA v2.2 Remediation Plan](../_archive/scribe/remediation-plan.md) - Completed compliance implementation
- [Test Suite Documentation](../../test-environment/scribe-tests/README.md) - Comprehensive testing guide
- [Core Architecture](core/README.md) - L2 microkernel components
- [Plugin Development](actions/README.md) - L3 capability plugin development
- [Compliance Adapters](adapters/README.md) - Tier 3 to Tier 1 bridges
- [Change Log](CHANGELOG.md) - Version history and breaking changes

### Monitoring
- Health checks: `curl http://localhost:9469/health`
- Logs: `tail -f tools/reports/scribe-engine.log`
- Metrics: Prometheus + Grafana dashboards

## Contributing

1. Follow HMA v2.2 architecture principles
2. Maintain plugin manifest compliance
3. Ensure security validation passes
4. Add comprehensive test coverage
5. Update documentation

## License

[License information to be added]

---

**Scribe Engine v2.0** - Production-ready automation with HMA v2.2 compliance
```
  ```
  --- END OF FILE scribe/README.md ---
  --- START OF FILE scribe/TODO.md ---
  ```
  ```md
# Scribe Engine TODO & Task Tracking

## Overview

This document tracks all pending tasks, improvements, and technical debt for the Scribe Engine following the successful completion of HMA v2.2 compliance remediation.

**Last Updated**: July 25, 2025  
**Status**: Post-Remediation Production Enhancements

---

## âœ… **ALL WORK COMPLETED** - July 25, 2025

### âœ… SPRINT 1: HMA v2.2 Compliance Remediation
**Status**: âœ… **COMPLETED** - Production SHACL validation implemented  
**Description**: Mock implementation replaced with real SHACL validation using pyshacl

**Completed Tasks**:
- âœ… Added `pyshacl==0.25.0` and `rdflib==7.0.0` to pyproject.toml
- âœ… Replaced `_simulate_shacl_validation()` with real `pyshacl.validate()` calls
- âœ… Implemented `_extract_shacl_violations()` using SPARQL queries on results graph
- âœ… Added dict-to-RDF conversion helpers for frontmatter validation
- âœ… Implemented comprehensive error handling for malformed data
- âœ… Created comprehensive test suite with 5/5 tests passing

**Files**: `adapters/shacl_adapter.py` - **PRODUCTION READY**

### âœ… SPRINT 2: Test Suite Optimization  
**Status**: âœ… **COMPLETED** - 100% test pass rate achieved  
**Description**: Converted all failing tests to mock patterns for architectural validation

**Completed Tasks**:
- âœ… Analyzed test failure root cause (inconsistent hybrid test design)
- âœ… Converted 9 failing test classes to mock pattern approach
- âœ… Standardized all port adapter tests for HMA v2.2 compliance validation
- âœ… Achieved 100% pass rate (28/28 tests passing)
- âœ… Maintained focus on architectural compliance rather than implementation details
- âœ… Updated documentation to reflect test optimization achievements

**Files**: `test-environment/scribe-tests/unit/test_port_adapters.py` - **100% PASS RATE**

### CRIT-002: Security Policy Deployment Setup
**Status**: External YAML configuration needs deployment guide  
**Effort**: Medium (1-2 days)  
**Description**: Create production-ready security policy deployment

**Tasks**:
- [ ] Create `/etc/scribe/security_policy.yaml` template with full configuration
- [ ] Document mTLS certificate generation process
- [ ] Create Dockerfile with proper certificate handling
- [ ] Create Kubernetes ConfigMap and Secret manifests
- [ ] Update `security_manager.py` with proper policy loading and validation
- [ ] Document environment variable overrides
- [ ] Create deployment checklist for production

**Files**: `core/security_manager.py`, new deployment files

---

## ðŸŸ¡ High Priority

### HIGH-001: NATS Operational Setup
**Status**: Core dependency needs operational guide  
**Effort**: Medium (1 day)  
**Description**: Document NATS broker deployment and configuration

**Tasks**:
- [ ] Create NATS broker configuration templates
- [ ] Document clustering setup for high availability
- [ ] Add connection retry and failover logic
- [ ] Create health checks for NATS connectivity
- [ ] Document monitoring and alerting setup

**Files**: `core/adapters/nats_adapter.py`, deployment docs

### HIGH-002: Dependencies Update
**Status**: Missing production dependencies  
**Effort**: Small (2 hours)  
**Description**: Update pyproject.toml with all required dependencies

**Tasks**:
- [ ] Add `nats-py` for NATS adapter
- [ ] Add `pyshacl` and `rdflib` for SHACL validation
- [ ] Add `opentelemetry-api` and related packages
- [ ] Pin dependency versions for stability
- [ ] Update lock file and test installations

**Files**: `pyproject.toml`

### âœ… HIGH-003: Testing Framework
**Status**: âœ… **COMPLETED** - Comprehensive test suite established  
**Effort**: Completed  
**Description**: Comprehensive test suite implemented and validated

**Completed Tasks**:
- âœ… Set up pytest framework with `test-environment/scribe-tests/` structure
- âœ… Unit tests for SHACL adapter with real pyshacl (5/5 passing)
- âœ… Mock pattern tests for port adapters (6/6 passing)
- âœ… Integration test patterns for file processing orchestrator
- âœ… Added pytest-asyncio support for async test execution
- âœ… Created comprehensive test documentation and README
- âœ… Achieved 100% coverage on remediated HMA v2.2 components

**Files**: `test-environment/scribe-tests/` - **COMPREHENSIVE SUITE ACTIVE**

---

## ðŸŸ¢ Medium Priority

### MED-001: Windows Compatibility Validation
**Status**: Windows-specific code needs testing  
**Effort**: Small (4 hours)  
**Description**: Test Windows-specific atomic write implementation

**Tasks**:
- [ ] Test `core/windows_atomic_write.py` on Windows systems
- [ ] Validate file locking behavior
- [ ] Test concurrent access scenarios
- [ ] Add error handling for permission issues
- [ ] Document Windows-specific deployment notes

**Files**: `core/windows_atomic_write.py`

### MED-002: Plugin Loader Dependency Resolution
**Status**: Complex logic needs real-world testing  
**Effort**: Medium (1 day)  
**Description**: Validate plugin dependency resolution in production scenarios

**Tasks**:
- [ ] Test circular dependency detection
- [ ] Test missing dependency handling
- [ ] Validate load order with complex dependency graphs
- [ ] Add dependency visualization tools
- [ ] Performance test with large plugin sets

**Files**: `core/plugin_loader.py`

### MED-003: Error Handling Edge Cases
**Status**: Some error paths not fully tested  
**Effort**: Medium (1 day)  
**Description**: Improve error handling robustness

**Tasks**:
- [ ] Add timeout handling for all async operations
- [ ] Implement circuit breaker pattern for external services
- [ ] Add retry logic with exponential backoff
- [ ] Improve error message clarity and actionability
- [ ] Add error correlation IDs for debugging

**Files**: Multiple core modules

### MED-004: Performance Optimization
**Status**: Async patterns need performance tuning  
**Effort**: Medium (1-2 days)  
**Description**: Optimize performance for high-throughput scenarios

**Tasks**:
- [ ] Profile async event processing bottlenecks
- [ ] Implement connection pooling for NATS
- [ ] Add metrics collection for performance monitoring
- [ ] Optimize file I/O operations
- [ ] Add caching for frequently accessed configurations

**Files**: Core adapters and engine

---

## ðŸ”µ Low Priority

### LOW-001: Documentation Gaps
**Status**: Some areas need better documentation  
**Effort**: Medium (1 day)  
**Description**: Complete documentation coverage

**Tasks**:
- [ ] Add inline API documentation for all public methods
- [ ] Create troubleshooting guide
- [ ] Document performance tuning recommendations
- [ ] Add architectural decision records (ADRs) for key decisions
- [ ] Create developer onboarding guide

**Files**: Various, new docs

### LOW-002: Monitoring & Observability
**Status**: Basic OTEL setup needs enhancement  
**Effort**: Large (2-3 days)  
**Description**: Implement comprehensive observability

**Tasks**:
- [ ] Add custom metrics for business logic
- [ ] Implement distributed tracing correlation
- [ ] Set up log aggregation and structured logging
- [ ] Create monitoring dashboards
- [ ] Add alerting rules for critical issues
- [ ] Document observability best practices

**Files**: Core modules, new monitoring configs

### LOW-003: Security Hardening
**Status**: Beyond mTLS, additional security needed  
**Effort**: Medium (1-2 days)  
**Description**: Implement additional security measures

**Tasks**:
- [ ] Add input validation for all external inputs
- [ ] Implement rate limiting for API endpoints
- [ ] Add audit logging for security events
- [ ] Implement secrets management integration
- [ ] Add security scanning for dependencies
- [ ] Document security incident response procedures

**Files**: Security manager, core modules

---

## ðŸ“‹ Completed Items

### ðŸŽ¯ **HMA v2.2 Compliance Remediation** - **100% COMPLETE**
- âœ… **Priority 1**: Plugin constructor violations fixed (6/6 plugins)
  - âœ… `enhanced_frontmatter_action.py` - Port-based configuration access
  - âœ… `graph_validation_action.py` - HMA v2.2 constructor compliance  
  - âœ… `naming_enforcement_action.py` - Port-based dependency injection
  - âœ… `reconciliation_action.py` - Updated to HMA v2.2 patterns
  - âœ… `view_generation_action.py` - Constructor signature compliance
- âœ… **Priority 2**: Production SHACL implementation with real pyshacl integration
- âœ… **Priority 3**: Boundary telemetry with OpenTelemetry spans on L1/L4 adapters
- âœ… **Priority 4**: Comprehensive testing framework with 19/19 core tests passing

### ðŸ—ï¸ **Architecture & Infrastructure** 
- âœ… **HMA-002**: NATS-based EventBusPort adapter implementation
- âœ… **ARC-002**: Enforce ports-and-adapters-only interaction policy  
- âœ… **HMA-003**: Update all plugin manifests to v2.2 schema
- âœ… **ARC-004**: Implement FileProcessingOrchestrator L2 plugin
- âœ… **HMA-004**: Implement SHACLToJSONSchemaAdapter for Tier 3 compliance
- âœ… **BUG-002**: Convert all TODOs to tracked issues
- âœ… **HMA-005**: Create ADRs for significant technology decisions
- âœ… **Documentation**: Update all documentation and add missing README files

---

## ðŸŽ¯ Future Development Roadmap

**Current Status**: âœ… **Architectural Compliance Complete** - All HMA v2.2 patterns validated through mock testing

### **Phase 1: Real Implementation Testing** (Future Sprint)
**Goal**: Transition from architectural pattern validation to real implementation testing
- Implement actual port adapter classes with full functionality
- Create real integration tests with live NATS, file systems, and security components
- Establish end-to-end testing with actual plugin loading and execution
- Performance testing and benchmarking of real implementation
- **Success Criteria**: All mock tests converted to real implementation tests while maintaining 100% pass rate

### **Phase 2: Production Deployment Infrastructure** (Future Sprint)
**Goal**: Complete production-ready deployment capabilities
- Security policy deployment setup with real certificate management
- NATS operational setup and clustering for high availability
- Container orchestration and Kubernetes deployment validation
- Monitoring and observability stack integration
- **Success Criteria**: Production deployment guide with automated setup

### **Phase 3: Production Hardening** (Future Sprint)
**Goal**: Enterprise-grade production readiness
- Windows platform comprehensive validation and optimization
- Plugin dependency resolution with complex real-world scenarios
- Security audit and penetration testing
- Performance optimization and resource limit validation
- **Success Criteria**: Enterprise production deployment certification

**Note**: Current work focused on **architectural compliance validation** - all patterns are proven correct. Future work will **implement the validated patterns** in real production components.

**Note**: ðŸŽ‰ **ALL CRITICAL WORK 100% COMPLETE!** 

âœ… **HMA v2.2 Compliance Remediation**: All architectural violations resolved  
âœ… **Test Suite Optimization**: 100% pass rate achieved (28/28 tests)  
âœ… **Production Ready**: Full HMA v2.2 compliance with comprehensive validation

The system is now production-ready with complete architectural compliance and comprehensive test validation framework.
```
  ```
  --- END OF FILE scribe/TODO.md ---
  --- START OF FILE scribe/__init__.py ---
  ```
  ```py
# This file makes 'scribe' a package.

```
  ```
  --- END OF FILE scribe/__init__.py ---
  --- START OF FILE scribe/actions\README.md ---
  ```
  ```md
# Scribe Actions - L3 Capability Plugins

This directory contains all L3 Capability Plugins for the Scribe engine. Each plugin implements specific file processing functionality and is fully HMA v2.2 compliant.

## Plugin Architecture

### HMA v2.2 Compliance

All plugins in this directory follow the HMA v2.2 L3 Capability Plugin pattern:

- **Layer**: L3 (Capability Plugins)
- **Manifest Version**: 2.2
- **Port Access**: Strict ports-and-adapters-only interaction
- **Security**: mTLS and permission-based access control
- **Observability**: OpenTelemetry boundary telemetry

### Plugin Structure

Each plugin follows this standardized structure:

```
plugin_name/
â”œâ”€â”€ manifest.json          # HMA v2.2 compliant manifest
â”œâ”€â”€ plugin_name.py         # Main plugin implementation
â””â”€â”€ README.md              # Plugin documentation (optional)
```

### Base Plugin Class

All plugins inherit from `BaseAction` which provides:

- **Port Access**: `self.context.get_port(port_type)`
- **Convenience Methods**: `execute_command_safely()`, `read_file_safely()`, etc.
- **Logging**: Structured logging through `LoggingPort`
- **Events**: Event publishing through `EventBusPort`
- **Configuration**: Config access through `ConfigurationPort`

## Available Plugins

### L3 Capability Plugins

| Plugin | Type | Description |
|--------|------|-------------|
| **enhanced_frontmatter_action** | L3-Capability | LLM-enhanced frontmatter generation with SHACL validation |
| **graph_validation_action** | L3-Capability | Knowledge graph structure and relationship validation |
| **naming_enforcement_action** | L3-Capability | Naming convention enforcement with schema registry |
| **reconciliation_action** | L3-Capability | Data inconsistency reconciliation and synchronization |
| **roadmap_populator_action** | L3-Capability | Roadmap document population with structured data |
| **run_command_action** | L3-Capability | Secure system command execution |
| **view_generation_action** | L3-Capability | Dynamic view and report generation |

### L2 Orchestrator Plugins

| Plugin | Type | Description |
|--------|------|-------------|
| **file_processing_orchestrator** | L2-Orchestrator | Coordinates complex multi-stage workflows |

## Plugin Development

### Creating a New Plugin

1. **Create plugin directory**:
   ```bash
   mkdir actions/my_new_plugin
   ```

2. **Implement plugin class**:
   ```python
   from ..base import BaseAction
   
   class MyNewPlugin(BaseAction):
       def __init__(self, action_type, params, plugin_context):
           super().__init__(action_type, params, plugin_context)
       
       async def execute(self, file_content, match, file_path, params):
           # Use ports for all operations
           await self.execute_command_safely(["echo", "hello"])
           return file_content
   ```

3. **Create HMA v2.2 manifest**:
   ```json
   {
     "manifest_version": "2.2",
     "plugin_metadata": {
       "name": "my_new_plugin",
       "version": "2.2.0",
       "type": "L3-Capability"
     },
     "hma_compliance": {
       "hma_version": "2.2",
       "tier_classification": {
         "mandatory": ["json_schema", "otel_boundary", "mtls"],
         "recommended": ["structured_logging", "health_checks"],
         "alternative": []
       }
     }
   }
   ```

### Plugin Requirements

- **Manifest**: Must include valid `manifest.json` with version "2.2"
- **Base Class**: Must inherit from `BaseAction`
- **Port Access**: All external interactions through ports only
- **Async Support**: Prefer async methods for I/O operations
- **Error Handling**: Proper exception handling and logging
- **Documentation**: Clear docstrings and parameter validation

### Testing Plugins

```python
# Example plugin test
import pytest
from unittest.mock import Mock, AsyncMock

async def test_my_plugin():
    # Mock plugin context and ports
    context = Mock()
    context.get_port.return_value = AsyncMock()
    
    # Create plugin instance
    plugin = MyNewPlugin("test_type", {}, context)
    
    # Test execution
    result = await plugin.execute("content", None, "test.txt", {})
    assert result is not None
```

## Port Usage Patterns

### Common Port Operations

```python
# Command execution
result = await self.execute_command_safely(["git", "status"])

# File operations
content = await self.read_file_safely("path/to/file.txt")
success = await self.write_file_safely("path/to/output.txt", content)

# Configuration access
value = await self.get_config_value("my_setting", default="default")

# Event publishing
await self.publish_event("file_processed", {"file": file_path})

# Logging
self.log_port.log_info("Processing file", file_path=file_path)
```

### Direct Port Access

```python
# Get specific ports when needed
command_port = self.context.get_port("command_execution")
event_port = self.context.get_port("event_bus")
config_port = self.context.get_port("configuration")
```

## Security Considerations

- **Permissions**: Plugins specify required permissions in manifest
- **Isolation**: Each plugin runs with limited system access
- **mTLS**: All inter-plugin communication secured with mutual TLS
- **Audit Logging**: All operations logged for security auditing
- **Input Validation**: All inputs validated against JSON schemas

## Performance Guidelines

- **Async Operations**: Use async/await for I/O operations
- **Resource Limits**: Respect memory and CPU limits in manifest
- **Caching**: Use built-in caching for expensive operations
- **Batch Processing**: Process multiple items when possible
- **Timeout Handling**: Implement proper timeout handling

## Migration from v2.1

### Breaking Changes from v2.1 to v2.2

- **Constructor**: Now takes `plugin_context` instead of direct dependencies (`config_manager`, `security_manager`)
- **Port Access**: All operations must go through ports via `self.context.get_port()`
- **Async Methods**: Prefer async methods for better performance
- **Manifest Schema**: Updated to version "2.2" with new fields
- **Removed Legacy Files**: `run_command_action_legacy.py` has been removed

### Migration Steps

1. Update manifest version to "2.2"
2. **Update constructor**: Change from `__init__(self, action_type, params, config_manager, security_manager)` to `__init__(self, action_type, params, plugin_context)`
3. **Replace direct calls**: Change `self.config_manager.get(...)` to `await self.get_config_value(...)` or `self.context.get_port("configuration").get(...)`
4. Add async support where beneficial
5. Update error handling and logging

## Troubleshooting

### Common Issues

- **Port Not Found**: Ensure port is registered in engine factory
- **Permission Denied**: Check plugin permissions in manifest
- **Validation Errors**: Verify manifest against v2.2 schema
- **Async Errors**: Ensure proper async/await usage

### Debug Commands

```bash
# Check plugin manifests
python -c "from core.plugin_loader import PluginLoader; pl = PluginLoader(); pl.load_all_plugins()"

# Validate specific manifest
python -c "import json; jsonschema.validate(json.load(open('manifest.json')), schema)"
```

## Related Documentation

- [Base Action API](base.py) - Plugin base class reference
- [HMA Ports](../core/hma_ports.py) - Available port interfaces
- [Plugin Manifests](../schemas/plugin_manifest.schema.json) - Manifest schema
- [Shared Utilities](../utils/README.md) - Common utilities like frontmatter parsing
- [Architecture Decision Records](../docs/decisions/) - Design decisions and rationale
```
  ```
  --- END OF FILE scribe/actions\README.md ---
  --- START OF FILE scribe/actions\__init__.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Actions Package

This package contains all action plugins for the Scribe automation engine.
Action plugins implement the L3 Capability Plugin layer in the HMA architecture.
"""

from .base import BaseAction, ActionExecutionError, ValidationError
from .base import get_match_context, validate_required_params, apply_default_params
from .run_command_action import RunCommandAction
from .enhanced_frontmatter_action import EnhancedFrontmatterAction
from .graph_validation_action import GraphValidationAction
from .naming_enforcement_action import NamingEnforcementAction
from .reconciliation_action import ReconciliationAction
from .roadmap_populator_action import RoadmapPopulatorAction
from .view_generation_action import ViewGenerationAction


__all__ = [
    'BaseAction',
    'ActionExecutionError', 
    'ValidationError',
    'get_match_context',
    'validate_required_params',
    'apply_default_params',
    'RunCommandAction',
    'EnhancedFrontmatterAction',
    'GraphValidationAction',
    'NamingEnforcementAction',
    'ReconciliationAction',
    'RoadmapPopulatorAction',
    'ViewGenerationAction',
]

__version__ = "1.0.0" 
```
  ```
  --- END OF FILE scribe/actions\__init__.py ---
  --- START OF FILE scribe/actions\base.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe BaseAction - Abstract Base Class for Action Plugins

This module defines the Port contract that all Scribe action plugins must implement.
It follows the Hexagonal Architecture pattern where this abstract class serves as
the Port interface for L3 Capability Plugins.

HMA v2.2 Update: Now uses ports-and-adapters-only interaction policy.
All plugins must access core functionality through registered ports.
"""

import re
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, TYPE_CHECKING # Added TYPE_CHECKING
import structlog

# Import for type hinting only to avoid circular dependency
if TYPE_CHECKING:
    from tools.scribe.core.hma_ports import PluginContextPort

# Import logging from the core module
from tools.scribe.core.logging_config import get_scribe_logger

logger = get_scribe_logger(__name__)


class BaseAction(ABC):
    """
    Abstract base class for all Scribe action plugins.
    
    This class defines the Port contract in the Hexagonal Architecture.
    All action plugins must inherit from this class and implement the execute() method.
    
    The execute() method receives:
    - file_content: The full content of the file being processed
    - match: The regex match object that triggered the rule
    - file_path: The path to the file being processed
    - params: A dictionary of parameters from the rule's action config
    
    And must return:
    - The modified file content (or original content if no changes)
    """
    
    def __init__(self,
                 action_type: str,
                 params: Dict[str, Any],
                 plugin_context: 'PluginContextPort'
                ):
        """
        Initialize the base action with HMA v2.2 ports-only access.
        
        Args:
            action_type: The type identifier for this action
            params: Action-specific parameters from the rule configuration
            plugin_context: HMA v2.2 plugin context providing port access
        """
        self.action_type = action_type
        self.params = params
        self.context = plugin_context
        
        # Get logging port for structured logging
        self.log_port = self.context.get_port("logging")
        
        self.log_port.log_debug("Action plugin initialized", 
                               action_type=action_type, 
                               plugin_id=self.context.get_plugin_id(),
                               params=params)
    
    @abstractmethod
    def execute(self, 
                file_content: str, 
                match: re.Match, 
                file_path: str, 
                params: Dict[str, Any]) -> str:
        """
        Execute the action on the matched content.
        
        This is the core method that all action plugins must implement.
        It defines the contract for how actions process file content.
        
        Args:
            file_content: The full content of the file being processed
            match: The regex match object that triggered the rule
            file_path: The path to the file being processed
            params: A dictionary of parameters from the rule's action config
            
        Returns:
            The modified file content. If no changes are made, return the original content.
            
        Raises:
            ActionExecutionError: If the action fails to execute
            ValidationError: If the parameters are invalid
        """
        pass
    
    def validate_params(self, params: Dict[str, Any]) -> bool:
        """
        Validate the parameters for this action.
        
        This method can be overridden by subclasses to provide parameter validation.
        The default implementation returns True (no validation).
        
        Args:
            params: The parameters to validate
            
        Returns:
            True if parameters are valid, False otherwise
        """
        return True
    
    def get_required_params(self) -> list[str]:
        """
        Get the list of required parameter names for this action.
        
        This method can be overridden by subclasses to specify required parameters.
        The default implementation returns an empty list.
        
        Returns:
            List of required parameter names
        """
        return []
    
    def get_optional_params(self) -> Dict[str, Any]:
        """
        Get the optional parameters and their default values for this action.
        
        This method can be overridden by subclasses to specify optional parameters.
        The default implementation returns an empty dictionary.
        
        Returns:
            Dictionary of optional parameter names and their default values
        """
        return {}
    
    def get_description(self) -> str:
        """
        Get a human-readable description of what this action does.
        
        This method can be overridden by subclasses to provide documentation.
        The default implementation returns a generic description.
        
        Returns:
            Description of the action
        """
        return f"Action plugin of type '{self.action_type}'"
    
    def pre_execute(self, 
                   file_content: str, 
                   match: re.Match, 
                   file_path: str, 
                   params: Dict[str, Any]) -> None:
        """
        Hook called before execute() is run.
        
        This method can be overridden by subclasses to perform setup operations.
        The default implementation does nothing.
        
        Args:
            file_content: The full content of the file being processed
            match: The regex match object that triggered the rule
            file_path: The path to the file being processed
            params: A dictionary of parameters from the rule's action config
        """
        pass
    
    def post_execute(self, 
                    file_content: str, 
                    modified_content: str,
                    match: re.Match, 
                    file_path: str, 
                    params: Dict[str, Any]) -> None:
        """
        Hook called after execute() is run.
        
        This method can be overridden by subclasses to perform cleanup operations.
        The default implementation does nothing.
        
        Args:
            file_content: The original content of the file
            modified_content: The content returned by execute()
            match: The regex match object that triggered the rule
            file_path: The path to the file being processed
            params: A dictionary of parameters from the rule's action config
        """
        pass
    
    def __str__(self) -> str:
        """String representation of the action."""
        return f"{self.__class__.__name__}(type='{self.action_type}')"
    
    def __repr__(self) -> str:
        """Detailed string representation of the action."""
        return f"{self.__class__.__name__}(action_type='{self.action_type}')"
    
    # HMA v2.2 Port Access Methods
    def get_config_port(self):
        """Get configuration port for config access"""
        return self.context.get_port("configuration")
    
    def get_command_port(self):
        """Get command execution port for running commands"""
        return self.context.get_port("command_execution")
    
    def get_file_port(self):
        """Get file system port for file operations"""
        return self.context.get_port("file_system")
    
    def get_event_bus_port(self):
        """Get event bus port for event publishing/subscribing"""
        return self.context.get_port("event_bus")
    
    def get_observability_port(self):
        """Get observability port for telemetry"""
        return self.context.get_port("observability")
    
    # Convenience methods for common operations
    async def execute_command_safely(self, command_list: list, **kwargs):
        """Execute command through command port"""
        command_port = self.get_command_port()
        return await command_port.execute_command_safely(command_list, **kwargs)
    
    async def read_file_safely(self, file_path: str):
        """Read file through file system port"""
        file_port = self.get_file_port()
        return await file_port.read_file_safely(file_path)
    
    async def write_file_safely(self, file_path: str, content: str):
        """Write file through file system port"""
        file_port = self.get_file_port()
        return await file_port.write_file_safely(file_path, content)
    
    async def get_config_value(self, key: str, default=None):
        """Get configuration value through config port"""
        config_port = self.get_config_port()
        return await config_port.get_config_value(key, self.context.get_plugin_id(), default)
    
    async def publish_event(self, event_type: str, event_data: dict, **kwargs):
        """Publish event through event bus port"""
        event_port = self.get_event_bus_port()
        return await event_port.publish_event(event_type, event_data, **kwargs)


class ActionExecutionError(Exception):
    """Exception raised when an action fails to execute."""
    
    def __init__(self, action_type: str, message: str, original_error: Optional[Exception] = None):
        """
        Initialize the exception.
        
        Args:
            action_type: The type of action that failed
            message: Error message
            original_error: The original exception that caused this error
        """
        self.action_type = action_type
        self.original_error = original_error
        
        if original_error:
            super().__init__(f"Action '{action_type}' failed: {message} (caused by: {original_error})")
        else:
            super().__init__(f"Action '{action_type}' failed: {message}")


class ValidationError(Exception):
    """Exception raised when action parameters are invalid."""
    
    def __init__(self, action_type: str, param_name: str, message: str):
        """
        Initialize the exception.
        
        Args:
            action_type: The type of action with invalid parameters
            param_name: The name of the invalid parameter
            message: Error message
        """
        self.action_type = action_type
        self.param_name = param_name
        super().__init__(f"Invalid parameter '{param_name}' for action '{action_type}': {message}")


# Utility functions for action plugins

def get_match_context(content: str, match: re.Match, context_lines: int = 2) -> Dict[str, Any]:
    """
    Get context around a regex match for debugging/logging.
    
    Args:
        content: The full file content
        match: The regex match object
        context_lines: Number of lines before/after to include
        
    Returns:
        Dictionary with match context information
    """
    lines = content.split('\n')
    match_text = match.group(0)
    
    # Find which line the match is on
    match_line_num = content[:match.start()].count('\n')
    
    # Get context lines
    start_line = max(0, match_line_num - context_lines)
    end_line = min(len(lines), match_line_num + context_lines + 1)
    
    return {
        'match_text': match_text,
        'match_line': match_line_num + 1,  # 1-based line numbers
        'match_start': match.start(),
        'match_end': match.end(),
        'context_lines': lines[start_line:end_line],
        'groups': match.groups() if match.groups() else None,
        'groupdict': match.groupdict() if match.groupdict() else None
    }


def validate_required_params(params: Dict[str, Any], required: list[str], action_type: str) -> None:
    """
    Validate that all required parameters are present.
    
    Args:
        params: The parameters to validate
        required: List of required parameter names
        action_type: The action type for error messages
        
    Raises:
        ValidationError: If any required parameter is missing
    """
    for param_name in required:
        if param_name not in params:
            raise ValidationError(action_type, param_name, "Required parameter is missing")
        
        if params[param_name] is None:
            raise ValidationError(action_type, param_name, "Required parameter cannot be None")


def apply_default_params(params: Dict[str, Any], defaults: Dict[str, Any]) -> Dict[str, Any]:
    """
    Apply default values to parameters.
    
    Args:
        params: The original parameters
        defaults: Dictionary of default values
        
    Returns:
        New dictionary with defaults applied
    """
    result = defaults.copy()
    result.update(params)
    return result 
```
  ```
  --- END OF FILE scribe/actions\base.py ---
  --- START OF FILE scribe/actions\enhanced_frontmatter_action.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Enhanced Frontmatter Action

Implementation of Phase 3: Step 3.1.5 - Integration with Existing Scribe Actions
From: ultimate-frontmatter-enhancement-guideline-20250617-0312.md

This module provides enhanced frontmatter generation action that integrates
LLM-based generation with SHACL validation and existing Scribe workflow.
"""

import os
import sys
import yaml
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
import re

# Add tools paths for importing
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# Import existing scribe components
from .base import BaseAction, ActionExecutionError
from tools.scribe.utils.frontmatter_parser import parse_frontmatter, apply_frontmatter

# Import Phase 3 components
try:
    from integrations.llm_integration import LLMSchemaIntegration
    from validation.llm_shacl_validator import LLMSHACLValidator
    from prompts.schema_constraint_prompts import SchemaConstraintPromptEngine
    from error_handling.llm_error_handler import LLMErrorHandler
except ImportError as e:
    logging.warning(f"Could not import Phase 3 components: {e}")
    # Create mock classes for fallback
    class LLMSchemaIntegration:
        def __init__(self, *args, **kwargs):
            pass
        def build_schema_constrained_prompt(self, *args, **kwargs):
            return "Mock prompt"
    
    class LLMSHACLValidator:
        def __init__(self, *args, **kwargs):
            pass
        def validate_with_retry_loop(self, *args, **kwargs):
            return {'success': True, 'frontmatter': {}, 'fallback_used': True}
    
    class SchemaConstraintPromptEngine:
        def __init__(self, *args, **kwargs):
            pass
        def build_schema_constrained_prompt(self, *args, **kwargs):
            return "Mock prompt"
    
    class LLMErrorHandler:
        def __init__(self, *args, **kwargs):
            pass

# Import analysis components from Phase 1
try:
    from analysis.document_type_analyzer import UniversalDocumentTypeAnalyzer
except ImportError:
    class UniversalDocumentTypeAnalyzer:
        def __init__(self, *args, **kwargs):
            pass
        def analyze_on_demand(self, *args, **kwargs):
            return {'document_types_identified': {}}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedFrontmatterAction(BaseAction):
    """
    Enhanced Frontmatter Action with LLM integration and SHACL validation.
    
    Provides complete frontmatter generation workflow integrating:
    - Document type analysis
    - Schema-constrained LLM generation  
    - SHACL validation with retry loops
    - Error handling with 100% success guarantee
    """
    
    def __init__(self, action_type: str, params: Dict[str, Any], plugin_context: 'PluginContextPort'):
        super().__init__(action_type, params, plugin_context)
        
        # HMA v2.2 compliant logging through port
        self.log_port = self.context.get_port("logging")
        self.log_port.log_info("Enhanced frontmatter action initialized", action_type=action_type)
        
        # Initialize Phase 3 components using port-based access
        config_port = self.context.get_port("configuration")
        shacl_shapes_path = config_port.get_config_value("shacl_shapes_path", self.context.get_plugin_id(), 'standards/registry/shacl-shapes.ttl')
        jsonld_context_path = config_port.get_config_value("jsonld_context_path", self.context.get_plugin_id(), 'standards/registry/contexts/fields.jsonld')
        
        self.llm_integration = LLMSchemaIntegration(shacl_shapes_path, jsonld_context_path)
        self.validator = LLMSHACLValidator()
        self.prompt_engine = SchemaConstraintPromptEngine()
        self.error_handler = LLMErrorHandler()
        
        # Initialize Phase 1 components
        self.document_analyzer = UniversalDocumentTypeAnalyzer()
        
        # Generation statistics
        self.generation_stats = {
            'total_processed': 0,
            'successful_generations': 0,
            'fallback_generations': 0,
            'average_attempts': 0,
            'processing_history': []
        }
    
    @property
    def name(self) -> str:
        """Action name for Scribe registration."""
        return "enhanced-frontmatter"
    
    @property 
    def description(self) -> str:
        """Action description."""
        return "Generate enhanced frontmatter with LLM integration and SHACL validation"
    
    def execute(self, file_content: str, match: re.Match, file_path: str, params: Dict[str, Any]) -> str:
        """
        Step 5: Complete integration with Scribe workflow.
        
        Args:
            file_path: Path to the markdown file to process
            **kwargs: Additional options (info_type, force_regenerate, etc.)
            
        Returns:
            Execution result with success status and metadata
        """
        self.logger.info(f"Processing file: {file_path}")
        start_time = datetime.now()
        
        try:
            # Sub-step 5.1: Read document content
            content = self._read_file_content(file_path)
            
            # Sub-step 5.2: Detect or specify info-type
            info_type = params.get('info_type') or self._detect_info_type(content, file_path)
            
            # Sub-step 5.3: Check if regeneration is needed
            if not params.get('force_regenerate', False):
                existing_frontmatter = parse_frontmatter(content)
                if existing_frontmatter and self._is_frontmatter_valid(existing_frontmatter, info_type):
                    self.logger.info(f"Valid frontmatter exists, skipping generation for {file_path}")
                    return self._create_success_result(file_path, existing_frontmatter, 'skipped_valid')
            
            # Sub-step 5.4: Build schema-constrained prompt
            prompt = self._build_comprehensive_prompt(content, info_type, file_path)
            
            # Sub-step 5.5: Generate with validation loop (100% success)
            generation_result = self.validator.validate_with_retry_loop(prompt, info_type)
            
            # Sub-step 5.6: Apply frontmatter to document
            updated_content = apply_frontmatter(content, generation_result['frontmatter'])
            
            # Sub-step 5.7: Write updated content back to file
            self._write_file_content(file_path, updated_content)
            
            # Sub-step 5.8: Log success metrics
            processing_time = (datetime.now() - start_time).total_seconds()
            result = self._log_generation_metrics(file_path, generation_result, processing_time)
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error processing {file_path}: {e}")
            
            # Use error handler for recovery
            error_type = self.error_handler.classify_error(str(e))
            recovery_result = self.error_handler.handle_generation_errors(
                error_type, {'error_message': str(e), 'file_path': file_path}
            )
            
            # If recovery suggests deterministic fallback, use it
            if recovery_result.get('recovery_strategy') == 'deterministic_fallback':
                fallback_result = self.validator._generate_deterministic_fallback(
                    params.get('info_type', 'general-document')
                )
                
                try:
                    content = self._read_file_content(file_path)
                    updated_content = apply_frontmatter(content, fallback_result['frontmatter'])
                    self._write_file_content(file_path, updated_content)
                    
                    processing_time = (datetime.now() - start_time).total_seconds()
                    return self._log_generation_metrics(file_path, fallback_result, processing_time)
                    
                except Exception as fallback_error:
                    self.logger.error(f"Fallback generation failed for {file_path}: {fallback_error}")
            
            # Return error result if all recovery attempts fail
            return {
                'success': False,
                'file_processed': file_path,
                'error': str(e),
                'recovery_attempted': True,
                'recovery_result': recovery_result
            }
    
    def _read_file_content(self, file_path: str) -> str:
        """Sub-step 5.1: Read document content safely."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            self.logger.error(f"Could not read file {file_path}: {e}")
            raise
    
    def _detect_info_type(self, content: str, file_path: str) -> str:
        """Sub-step 5.2: Detect or specify info-type using Phase 1 analyzer."""
        try:
            # Use document analyzer for sophisticated type detection
            analysis_result = self.document_analyzer.analyze_on_demand([Path(file_path).parent])
            
            # Extract classification for this specific file
            file_classifications = analysis_result.get('document_types_identified', {})
            
            # Find classification for this file
            for file_key, classification in file_classifications.items():
                if Path(file_path).name in file_key or str(file_path) in file_key:
                    detected_type = classification.get('inferred_type', 'general-document')
                    self.logger.info(f"Detected info-type '{detected_type}' for {file_path}")
                    return detected_type
            
            # Fallback to content-based detection
            return self._fallback_content_type_detection(content, file_path)
            
        except Exception as e:
            self.logger.warning(f"Type detection failed, using fallback: {e}")
            return self._fallback_content_type_detection(content, file_path)
    
    def _fallback_content_type_detection(self, content: str, file_path: str) -> str:
        """Fallback content-based type detection."""
        content_lower = content.lower()
        
        # Simple pattern matching for common types
        if 'standard' in content_lower and ('definition' in content_lower or 'compliance' in content_lower):
            return 'standard-definition'
        elif 'policy' in content_lower or 'procedure' in content_lower:
            return 'policy-document'
        elif 'report' in content_lower or 'analysis' in content_lower:
            return 'technical-report'
        elif 'meeting' in content_lower or 'attendees' in content_lower:
            return 'meeting-notes'
        else:
            return 'general-document'
    
    
    def _is_frontmatter_valid(self, frontmatter: Dict[str, Any], info_type: str) -> bool:
        """Check if existing frontmatter is valid for the info-type."""
        try:
            # Use validator to check if current frontmatter passes SHACL
            validation_result = self.validator._validate_against_shacl(frontmatter, info_type)
            return validation_result.get('conforms', False)
        except Exception as e:
            self.logger.warning(f"Frontmatter validation check failed: {e}")
            return False
    
    def _build_comprehensive_prompt(self, content: str, info_type: str, file_path: str) -> str:
        """Sub-step 5.3: Build comprehensive schema-constrained prompt."""
        try:
            # Get SHACL constraints for the info-type
            constraints = self._get_constraints_for_type(info_type)
            
            # Use prompt engine to build sophisticated prompt
            prompt = self.prompt_engine.build_schema_constrained_prompt(
                content, info_type, constraints
            )
            
            # Add file-specific context
            file_context = f"""
FILE CONTEXT:
- File path: {file_path}
- File name: {Path(file_path).name}
- Processing timestamp: {datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')}

"""
            
            return file_context + prompt
            
        except Exception as e:
            self.logger.warning(f"Could not build comprehensive prompt: {e}")
            # Fallback to basic prompt
            return self._build_basic_prompt(content, info_type)
    
    def _get_constraints_for_type(self, info_type: str) -> Dict[str, Any]:
        """Get SHACL constraints for a specific info-type."""
        # This would typically load from SHACL shapes
        # For now, use predefined constraints based on the guideline
        constraints_map = {
            'general-document': {
                'mandatory_fields': ['title', 'info-type', 'kb-id'],
                'forbidden_fields': ['standard_id'],
                'pattern_constraints': {},
                'value_constraints': {'info-type': 'general-document'}
            },
            'standard-definition': {
                'mandatory_fields': ['title', 'info-type', 'standard_id', 'version', 'kb-id'],
                'forbidden_fields': [],
                'pattern_constraints': {'standard_id': '^[A-Z]{2}-[A-Z]{2,6}-[A-Z0-9\\-]+$'},
                'value_constraints': {'info-type': 'standard-definition'}
            },
            'policy-document': {
                'mandatory_fields': ['title', 'info-type', 'standard_id', 'version', 'kb-id'],
                'forbidden_fields': [],
                'pattern_constraints': {'standard_id': '^[A-Z]{2}-[A-Z]{2,6}-[A-Z0-9\\-]+$'},
                'value_constraints': {'info-type': 'policy-document'}
            },
            'technical-report': {
                'mandatory_fields': ['title', 'info-type', 'version', 'kb-id'],
                'forbidden_fields': ['standard_id'],
                'pattern_constraints': {},
                'value_constraints': {'info-type': 'technical-report'}
            },
            'meeting-notes': {
                'mandatory_fields': ['title', 'info-type', 'kb-id'],
                'forbidden_fields': ['standard_id', 'version'],
                'pattern_constraints': {},
                'value_constraints': {'info-type': 'meeting-notes'}
            }
        }
        
        return constraints_map.get(info_type, constraints_map['general-document'])
    
    def _build_basic_prompt(self, content: str, info_type: str) -> str:
        """Build basic prompt as fallback."""
        return f"""
Generate YAML frontmatter for a {info_type} document.

Document content preview:
{content[:500]}...

Requirements:
- Generate valid YAML frontmatter
- Include 'title', 'info-type', and 'kb-id' fields
- Set 'info-type' to '{info_type}'
- Use ISO 8601 format for dates

Generate ONLY the YAML frontmatter block:
"""
    
    
    def _write_file_content(self, file_path: str, content: str):
        """Write updated content back to file."""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            self.logger.info(f"Successfully updated {file_path}")
        except Exception as e:
            self.logger.error(f"Could not write file {file_path}: {e}")
            raise
    
    def _log_generation_metrics(self, file_path: str, generation_result: Dict[str, Any], 
                              processing_time: float) -> Dict[str, Any]:
        """Sub-step 5.8: Log success metrics and return result."""
        success = generation_result.get('success', False)
        attempts_used = generation_result.get('attempts_used', 1)
        fallback_used = generation_result.get('fallback_used', False)
        validation_method = generation_result.get('validation_method', 'unknown')
        
        # Update statistics
        self.generation_stats['total_processed'] += 1
        if success:
            self.generation_stats['successful_generations'] += 1
        if fallback_used:
            self.generation_stats['fallback_generations'] += 1
        
        # Update average attempts
        total_attempts = (self.generation_stats['average_attempts'] * 
                         (self.generation_stats['total_processed'] - 1) + attempts_used)
        self.generation_stats['average_attempts'] = total_attempts / self.generation_stats['total_processed']
        
        # Record processing history
        history_entry = {
            'file_path': file_path,
            'timestamp': datetime.now().isoformat(),
            'success': success,
            'attempts_used': attempts_used,
            'fallback_used': fallback_used,
            'validation_method': validation_method,
            'processing_time_seconds': processing_time
        }
        self.generation_stats['processing_history'].append(history_entry)
        
        # Log metrics
        self.logger.info(f"Generation metrics for {file_path}:")
        self.logger.info(f"  Success: {success}")
        self.logger.info(f"  Attempts: {attempts_used}")
        self.logger.info(f"  Fallback used: {fallback_used}")
        self.logger.info(f"  Processing time: {processing_time:.2f}s")
        self.logger.info(f"  Validation method: {validation_method}")
        
        return self._create_success_result(file_path, generation_result['frontmatter'], validation_method)
    
    def _create_success_result(self, file_path: str, frontmatter: Dict[str, Any], 
                             method: str) -> Dict[str, Any]:
        """Create success result dictionary."""
        return {
            'success': True,
            'file_processed': file_path,
            'info_type': frontmatter.get('info-type', 'unknown'),
            'frontmatter_fields': list(frontmatter.keys()),
            'generation_method': method,
            'processing_timestamp': datetime.now().isoformat()
        }
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """Get comprehensive processing statistics."""
        stats = self.generation_stats.copy()
        
        if stats['total_processed'] > 0:
            stats['success_rate'] = (stats['successful_generations'] / 
                                   stats['total_processed'] * 100)
            stats['fallback_rate'] = (stats['fallback_generations'] / 
                                    stats['total_processed'] * 100)
        else:
            stats['success_rate'] = 0
            stats['fallback_rate'] = 0
        
        return stats
    
    def process_multiple_files(self, file_paths: List[str], params: Dict[str, Any]) -> Dict[str, Any]:
        """Process multiple files with enhanced frontmatter generation."""
        results = {}
        total_start_time = datetime.now()
        
        for file_path in file_paths:
            try:
                result = self.execute(file_path, **params)
                results[file_path] = result
            except Exception as e:
                results[file_path] = {
                    'success': False,
                    'error': str(e)
                }
        
        total_time = (datetime.now() - total_start_time).total_seconds()
        
        return {
            'total_files': len(file_paths),
            'results': results,
            'total_processing_time': total_time,
            'statistics': self.get_processing_statistics()
        }


# Registration function for Scribe engine
def register_action():
    """Register the enhanced frontmatter action with Scribe."""
    return EnhancedFrontmatterAction()


if __name__ == "__main__":
    # Example usage for testing
    action = EnhancedFrontmatterAction()
    
    # Test with a sample file (if it exists)
    test_files = [
        'README.md',
        'standards/src/MT-SCHEMA-FRONTMATTER.md'
    ]
    
    for test_file in test_files:
        if os.path.exists(test_file):
            print(f"Testing enhanced frontmatter generation on: {test_file}")
            result = action.execute(test_file, force_regenerate=True)
            print(f"Result: {result}")
            break
    else:
        print("No test files found for demonstration")
    
    # Get statistics
    stats = action.get_processing_statistics()
    print(f"Processing Statistics: {stats}")
```
  ```
  --- END OF FILE scribe/actions\enhanced_frontmatter_action.py ---
  --- START OF FILE scribe/actions\enhanced_frontmatter_action\manifest.json ---
  ```
  ```json
{
  "manifest_version": "2.2",
  "plugin_metadata": {
    "name": "enhanced_frontmatter_action",
    "version": "2.2.0",
    "description": "LLM-enhanced frontmatter generation with SHACL validation and schema constraint integration",
    "author": "Scribe Engine Development Team",
    "license": "MIT",
    "type": "L3-Capability"
  },
  "hma_compliance": {
    "hma_version": "2.2",
    "tier_classification": {
      "mandatory": [
        "json_schema",
        "otel_boundary",
        "mtls"
      ],
      "recommended": [
        "structured_logging",
        "health_checks",
        "kubernetes"
      ],
      "alternative": []
    },
    "boundary_interfaces": [
      {
        "port_type": "PluginExecutionPort",
        "direction": "inbound",
        "validation": "json_schema",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "FileSystemPort",
        "direction": "outbound",
        "validation": "path_security",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "LoggingPort",
        "direction": "outbound",
        "validation": "structured_format",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "EventBusPort",
        "direction": "outbound",
        "validation": "event_schema",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "PluginContextPort",
        "direction": "inbound",
        "validation": "context_schema",
        "telemetry": "otel_spans"
      }
    ]
  },
  "runtime_requirements": {
    "python_version": ">=3.8",
    "dependencies": {
      "required": [
        {
          "name": "pyyaml",
          "version": ">=6.0",
          "tier": "mandatory"
        },
        {
          "name": "structlog",
          "version": ">=23.1.0",
          "tier": "recommended"
        }
      ],
      "optional": [
        {
          "name": "rdflib",
          "version": ">=6.0.0",
          "feature": "SHACL validation"
        },
        {
          "name": "pyshacl",
          "version": ">=0.20.0",
          "feature": "SHACL validation"
        },
        {
          "name": "openai",
          "version": ">=1.0.0",
          "feature": "LLM integration"
        }
      ]
    },
    "platform_support": [
      "windows",
      "linux",
      "macos"
    ],
    "resource_limits": {
      "max_memory_mb": 512,
      "max_cpu_percent": 50,
      "max_file_handles": 50
    }
  },
  "interface_contracts": {
    "action_interface": {
      "entry_point": "enhanced_frontmatter_action.EnhancedFrontmatterAction",
      "configuration_schema": {
        "type": "object",
        "required": [
          "schema_file"
        ],
        "properties": {
          "schema_file": {
            "type": "string",
            "description": "Path to YAML schema file for frontmatter validation"
          },
          "llm_enabled": {
            "type": "boolean",
            "default": true,
            "description": "Whether to use LLM for enhanced generation"
          },
          "validation_mode": {
            "type": "string",
            "enum": [
              "strict",
              "permissive",
              "disabled"
            ],
            "default": "strict",
            "description": "SHACL validation strictness level"
          },
          "fallback_mode": {
            "type": "string",
            "enum": [
              "template",
              "basic",
              "skip"
            ],
            "default": "template",
            "description": "Fallback behavior when LLM fails"
          },
          "max_retries": {
            "type": "integer",
            "minimum": 0,
            "maximum": 10,
            "default": 3,
            "description": "Maximum LLM generation retry attempts"
          }
        }
      },
      "event_patterns": [
        {
          "event_type": "file_created",
          "file_patterns": [
            "*.md",
            "*.markdown"
          ],
          "priority": 7
        },
        {
          "event_type": "file_modified",
          "file_patterns": [
            "*.md",
            "*.markdown"
          ],
          "priority": 6
        }
      ],
      "output_schema": {
        "type": "object",
        "properties": {
          "frontmatter_added": {
            "type": "boolean"
          },
          "validation_passed": {
            "type": "boolean"
          },
          "llm_used": {
            "type": "boolean"
          },
          "fallback_used": {
            "type": "boolean"
          },
          "retry_count": {
            "type": "integer"
          }
        }
      }
    },
    "lifecycle_hooks": {
      "on_load": "initialize_llm_components",
      "on_config_change": "reload_schema_configuration",
      "health_check": "health_check"
    }
  },
  "security": {
    "permissions": [
      "file_read",
      "file_write",
      "network_access",
      "environment_access"
    ],
    "sandbox_compatible": true,
    "mtls_required": true
  }
}
```
  ```
  --- END OF FILE scribe/actions\enhanced_frontmatter_action\manifest.json ---
  --- START OF FILE scribe/actions\file_processing_orchestrator\__init__.py ---
  ```
  ```py
# FileProcessingOrchestrator L2 Plugin Module
```
  ```
  --- END OF FILE scribe/actions\file_processing_orchestrator\__init__.py ---
  --- START OF FILE scribe/actions\file_processing_orchestrator\file_processing_orchestrator.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
File Processing Orchestrator - L2 Orchestrator Plugin

This L2 Orchestrator plugin coordinates complex file processing workflows
by invoking L3 capability plugins in the correct sequence. It replaces the
legacy Worker logic with proper HMA v2.2 orchestration patterns.
"""

import re
import asyncio
from typing import Dict, Any, List, Optional
from pathlib import Path

from ..base import BaseAction, ActionExecutionError


class FileProcessingOrchestrator(BaseAction):
    """
    L2 Orchestrator plugin for coordinating file processing workflows.
    
    This plugin implements the HMA v2.2 L2 Orchestrator pattern, replacing
    the legacy Worker's orchestration logic with proper port-based coordination
    of L3 capability plugins.
    
    HMA Layer: L2-Orchestrator
    Purpose: Coordinates multi-step file processing workflows
    """
    
    def get_required_params(self) -> List[str]:
        """Get required parameters for the orchestrator."""
        return ["workflow_config"]
    
    def get_optional_params(self) -> Dict[str, Any]:
        """Get optional parameters and their defaults."""
        return {
            "parallel_processing": False,
            "error_strategy": "fail_fast",  # fail_fast, continue_on_error, retry
            "max_retries": 3,
            "timeout_seconds": 300
        }
    
    def validate_params(self, params: Dict[str, Any]) -> bool:
        """Validate orchestrator parameters."""
        try:
            workflow_config = params.get("workflow_config")
            if not isinstance(workflow_config, dict):
                raise ActionExecutionError(
                    self.action_type,
                    "workflow_config must be a dictionary"
                )
            
            # Validate workflow stages
            stages = workflow_config.get("stages", [])
            if not isinstance(stages, list) or len(stages) == 0:
                raise ActionExecutionError(
                    self.action_type,
                    "workflow_config must contain at least one stage"
                )
            
            # Validate each stage
            for i, stage in enumerate(stages):
                if not isinstance(stage, dict):
                    raise ActionExecutionError(
                        self.action_type,
                        f"Stage {i} must be a dictionary"
                    )
                
                if "plugin_id" not in stage:
                    raise ActionExecutionError(
                        self.action_type,
                        f"Stage {i} missing required 'plugin_id' field"
                    )
            
            return True
            
        except ActionExecutionError:
            raise
        except Exception as e:
            raise ActionExecutionError(
                self.action_type,
                f"Parameter validation failed: {e}"
            )
    
    async def execute(self, 
                      file_content: str, 
                      match: re.Match, 
                      file_path: str, 
                      params: Dict[str, Any]) -> str:
        """
        Execute the file processing orchestration workflow.
        
        Args:
            file_content: The full content of the file being processed
            match: The regex match object that triggered the rule
            file_path: The path to the file being processed
            params: Orchestrator parameters including workflow_config
            
        Returns:
            The final processed file content after all stages
            
        Raises:
            ActionExecutionError: If orchestration fails
        """
        try:
            # Validate parameters
            self.validate_params(params)
            
            # Extract orchestration parameters
            workflow_config = params["workflow_config"]
            stages = workflow_config["stages"]
            parallel_processing = params.get("parallel_processing", False)
            error_strategy = params.get("error_strategy", "fail_fast")
            max_retries = params.get("max_retries", 3)
            timeout_seconds = params.get("timeout_seconds", 300)
            
            # Log orchestration start
            self.log_port.log_info("Starting file processing orchestration",
                                  file_path=file_path,
                                  workflow_name=workflow_config.get("name", "default"),
                                  stage_count=len(stages),
                                  parallel_processing=parallel_processing,
                                  plugin_id=self.context.get_plugin_id())
            
            # Publish orchestration start event
            await self.publish_event(
                "orchestration_started",
                {
                    "file_path": file_path,
                    "workflow_name": workflow_config.get("name", "default"),
                    "stage_count": len(stages),
                    "orchestrator_id": self.context.get_plugin_id()
                }
            )
            
            # Execute workflow based on processing mode
            if parallel_processing:
                final_content = await self._execute_parallel_workflow(
                    file_content, match, file_path, stages, error_strategy, max_retries, timeout_seconds
                )
            else:
                final_content = await self._execute_sequential_workflow(
                    file_content, match, file_path, stages, error_strategy, max_retries, timeout_seconds
                )
            
            # Log orchestration completion
            self.log_port.log_info("File processing orchestration completed successfully",
                                  file_path=file_path,
                                  workflow_name=workflow_config.get("name", "default"),
                                  content_changed=final_content != file_content,
                                  plugin_id=self.context.get_plugin_id())
            
            # Publish orchestration completion event
            await self.publish_event(
                "orchestration_completed",
                {
                    "file_path": file_path,
                    "workflow_name": workflow_config.get("name", "default"),
                    "success": True,
                    "content_changed": final_content != file_content,
                    "orchestrator_id": self.context.get_plugin_id()
                }
            )
            
            return final_content
            
        except ActionExecutionError:
            raise
        except Exception as e:
            self.log_port.log_error("File processing orchestration failed",
                                   file_path=file_path,
                                   error=str(e),
                                   plugin_id=self.context.get_plugin_id())
            
            # Publish orchestration failure event
            await self.publish_event(
                "orchestration_failed",
                {
                    "file_path": file_path,
                    "error": str(e),
                    "orchestrator_id": self.context.get_plugin_id()
                }
            )
            
            raise ActionExecutionError(
                self.action_type,
                f"Orchestration failed: {e}"
            )
    
    async def _execute_sequential_workflow(self,
                                         file_content: str,
                                         match: re.Match,
                                         file_path: str,
                                         stages: List[Dict[str, Any]],
                                         error_strategy: str,
                                         max_retries: int,
                                         timeout_seconds: int) -> str:
        """Execute workflow stages sequentially."""
        current_content = file_content
        
        for stage_index, stage in enumerate(stages):
            plugin_id = stage["plugin_id"]
            stage_params = stage.get("params", {})
            
            try:
                # Log stage start
                self.log_port.log_info("Executing workflow stage",
                                      stage_index=stage_index,
                                      plugin_id=plugin_id,
                                      file_path=file_path)
                
                # Execute stage with retry logic
                for attempt in range(max_retries + 1):
                    try:
                        # Get plugin execution port
                        plugin_port = self.context.get_port("plugin_execution")
                        
                        # Prepare execution data
                        input_data = {
                            "file_content": current_content,
                            "match": match,
                            "file_path": file_path,
                            "params": stage_params
                        }
                        
                        # Execute the plugin
                        result = await asyncio.wait_for(
                            plugin_port.execute_plugin(plugin_id, input_data),
                            timeout=timeout_seconds
                        )
                        
                        if result.get("success", False):
                            current_content = result.get("result", current_content)
                            self.log_port.log_info("Workflow stage completed successfully",
                                                  stage_index=stage_index,
                                                  plugin_id=plugin_id,
                                                  attempt=attempt + 1)
                            break
                        else:
                            error_msg = result.get("error", "Unknown error")
                            raise ActionExecutionError(plugin_id, error_msg)
                            
                    except asyncio.TimeoutError:
                        if attempt < max_retries:
                            self.log_port.log_warning("Stage execution timed out, retrying",
                                                     stage_index=stage_index,
                                                     plugin_id=plugin_id,
                                                     attempt=attempt + 1)
                            continue
                        else:
                            raise ActionExecutionError(
                                plugin_id,
                                f"Stage timed out after {max_retries} attempts"
                            )
                    except Exception as e:
                        if attempt < max_retries:
                            self.log_port.log_warning("Stage execution failed, retrying",
                                                     stage_index=stage_index,
                                                     plugin_id=plugin_id,
                                                     attempt=attempt + 1,
                                                     error=str(e))
                            continue
                        else:
                            raise
                
            except Exception as e:
                self.log_port.log_error("Workflow stage failed",
                                       stage_index=stage_index,
                                       plugin_id=plugin_id,
                                       error=str(e))
                
                # Handle error based on strategy
                if error_strategy == "fail_fast":
                    raise ActionExecutionError(
                        self.action_type,
                        f"Stage {stage_index} ({plugin_id}) failed: {e}"
                    )
                elif error_strategy == "continue_on_error":
                    self.log_port.log_warning("Continuing workflow despite stage failure",
                                             stage_index=stage_index,
                                             plugin_id=plugin_id)
                    continue
                else:
                    raise ActionExecutionError(
                        self.action_type,
                        f"Unknown error strategy: {error_strategy}"
                    )
        
        return current_content
    
    async def _execute_parallel_workflow(self,
                                        file_content: str,
                                        match: re.Match,
                                        file_path: str,
                                        stages: List[Dict[str, Any]],
                                        error_strategy: str,
                                        max_retries: int,
                                        timeout_seconds: int) -> str:
        """Execute workflow stages in parallel (for independent operations)."""
        # For parallel execution, all stages receive the same input content
        # and their results are merged or the first successful result is returned
        
        self.log_port.log_info("Starting parallel workflow execution",
                              stage_count=len(stages),
                              file_path=file_path)
        
        tasks = []
        for stage_index, stage in enumerate(stages):
            plugin_id = stage["plugin_id"]
            stage_params = stage.get("params", {})
            
            task = asyncio.create_task(
                self._execute_single_stage(
                    file_content, match, file_path, plugin_id, stage_params, 
                    stage_index, max_retries, timeout_seconds
                )
            )
            tasks.append((stage_index, plugin_id, task))
        
        # Wait for all tasks with error handling
        results = []
        for stage_index, plugin_id, task in tasks:
            try:
                result = await task
                results.append((stage_index, plugin_id, result, None))
            except Exception as e:
                results.append((stage_index, plugin_id, None, e))
                
                if error_strategy == "fail_fast":
                    # Cancel remaining tasks
                    for _, _, remaining_task in tasks:
                        remaining_task.cancel()
                    raise ActionExecutionError(
                        self.action_type,
                        f"Parallel stage {stage_index} ({plugin_id}) failed: {e}"
                    )
        
        # Process results - for now, return the content from the first successful stage
        # In a more sophisticated implementation, this could merge results or apply other logic
        for stage_index, plugin_id, result, error in results:
            if error is None and result:
                self.log_port.log_info("Using result from parallel stage",
                                      stage_index=stage_index,
                                      plugin_id=plugin_id)
                return result
        
        # If no stage succeeded, return original content
        self.log_port.log_warning("No parallel stages succeeded, returning original content")
        return file_content
    
    async def _execute_single_stage(self,
                                   file_content: str,
                                   match: re.Match,
                                   file_path: str,
                                   plugin_id: str,
                                   stage_params: Dict[str, Any],
                                   stage_index: int,
                                   max_retries: int,
                                   timeout_seconds: int) -> str:
        """Execute a single stage with retry logic."""
        for attempt in range(max_retries + 1):
            try:
                # Get plugin execution port
                plugin_port = self.context.get_port("plugin_execution")
                
                # Prepare execution data
                input_data = {
                    "file_content": file_content,
                    "match": match,
                    "file_path": file_path,
                    "params": stage_params
                }
                
                # Execute the plugin
                result = await asyncio.wait_for(
                    plugin_port.execute_plugin(plugin_id, input_data),
                    timeout=timeout_seconds
                )
                
                if result.get("success", False):
                    return result.get("result", file_content)
                else:
                    error_msg = result.get("error", "Unknown error")
                    raise ActionExecutionError(plugin_id, error_msg)
                    
            except asyncio.TimeoutError:
                if attempt < max_retries:
                    self.log_port.log_warning("Single stage execution timed out, retrying",
                                             stage_index=stage_index,
                                             plugin_id=plugin_id,
                                             attempt=attempt + 1)
                    continue
                else:
                    raise ActionExecutionError(
                        plugin_id,
                        f"Stage timed out after {max_retries} attempts"
                    )
            except Exception as e:
                if attempt < max_retries:
                    self.log_port.log_warning("Single stage execution failed, retrying",
                                             stage_index=stage_index,
                                             plugin_id=plugin_id,
                                             attempt=attempt + 1,
                                             error=str(e))
                    continue
                else:
                    raise
        
        # Should not reach here
        raise ActionExecutionError(
            plugin_id,
            f"Stage failed after {max_retries} attempts"
        )
    
    def get_description(self) -> str:
        """Get description of the orchestrator."""
        return "L2 Orchestrator plugin for coordinating complex file processing workflows"
```
  ```
  --- END OF FILE scribe/actions\file_processing_orchestrator\file_processing_orchestrator.py ---
  --- START OF FILE scribe/actions\file_processing_orchestrator\manifest.json ---
  ```
  ```json
{
  "manifest_version": "2.2",
  "plugin_metadata": {
    "name": "file_processing_orchestrator",
    "version": "2.2.0",
    "type": "L2-Orchestrator",
    "description": "HMA v2.2 compliant L2 Orchestrator for coordinating complex file processing workflows",
    "author": "Scribe Engine Development Team",
    "license": "MIT"
  },
  "hma_compliance": {
    "hma_version": "2.2",
    "tier_classification": {
      "mandatory": [
        "json_schema",
        "otel_boundary",
        "mtls"
      ],
      "recommended": [
        "structured_logging",
        "health_checks",
        "nats_messaging",
        "kubernetes"
      ],
      "alternative": []
    },
    "boundary_interfaces": [
      {
        "port_type": "PluginExecutionPort",
        "direction": "inbound",
        "validation": "json_schema",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "PluginExecutionPort",
        "direction": "outbound",
        "validation": "orchestration_schema",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "LoggingPort",
        "direction": "outbound",
        "validation": "structured_format",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "EventBusPort",
        "direction": "outbound",
        "validation": "event_schema",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "PluginContextPort",
        "direction": "inbound",
        "validation": "context_schema",
        "telemetry": "otel_spans"
      }
    ]
  },
  "runtime_requirements": {
    "python_version": ">=3.8",
    "dependencies": {
      "required": [
        {
          "name": "asyncio",
          "version": ">=3.8",
          "tier": "mandatory"
        },
        {
          "name": "structlog",
          "version": ">=23.1.0",
          "tier": "recommended"
        }
      ],
      "optional": []
    },
    "platform_support": [
      "windows",
      "linux",
      "macos"
    ],
    "resource_limits": {
      "max_memory_mb": 256,
      "max_cpu_percent": 40,
      "max_file_handles": 20
    }
  },
  "interface_contracts": {
    "action_interface": {
      "entry_point": "file_processing_orchestrator.FileProcessingOrchestrator",
      "configuration_schema": {
        "type": "object",
        "required": [
          "workflow_config"
        ],
        "properties": {
          "workflow_config": {
            "type": "object",
            "required": [
              "stages"
            ],
            "properties": {
              "name": {
                "type": "string",
                "description": "Human-readable workflow name"
              },
              "stages": {
                "type": "array",
                "minItems": 1,
                "items": {
                  "type": "object",
                  "required": [
                    "plugin_id"
                  ],
                  "properties": {
                    "plugin_id": {
                      "type": "string",
                      "description": "ID of the L3 capability plugin to execute"
                    },
                    "params": {
                      "type": "object",
                      "description": "Parameters to pass to the plugin"
                    }
                  }
                }
              }
            }
          },
          "parallel_processing": {
            "type": "boolean",
            "default": false,
            "description": "Whether to execute stages in parallel"
          },
          "error_strategy": {
            "type": "string",
            "enum": [
              "fail_fast",
              "continue_on_error",
              "retry"
            ],
            "default": "fail_fast",
            "description": "How to handle stage failures"
          },
          "max_retries": {
            "type": "integer",
            "minimum": 0,
            "maximum": 10,
            "default": 3,
            "description": "Maximum retry attempts per stage"
          },
          "timeout_seconds": {
            "type": "integer",
            "minimum": 1,
            "maximum": 3600,
            "default": 300,
            "description": "Timeout per stage in seconds"
          }
        }
      },
      "event_patterns": [
        {
          "event_type": "orchestration_requested",
          "file_patterns": [
            "*"
          ],
          "priority": 8
        }
      ],
      "output_schema": {
        "type": "object",
        "properties": {
          "workflow_executed": {
            "type": "boolean"
          },
          "stages_completed": {
            "type": "integer"
          },
          "stages_failed": {
            "type": "integer"
          },
          "content_modified": {
            "type": "boolean"
          },
          "execution_time_ms": {
            "type": "number"
          }
        }
      }
    },
    "lifecycle_hooks": {
      "health_check": "health_check"
    }
  },
  "security": {
    "permissions": [
      "plugin_execution",
      "event_publish",
      "logging"
    ],
    "sandbox_compatible": true,
    "mtls_required": true
  }
}
```
  ```
  --- END OF FILE scribe/actions\file_processing_orchestrator\manifest.json ---
  --- START OF FILE scribe/actions\graph_validation_action.py ---
  ```
  ```py
# tools/scribe/actions/graph_validation_action.py
import logging
from pathlib import Path
import json # For loading context if master_index_data is passed as string
from typing import Dict, Any

from .base import BaseAction, ActionExecutionError
from tools.validators.graph_validator import GraphValidator


class GraphValidationAction(BaseAction):
    def __init__(self, action_type: str, params: Dict[str, Any], plugin_context: 'PluginContextPort'):
        super().__init__(action_type, params, plugin_context)
        
        # HMA v2.2 compliant configuration access through port
        config_port = self.context.get_port("configuration")
        self.log_port = self.context.get_port("logging")
        
        self.schema_registry_path_str = self.params.get("schema_registry_path", "standards/registry/schema-registry.jsonld")
        self.master_index_path_str = self.params.get("master_index_path", "standards/registry/master-index.jsonld")
        self.shacl_shapes_path_str = self.params.get("shacl_shapes_path", "standards/registry/shacl-shapes.ttl")
        self.report_output_path_str = self.params.get("report_output_path", "scribe_validation_report.json")
        self.validator_instance = None
        
        # Get repo root through configuration port
        try:
            self.repo_root = Path(config_port.get_config_value("repo_root", self.context.get_plugin_id(), "."))
        except Exception as e:
            self.log_port.log_warning("Could not get repo_root from config, using current directory", error=str(e))
            self.repo_root = Path(".")


    def setup(self):

        self.schema_registry_file = self.repo_root / self.schema_registry_path_str
        self.master_index_file = self.repo_root / self.master_index_path_str
        self.shacl_shapes_file = self.repo_root / self.shacl_shapes_path_str
        self.report_output_file = self.repo_root / self.report_output_path_str # Ensure report path is absolute or relative to repo_root

        # Validate paths
        if not self.schema_registry_file.exists():
            self.logger.error(f"Schema registry not found: {self.schema_registry_file}")
            return False
        if not self.master_index_file.exists(): # Master index might be created by a previous step, but path should be valid
            self.logger.warning(f"Master index path specified, but file may not exist yet: {self.master_index_file}")
        if not self.shacl_shapes_file.exists():
            self.logger.error(f"SHACL shapes file not found: {self.shacl_shapes_file}")
            return False

        # Ensure report output directory exists
        self.report_output_file.parent.mkdir(parents=True, exist_ok=True)

        try:
            # Pass repo_root to GraphValidator, as it constructs absolute paths from it.
            self.validator_instance = GraphValidator(repo_base_path=str(self.repo_root))
            # Configure validator paths directly (GraphValidator's __init__ loads schema, but master_index is loaded later)
            # GraphValidator's internal paths are relative to its repo_base_path.
            # We need to ensure it uses the paths provided in this action's config if they differ from its defaults.
            # However, GraphValidator's current __init__ directly loads schema based on its internal logic.
            # And master_index is loaded via _load_master_index().
            # This action will primarily call validator.validate_all_documents() and validator.generate_validation_report()
            # The paths used by GraphValidator for schema, master_index, shacl are derived inside it using its self.registry_path.
            # To override, we'd need to modify GraphValidator or pass paths more directly.
            # For now, assume GraphValidator's default path construction (relative to its repo_root) is acceptable
            # as long as self.repo_root is correctly set for it.
            self.logger.info(f"GraphValidator instance created with repo_base_path: {self.repo_root}")
            self.logger.info(f"GraphValidator will use schema: {self.validator_instance.registry_path / 'schema-registry.jsonld'}")
            self.logger.info(f"GraphValidator will use master index: {self.validator_instance.registry_path / 'master-index.jsonld'}")
            self.logger.info(f"GraphValidator will use SHACL shapes from its internal shacl_shapes_path attribute if set, or its default logic.")

        except Exception as e:
            self.logger.error(f"Failed to initialize GraphValidator: {e}")
            return False

        self.logger.info("GraphValidationAction setup complete.")
        return True

    def execute(self, file_content: str, match, file_path: str, params: Dict[str, Any]) -> str:
        self.logger.info(f"Executing GraphValidationAction. Context: {params}")
        if not self.validator_instance:
            self.logger.error("GraphValidator instance not available. Setup might have failed.")
            raise ActionExecutionError(self.action_type, "Setup failed: GraphValidator not initialized.")

        # Handle potential master_index_data from context (e.g., if index was just generated)
        # GraphValidator's _load_master_index currently reads from a file.
        # If master_index_data is in context, we'd ideally pass it to the validator.
        # This might require a small modification to GraphValidator or a temporary file write.
        # For now, assume GraphValidator reads from MASTER_INDEX_PATH as defined.
        if 'master_index_data' in params and params['master_index_data']:
            self.logger.info("Master index data found in execution_context. Current GraphValidator loads from file; this data is not directly used yet.")


        try:
            # The GraphValidator needs its internal master_index path to point to the correct file.
            # This is handled if self.repo_root passed to its constructor is correct.
            # GraphValidator also has its own _load_shacl_shapes which uses self.shacl_shapes_path.
            # We need to ensure GraphValidator uses the SHACL path from this action's config.
            # This might involve setting it on the instance if GraphValidator allows it.
            if hasattr(self.validator_instance, 'shacl_shapes_path'):
                 self.validator_instance.shacl_shapes_path = self.shacl_shapes_file # Override if possible
                 self.logger.info(f"Set GraphValidator.shacl_shapes_path to {self.shacl_shapes_file}")


            validation_errors = self.validator_instance.validate_all_documents() # This also generates relationships and SHACL errors
            report = self.validator_instance.generate_validation_report(validation_errors)

            # Save the report
            with open(self.report_output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Validation report saved to: {self.report_output_file}")

            error_count = report.get("total_errors", 0)
            # Assuming shacl_validation_errors is a list of strings within report['errors'] or a specific key
            # The current GraphValidator appends SHACL errors to the main 'errors' list.
            # And has a shacl_validation_errors list in its instance.
            shacl_error_count = 0
            if hasattr(self.validator_instance, 'shacl_validation_errors'):
                shacl_error_count = len(self.validator_instance.shacl_validation_errors)


            status = 'success'
            if error_count > 0:
                # Distinguish critical? For now, any error is non_critical failure for Scribe.
                # Critical failure might be if the validator itself crashes.
                status = 'failure_non_critical'
                raise ActionExecutionError(self.action_type, f"Graph validation complete. Total errors: {error_count}. SHACL errors: {shacl_error_count}.")


            return file_content # Return original content

        except Exception as e:
            self.logger.exception(f"Error during graph validation: {e}")
            raise ActionExecutionError(self.action_type, f"Graph validation failed with exception: {e}", original_error=e)

# Example usage (for testing)
if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger("GraphValidationActionTest")

    mock_global_config = {
        "repo_root": str(Path(__file__).resolve().parent.parent.parent)
    }
    mock_action_config = {
        "report_output_path": "temp_scribe_validation_report.json"
        # Paths for schema, master_index, shacl_shapes will use defaults relative to repo_root
    }

    # Ensure necessary files (schema, master_index, shacl) exist for testing
    # For this test, we assume they are in their default locations within the mock_repo_root
    # e.g., mock_repo_root/standards/registry/schema-registry.jsonld etc.
    # Actual testing would require these files to be present.

    action = GraphValidationAction(action_type="graph_validation", params=mock_action_config, config_manager=None, security_manager=None) # Mock ConfigManager and SecurityManager

    if action.setup():
        # Create dummy master-index if it doesn't exist for the test, to avoid setup failure for path check
        # This is a simplified test setup.
        mj_path = action.repo_root / action.params.get("master_index_path", "standards/registry/master-index.jsonld")
        mj_path.parent.mkdir(parents=True, exist_ok=True)
        if not mj_path.exists():
            with open(mj_path, 'w') as f:
                json.dump({"kb:documents":[]}, f) # Minimal master index
            logger.info(f"Created dummy master index for test: {mj_path}")


        test_context = {}
        # Mock file_content, match, file_path for the test execution
        test_file_content = '{"kb:documents": [{"@id": "http://example.org/doc1", "kb:name": "Document 1", "kb:content": "This is a test document."}]}'
        test_match = None # No specific match for this action
        test_file_path = "test_document.jsonld"

        try:
            result = action.execute(test_file_content, test_match, test_file_path, test_context)
            logger.info(f"Test execution result: {result}")

            # Clean up report
            if Path(action.report_output_file).exists():
                 action.report_output_file.unlink()
                 logger.info(f"Cleaned up report: {action.report_output_file}")
            elif (action.repo_root / mock_action_config['report_output_path']).exists():
                 (action.repo_root / mock_action_config['report_output_path']).unlink()
                 logger.info(f"Cleaned up report: {mock_action_config['report_output_path']}")

        except ActionExecutionError as e:
            logger.error(f"Test execution failed with ActionExecutionError: {e}")
            if Path(action.report_output_file).exists():
                action.report_output_file.unlink()
                logger.info(f"Cleaned up report: {action.report_output_file}")
            elif (action.repo_root / mock_action_config['report_output_path']).exists():
                (action.repo_root / mock_action_config['report_output_path']).unlink()
                logger.info(f"Cleaned up report: {mock_action_config['report_output_path']}")


    else:
        logger.error("Test setup failed.")

```
  ```
  --- END OF FILE scribe/actions\graph_validation_action.py ---
  --- START OF FILE scribe/actions\graph_validation_action\manifest.json ---
  ```
  ```json
{
  "manifest_version": "2.2",
  "plugin_metadata": {
    "name": "graph_validation_action",
    "version": "2.2.0",
    "description": "Validates knowledge graph structures and relationships for consistency",
    "author": "Scribe Engine Development Team",
    "license": "MIT",
    "type": "L3-Capability"
  },
  "hma_compliance": {
    "hma_version": "2.2",
    "tier_classification": {
      "mandatory": [
        "json_schema",
        "otel_boundary",
        "mtls"
      ],
      "recommended": [
        "structured_logging",
        "health_checks",
        "kubernetes"
      ],
      "alternative": []
    },
    "boundary_interfaces": [
      {
        "port_type": "PluginExecutionPort",
        "direction": "inbound",
        "validation": "json_schema",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "EventBusPort",
        "direction": "outbound",
        "validation": "event_schema",
        "telemetry": "otel_spans"
      }
    ]
  },
  "runtime_requirements": {
    "python_version": ">=3.8",
    "dependencies": {
      "required": [
        {
          "name": "structlog",
          "version": ">=23.1.0",
          "tier": "recommended"
        }
      ],
      "optional": [
        {
          "name": "networkx",
          "version": ">=2.8.0",
          "feature": "Graph analysis algorithms"
        },
        {
          "name": "rdflib",
          "version": ">=6.0.0",
          "feature": "RDF graph validation"
        }
      ]
    },
    "platform_support": [
      "windows",
      "linux",
      "macos"
    ],
    "resource_limits": {
      "max_memory_mb": 300,
      "max_cpu_percent": 40,
      "max_file_handles": 75
    }
  },
  "interface_contracts": {
    "action_interface": {
      "entry_point": "graph_validation_action.GraphValidationAction",
      "configuration_schema": {
        "type": "object",
        "properties": {
          "graph_data_path": {
            "type": "string",
            "description": "Path to graph data files or directory"
          },
          "validation_rules": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "List of validation rules to apply"
          },
          "output_report_path": {
            "type": "string",
            "description": "Path for validation report output"
          },
          "strict_mode": {
            "type": "boolean",
            "default": false,
            "description": "Whether to fail on any validation errors"
          }
        }
      },
      "event_patterns": [
        {
          "event_type": "file_created",
          "file_patterns": [
            "*.rdf",
            "*.ttl",
            "*.jsonld",
            "*.owl"
          ],
          "priority": 7
        },
        {
          "event_type": "file_modified",
          "file_patterns": [
            "*.rdf",
            "*.ttl",
            "*.jsonld",
            "*.owl"
          ],
          "priority": 6
        }
      ],
      "output_schema": {
        "type": "object",
        "properties": {
          "validation_passed": {
            "type": "boolean"
          },
          "errors_found": {
            "type": "integer"
          },
          "warnings_found": {
            "type": "integer"
          },
          "nodes_validated": {
            "type": "integer"
          },
          "edges_validated": {
            "type": "integer"
          }
        }
      }
    },
    "lifecycle_hooks": {
      "health_check": "health_check"
    }
  },
  "security": {
    "permissions": [
      "file_read",
      "file_write"
    ],
    "sandbox_compatible": true,
    "mtls_required": true
  }
}
```
  ```
  --- END OF FILE scribe/actions\graph_validation_action\manifest.json ---
  --- START OF FILE scribe/actions\naming_enforcement_action.py ---
  ```
  ```py
# tools/scribe/actions/naming_enforcement_action.py
import logging
from pathlib import Path
import sys
from typing import Dict, Any

from .base import BaseAction, ActionExecutionError

try:
    from tools.naming_enforcer.naming_enforcer import NamingEnforcerV2, SafetyLogger
except ImportError:
    # Fallback for path issues
    try:
        sys.path.append(str(Path(__file__).resolve().parent.parent.parent))
        from tools.naming_enforcer.naming_enforcer import NamingEnforcerV2, SafetyLogger
    except ImportError:
        # Create mock classes for testing/development
        class NamingEnforcerV2:
            def __init__(self, *args, **kwargs):
                pass
            def enforce_naming(self, *args, **kwargs):
                return {"status": "skipped", "reason": "NamingEnforcer not available"}
        
        class SafetyLogger:
            def __init__(self, *args, **kwargs):
                pass
            def info(self, *args, **kwargs):
                pass
            def error(self, *args, **kwargs):
                pass

class NamingEnforcementAction(BaseAction):
    def __init__(self, action_type: str, params: Dict[str, Any], plugin_context: 'PluginContextPort'):
        super().__init__(action_type, params, plugin_context)
        
        # HMA v2.2 compliant port-based access
        config_port = self.context.get_port("configuration")
        self.log_port = self.context.get_port("logging")
        
        self.scan_paths_str = self.params.get("scan_paths", ["."]) # List of paths relative to repo_root
        self.schema_registry_path_str = self.params.get("schema_registry_path", "standards/registry/schema-registry.jsonld")
        self.fix_mode = self.params.get("fix_mode", False)
        # Naming Enforcer V2 auto-detects .namingignore, so explicit path might not be needed unless overridden
        self.namingignore_path_str = self.params.get("namingignore_path", None)
        self.enforcer_instance = None
        self.safety_logger_instance = None
        
        # Get repo root through configuration port
        try:
            self.repo_root = Path(config_port.get_config_value("repo_root", self.context.get_plugin_id(), "."))
        except Exception as e:
            self.log_port.log_warning("Could not get repo_root from config, using current directory", error=str(e))
            self.repo_root = Path(".")

    def setup(self):
        if not super().setup():
            return False

        self.scan_paths = [self.repo_root / Path(p) for p in self.scan_paths_str]
        self.schema_registry_file = self.repo_root / self.schema_registry_path_str

        for sp in self.scan_paths:
            if not sp.exists():
                self.logger.error(f"Scan path not found: {sp}")
                return False

        if not self.schema_registry_file.exists():
            self.logger.error(f"Schema registry for naming enforcer not found: {self.schema_registry_file}")
            return False

        try:
            # NamingEnforcerV2's standard_path expects the path to the schema registry directly now
            self.enforcer_instance = NamingEnforcerV2(standard_path=str(self.schema_registry_file))

            # Configure include/exclude managers if paths are provided via action_config
            # NamingEnforcerV2's _load_automatic_files loads .namingignore from repo root or tool dir.
            # If specific ignore/include files are needed for this action, load them here.
            # For now, rely on its automatic detection or future config enhancements.
            if self.namingignore_path_str:
                 ignore_file = self.repo_root / self.namingignore_path_str
                 if ignore_file.exists():
                     self.enforcer_instance.exclude_manager.load_exclude_file(ignore_file)
                     self.logger.info(f"Loaded custom .namingignore from: {ignore_file}")
                 else:
                    self.logger.warning(f"Custom .namingignore file not found: {ignore_file}")

            # Setup SafetyLogger for fix mode
            if self.fix_mode:
                operation_name = f"scribe_naming_fix_{self.global_config.get('run_id', 'default')}"
                # SafetyLogger's __init__ sets up logging handlers.
                # Ensure this doesn't conflict with Scribe's main logger if Scribe has one.
                # It might be better if SafetyLogger could accept an existing logger.
                # For now, let it create its own as per its design.
                self.safety_logger_instance = SafetyLogger(operation_name=operation_name)
                self.logger.info(f"SafetyLogger initialized for fix mode: {operation_name}")

        except Exception as e:
            self.logger.exception(f"Failed to initialize NamingEnforcerV2: {e}")
            return False

        self.logger.info("NamingEnforcementAction setup complete.")
        return True

    def execute(self, file_content: str, match, file_path: str, params: Dict[str, Any]) -> str:
        self.logger.info(f"Executing NamingEnforcementAction. Context: {params}. Fix mode: {self.fix_mode}")
        if not self.enforcer_instance:
            self.logger.error("NamingEnforcerV2 instance not available. Setup might have failed.")
            raise ActionExecutionError(self.action_type, "Setup failed: NamingEnforcerV2 not initialized.")

        all_violations = []
        total_violations_count = 0
        scan_paths_to_process = self.scan_paths

        # Optionally refine scan_paths based on execution_context (e.g., changed_files)
        if "changed_files" in params and params["changed_files"]:
            # A more sophisticated approach might be needed here.
            # For now, if changed_files is present, we'll scan the unique parent directories.
            # Or, if NamingEnforcerV2 can take a list of files, that would be better.
            # Current NamingEnforcerV2.scan_directory takes a root_path.
            # We'll stick to the configured scan_paths but log the context.
            self.logger.info(f"Received changed_files in context: {params['changed_files']}. Scanning configured root paths.")


        for scan_path_root in scan_paths_to_process:
            self.logger.info(f"Scanning path: {scan_path_root}")
            # NamingEnforcerV2.scan_directory populates self.enforcer_instance.violations
            self.enforcer_instance.scan_directory(scan_path_root)
            all_violations.extend(self.enforcer_instance.violations) # Accumulate violations
            # Clear violations for next scan path if NamingEnforcerV2 accumulates them internally on instance
            self.enforcer_instance.violations = []

        total_violations_count = len(all_violations)
        # The NamingEnforcerV2.print_report() prints to stdout. We might want to capture this.
        # For Scribe, returning structured data is better.

        fixes_applied_count = 0
        if total_violations_count > 0:
            if self.fix_mode:
                if not self.safety_logger_instance:
                    self.logger.error("SafetyLogger not initialized for fix mode. Aborting fixes.")
                    return {'status': 'failure', 'message': "Fix mode error: SafetyLogger not initialized."}

                try:
                    self.logger.info(f"Attempting to fix {total_violations_count} violations.")
                    # Populate violations for the enforcer instance again, as they were cleared per scan_path_root loop
                    self.enforcer_instance.violations = all_violations
                    self.enforcer_instance.build_rename_operations() # Uses self.enforcer_instance.violations

                    # Backup files before modification (SafetyLogger handles this if files_to_backup is passed)
                    # This logic needs to be robust in NamingEnforcerV2 or adapted here.
                    # For now, assuming NamingEnforcerV2's existing backup logic within apply_... methods is sufficient.
                    # The SafetyLogger is passed to apply methods in NamingEnforcerV2.
                    # Here, we'd call the top-level fix application method of NamingEnforcerV2.

                    # The NamingEnforcerV2's main() has the fix logic. We need to replicate parts of it.
                    # NamingEnforcerV2.apply_frontmatter_fixes, .apply_content_updates, .apply_file_renames

                    fm_fixes = self.enforcer_instance.apply_frontmatter_fixes(self.safety_logger_instance, dry_run=False)
                    content_updates = self.enforcer_instance.apply_content_updates(self.safety_logger_instance, dry_run=False)
                    file_renames = self.enforcer_instance.apply_file_renames(self.safety_logger_instance, dry_run=False)
                    fixes_applied_count = fm_fixes + content_updates + file_renames # This is an approximation of "fixes"

                    self.safety_logger_instance.finalize_operation()
                    self.logger.info(f"Fix mode complete. Approximate fixes made: {fixes_applied_count}")
                    # Re-scan to confirm fixes (optional, or trust the fix counts)
                    # self.enforcer_instance.violations = []
                    # for scan_path_root in scan_paths_to_process:
                    #    self.enforcer_instance.scan_directory(scan_path_root)
                    # total_violations_count = len(self.enforcer_instance.violations)

                except Exception as e:
                    self.logger.exception(f"Error during fix application: {e}")
                    if self.safety_logger_instance: self.safety_logger_instance.finalize_operation()
                    raise ActionExecutionError(self.action_type, f"Error applying fixes: {e}", original_error=e)

            status = 'failure_violations_found' if total_violations_count > 0 and not self.fix_mode else 'success'
            if self.fix_mode and total_violations_count > 0 and fixes_applied_count < total_violations_count : # If fixes were attempted but some violations remain
                status = 'failure_violations_found' # Or a specific status like 'success_with_remaining_violations'
                raise ActionExecutionError(self.action_type, f"Naming enforcement scan complete. Violations found: {total_violations_count}. Fixes applied: {fixes_applied_count if self.fix_mode else 0}.")


            return file_content
        else:
            return file_content

# Example usage (for testing)
if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger("NamingEnforcementActionTest")

    mock_global_config = {
        "repo_root": str(Path(__file__).resolve().parent.parent.parent)
    }
    # Create a dummy test file with a violation for testing fix_mode
    test_dir = Path(mock_global_config["repo_root"]) / "temp_ne_test"
    test_dir.mkdir(exist_ok=True)
    violating_file = test_dir / "BadName.txt"
    with open(violating_file, "w") as f:
        f.write("test")

    mock_action_config_scan = {
        "scan_paths": ["temp_ne_test"], # Scan the temp directory
        "fix_mode": False
    }
    mock_action_config_fix = {
        "scan_paths": ["temp_ne_test"],
        "fix_mode": True
    }

    action_scan = NamingEnforcementAction(action_config=mock_action_config_scan, global_config=mock_global_config, logger=logger)
    if action_scan.setup():
        logger.info("--- SCAN MODE TEST ---")
        result_scan = action_scan.execute({})
        logger.info(f"Scan Test Result: {result_scan}")

    action_fix = NamingEnforcementAction(action_config=mock_action_config_fix, global_config=mock_global_config, logger=logger)
    if action_fix.setup():
        logger.info("--- FIX MODE TEST ---")
        result_fix = action_fix.execute({})
        logger.info(f"Fix Test Result: {result_fix}")

    # Cleanup
    import shutil
    shutil.rmtree(test_dir, ignore_errors=True)
    # SafetyLogger creates logs in master-knowledge-base/tools/reports, clean them up too if needed
    # For this test, we'll leave them.
    logger.info(f"Cleaned up test directory: {test_dir}")

```
  ```
  --- END OF FILE scribe/actions\naming_enforcement_action.py ---
  --- START OF FILE scribe/actions\naming_enforcement_action\manifest.json ---
  ```
  ```json
{
  "manifest_version": "2.2",
  "plugin_metadata": {
    "name": "naming_enforcement_action",
    "version": "2.2.0",
    "description": "Enforces naming conventions using NamingEnforcerV2 with schema registry and safety logging",
    "author": "Scribe Engine Development Team",
    "license": "MIT",
    "type": "L3-Capability"
  },
  "hma_compliance": {
    "hma_version": "2.2",
    "tier_classification": {
      "mandatory": [
        "json_schema",
        "otel_boundary",
        "mtls"
      ],
      "recommended": [
        "structured_logging",
        "health_checks",
        "kubernetes"
      ],
      "alternative": []
    },
    "boundary_interfaces": [
      {
        "port_type": "PluginExecutionPort",
        "direction": "inbound",
        "validation": "json_schema",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "EventBusPort",
        "direction": "outbound",
        "validation": "event_schema",
        "telemetry": "otel_spans"
      }
    ]
  },
  "runtime_requirements": {
    "python_version": ">=3.8",
    "dependencies": {
      "required": [
        {
          "name": "structlog",
          "version": ">=23.1.0",
          "tier": "recommended"
        }
      ],
      "optional": [
        {
          "name": "rdflib",
          "version": ">=6.0.0",
          "feature": "Schema registry JSON-LD support"
        }
      ]
    },
    "platform_support": [
      "windows",
      "linux",
      "macos"
    ],
    "resource_limits": {
      "max_memory_mb": 200,
      "max_cpu_percent": 30,
      "max_file_handles": 100
    }
  },
  "interface_contracts": {
    "action_interface": {
      "entry_point": "naming_enforcement_action.NamingEnforcementAction",
      "configuration_schema": {
        "type": "object",
        "properties": {
          "scan_paths": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "default": [
              "."
            ],
            "description": "Paths to scan for naming violations relative to repo root"
          },
          "schema_registry_path": {
            "type": "string",
            "default": "standards/registry/schema-registry.jsonld",
            "description": "Path to schema registry file"
          },
          "fix_mode": {
            "type": "boolean",
            "default": false,
            "description": "Whether to automatically fix naming violations"
          },
          "namingignore_path": {
            "type": "string",
            "description": "Optional path to .namingignore file override"
          }
        }
      },
      "event_patterns": [
        {
          "event_type": "file_created",
          "file_patterns": [
            "*"
          ],
          "priority": 8
        },
        {
          "event_type": "file_moved",
          "file_patterns": [
            "*"
          ],
          "priority": 8
        }
      ],
      "output_schema": {
        "type": "object",
        "properties": {
          "violations_found": {
            "type": "integer"
          },
          "violations_fixed": {
            "type": "integer"
          },
          "status": {
            "type": "string",
            "enum": [
              "success",
              "partial",
              "failed",
              "skipped"
            ]
          },
          "scan_duration": {
            "type": "number"
          }
        }
      }
    },
    "lifecycle_hooks": {
      "on_load": "setup",
      "health_check": "health_check"
    }
  },
  "security": {
    "permissions": [
      "file_read",
      "file_write",
      "file_delete"
    ],
    "sandbox_compatible": true,
    "mtls_required": true
  }
}
```
  ```
  --- END OF FILE scribe/actions\naming_enforcement_action\manifest.json ---
  --- START OF FILE scribe/actions\reconciliation_action.py ---
  ```
  ```py
# tools/scribe/actions/reconciliation_action.py
import json
import os
import datetime
import yaml # Requires PyYAML
import hashlib
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Set, Tuple

from .base import BaseAction, ActionExecutionError
from tools.scribe.utils.frontmatter_parser import parse_frontmatter

# Helper functions (adapted from tools/indexer/generate_index.py)
# These can be static methods, part of a helper class, or module-level functions

def _calculate_content_hash(file_content: str) -> str:
    return hashlib.sha256(file_content.encode('utf-8')).hexdigest()

def _create_node_from_file(filepath_rel_to_repo: str, file_content: str, file_stats: os.stat_result) -> Dict[str, Any]:
    frontmatter = parse_frontmatter(file_content)
    content_hash = _calculate_content_hash(file_content)

    node = {
        "@type": "kb:Document",
        "@id": f"kb:doc-{filepath_rel_to_repo.replace('/', '-').replace('.md', '')}",
        "kb:filepath": filepath_rel_to_repo,
        "kb:contentHash": content_hash,
        "kb:fileSize": len(file_content),
        "kb:lastModified": datetime.datetime.fromtimestamp(file_stats.st_mtime, datetime.timezone.utc).isoformat(),
        "kb:indexed": datetime.datetime.now(datetime.timezone.utc).isoformat()
    }

    if frontmatter:
        for key, value in frontmatter.items():
            processed_key = key
            if key == '@id':
                node['@id'] = str(value)
                continue
            elif key == '@type':
                node['@type'] = str(value)
                continue
            elif ':' not in key:
                processed_key = f"kb:{key.replace('-', '_')}"
            elif key.startswith('kb:') and '-' in key:
                prefix, local_name = key.split(':', 1)
                processed_key = f"{prefix}:{local_name.replace('-', '_')}"

            if isinstance(value, datetime.datetime):
                node[processed_key] = value.isoformat()
            elif isinstance(value, datetime.date):
                node[processed_key] = value.isoformat() + "T00:00:00Z"
            else:
                node[processed_key] = value
    return node

class ReconciliationAction(BaseAction):
    def __init__(self, action_type: str, params: Dict[str, Any], plugin_context: 'PluginContextPort'):
        super().__init__(action_type, params, plugin_context)
        
        # HMA v2.2 compliant port-based access
        config_port = self.context.get_port("configuration")
        self.log_port = self.context.get_port("logging")
        
        self.master_index_path_str = self.params.get("master_index_path", "standards/registry/master-index.jsonld")
        self.kb_root_dirs_str = self.params.get("kb_root_dirs", ["."]) # Scan whole repo by default relative to repo_root
        self.exclude_dirs_set = set(self.params.get("exclude_dirs",
            ['.git', 'node_modules', '__pycache__', '.vscode', 'archive', 'tools', 'temp-naming-enforcer-test']))
        
        # Get repo root through configuration port
        try:
            self.repo_root = Path(config_port.get_config_value("repo_root", self.context.get_plugin_id(), "."))
        except Exception as e:
            self.log_port.log_warning("Could not get repo_root from config, using current directory", error=str(e))
            self.repo_root = Path(".")


    def setup(self):

        self.master_index_file = self.repo_root / self.master_index_path_str
        self.kb_root_paths = [self.repo_root / Path(p) for p in self.kb_root_dirs_str]

        for p_path in self.kb_root_paths:
             if not p_path.exists() or not p_path.is_dir():
                self.logger.error(f"Knowledge base scan directory not found or not a directory: {p_path}")
                return False
        self.logger.info(f"ReconciliationAction setup complete. Master index: {self.master_index_file}")
        return True

    def _load_existing_index(self) -> Dict[str, Any]:
        if self.master_index_file.exists():
            try:
                with open(self.master_index_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                self.logger.warning(f"Could not load existing index: {e}. Creating new index.")

        return {
            "@context": ["contexts/base.jsonld", "contexts/fields.jsonld"], # Relative to registry
            "@type": "kb:MasterIndex", "@id": "kb:master-index",
            "kb:schemaVersion": "1.0.0", "kb:title": "Knowledge Base Master Index",
            "kb:created": datetime.datetime.now(datetime.timezone.utc).isoformat(),
            "kb:modified": datetime.datetime.now(datetime.timezone.utc).isoformat(),
            "kb:version": "1.0.0", "kb:documents": []
        }

    def _scan_knowledge_base(self) -> Dict[str, Dict[str, Any]]:
        found_files = {}
        for root_dir_path in self.kb_root_paths:
            self.logger.info(f"Scanning for .md files in: {root_dir_path}")
            for md_file in root_dir_path.rglob('*.md'):
                # Check if any part of the path is in exclude_dirs_set
                if any(excluded_part in md_file.relative_to(self.repo_root).parts for excluded_part in self.exclude_dirs_set):
                    continue

                rel_path_posix = md_file.relative_to(self.repo_root).as_posix()
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    found_files[rel_path_posix] = {'content': content, 'stats': md_file.stat()}
                except (IOError, UnicodeDecodeError) as e:
                    self.logger.warning(f"Could not read file {rel_path_posix}: {e}")
        self.logger.info(f"Scan found {len(found_files)} markdown files.")
        return found_files

    def _reconcile_index_logic(self, existing_index: Dict[str, Any], current_files: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, int]]:
        stats = {'added': 0, 'updated': 0, 'removed': 0, 'unchanged': 0}
        existing_docs_map = {doc['kb:filepath']: doc for doc in existing_index.get('kb:documents', [])}
        new_documents_list = []

        for filepath, file_info in current_files.items():
            content_hash = _calculate_content_hash(file_info['content'])
            if filepath in existing_docs_map:
                existing_doc = existing_docs_map[filepath]
                if existing_doc.get('kb:contentHash') != content_hash:
                    new_documents_list.append(_create_node_from_file(filepath, file_info['content'], file_info['stats']))
                    stats['updated'] += 1
                else:
                    new_documents_list.append(existing_doc)
                    stats['unchanged'] += 1
            else:
                new_documents_list.append(_create_node_from_file(filepath, file_info['content'], file_info['stats']))
                stats['added'] += 1

        current_filepaths_set = set(current_files.keys())
        for filepath in existing_docs_map.keys():
            if filepath not in current_filepaths_set:
                stats['removed'] += 1

        existing_index['kb:documents'] = new_documents_list
        existing_index['kb:modified'] = datetime.datetime.now(datetime.timezone.utc).isoformat()
        existing_index['kb:documentCount'] = len(new_documents_list)
        return existing_index, stats

    def _save_index(self, index_data: Dict[str, Any]) -> bool:
        try:
            self.master_index_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.master_index_file, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, indent=2, ensure_ascii=False)
            return True
        except IOError as e:
            self.logger.error(f"Error writing index file {self.master_index_file}: {e}")
            return False

    def execute(self, file_content: str, match, file_path: str, params: Dict[str, Any]) -> str:
        self.logger.info(f"Executing ReconciliationAction. Context: {params}")
        if not self.setup(): # Call setup if not already called or if it can be re-entrant
             raise ActionExecutionError(self.action_type, "Setup failed.")

        existing_index = self._load_existing_index()
        current_files = self._scan_knowledge_base()

        updated_index, stats = self._reconcile_index_logic(existing_index, current_files)

        if self._save_index(updated_index):
            msg = f"Reconciliation complete. Stats: Added {stats['added']}, Updated {stats['updated']}, Removed {stats['removed']}, Unchanged {stats['unchanged']}."
            self.logger.info(msg)
            return file_content
        else:
            msg = "Reconciliation failed: Could not save master index."
            self.logger.error(msg)
            raise ActionExecutionError(self.action_type, msg)

# Example usage (for testing, not part of the class itself)
if __name__ == '__main__':
    # This part is for direct testing of the action if needed
    # It won't run when Scribe imports the action
    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger("ReconciliationActionTest")

    # Mock global_config and action_config for testing
    mock_global_config = {
        "repo_root": str(Path(__file__).resolve().parent.parent.parent) # Assumes tools/scribe/actions structure
    }
    mock_action_config = {
        "master_index_path": "standards/registry/master-index.jsonld",
        "kb_root_dirs": ["kb", "standards/src"], # Example scan areas
        "exclude_dirs": ['.git', 'node_modules', 'archive', 'tools', 'temp-naming-enforcer-test']
    }

    action = ReconciliationAction(action_type="reconciliation", params=mock_action_config, config_manager=None, security_manager=None) # Mock ConfigManager and SecurityManager

    if action.setup():
        test_context = {} # Provide any necessary execution context
        result = action.execute(test_context)
        logger.info(f"Test execution result: {result}")
    else:
        logger.error("Test setup failed.")

```
  ```
  --- END OF FILE scribe/actions\reconciliation_action.py ---
  --- START OF FILE scribe/actions\reconciliation_action\manifest.json ---
  ```
  ```json
{
  "manifest_version": "2.2",
  "plugin_metadata": {
    "name": "reconciliation_action",
    "version": "2.2.0",
    "description": "Reconciles data inconsistencies and synchronizes knowledge base components",
    "author": "Scribe Engine Development Team",
    "license": "MIT",
    "type": "L3-Capability"
  },
  "hma_compliance": {
    "hma_version": "2.2",
    "tier_classification": {
      "mandatory": [
        "json_schema",
        "otel_boundary",
        "mtls"
      ],
      "recommended": [
        "structured_logging",
        "health_checks",
        "kubernetes"
      ],
      "alternative": []
    },
    "boundary_interfaces": [
      {
        "port_type": "PluginExecutionPort",
        "direction": "inbound",
        "validation": "json_schema",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "EventBusPort",
        "direction": "outbound",
        "validation": "event_schema",
        "telemetry": "otel_spans"
      }
    ]
  },
  "runtime_requirements": {
    "python_version": ">=3.8",
    "dependencies": {
      "required": [
        {
          "name": "structlog",
          "version": ">=23.1.0",
          "tier": "recommended"
        },
        {
          "name": "pyyaml",
          "version": ">=6.0",
          "tier": "mandatory"
        }
      ],
      "optional": [
        {
          "name": "deepdiff",
          "version": ">=6.0.0",
          "feature": "Advanced difference detection"
        }
      ]
    },
    "platform_support": [
      "windows",
      "linux",
      "macos"
    ],
    "resource_limits": {
      "max_memory_mb": 250,
      "max_cpu_percent": 35,
      "max_file_handles": 60
    }
  },
  "interface_contracts": {
    "action_interface": {
      "entry_point": "reconciliation_action.ReconciliationAction",
      "configuration_schema": {
        "type": "object",
        "properties": {
          "source_paths": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "Paths to source files for reconciliation"
          },
          "reconciliation_rules": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "type": {
                  "type": "string",
                  "enum": [
                    "merge",
                    "overwrite",
                    "append",
                    "validate"
                  ]
                },
                "priority": {
                  "type": "integer",
                  "minimum": 1,
                  "maximum": 10
                }
              }
            },
            "description": "Rules for reconciliation process"
          },
          "backup_enabled": {
            "type": "boolean",
            "default": true,
            "description": "Whether to create backups before reconciliation"
          },
          "conflict_resolution": {
            "type": "string",
            "enum": [
              "manual",
              "automatic",
              "prefer_newer",
              "prefer_larger"
            ],
            "default": "manual",
            "description": "Conflict resolution strategy"
          }
        }
      },
      "event_patterns": [
        {
          "event_type": "file_modified",
          "file_patterns": [
            "*.yaml",
            "*.json",
            "*.md"
          ],
          "priority": 4
        }
      ],
      "output_schema": {
        "type": "object",
        "properties": {
          "reconciled_files": {
            "type": "integer"
          },
          "conflicts_resolved": {
            "type": "integer"
          },
          "conflicts_remaining": {
            "type": "integer"
          },
          "backup_created": {
            "type": "boolean"
          }
        }
      }
    },
    "lifecycle_hooks": {
      "health_check": "health_check"
    }
  },
  "security": {
    "permissions": [
      "file_read",
      "file_write",
      "file_delete"
    ],
    "sandbox_compatible": true,
    "mtls_required": true
  }
}
```
  ```
  --- END OF FILE scribe/actions\reconciliation_action\manifest.json ---
  --- START OF FILE scribe/actions\roadmap_populator_action.py ---
  ```
  ```py
import json
import yaml
from pathlib import Path
from .base import BaseAction
from tools.scribe.core.atomic_write import atomic_write
from datetime import datetime
from typing import Dict, Any

class RoadmapPopulatorAction(BaseAction):
    def execute(self, file_content: str, match, file_path: str, params: Dict[str, Any]) -> str:
        # Parse JSON-LD or initialize from template
        jsonld_path = Path(file_path).with_suffix('.jsonld')
        if not jsonld_path.exists():
            # Load v2 template and initialize
            template = self._load_template('roadmap-template-v2.jsonld')
            data = self._populate_initial_data(template, params)
        else:
            with open(jsonld_path, 'r') as f:
                data = json.load(f)
        
        # Add/update timestamps
        timestamp = datetime.now().isoformat()
        data['date-modified'] = timestamp
        if 'date-created' not in data:
            data['date-created'] = timestamp
        
        # Validate structure
        if not self._validate_jsonld(data):
            # Fallback to minimal valid
            data = self._minimal_valid_jsonld(params)
        
        # Save JSON-LD atomically
        atomic_write(jsonld_path, json.dumps(data, indent=2))
        
        # Generate MD twin
        md_content = self._generate_md_twin(data)
        md_path = jsonld_path.with_suffix('.md')
        atomic_write(md_path, md_content)
        
        return file_content  # Original unchanged

    def _load_template(self, template_name: str) -> Dict:
        # Load JSON-LD template
        pass

    def _populate_initial_data(self, template: Dict, params: Dict) -> Dict:
        # Populate with params (e.g., project name, phases)
        pass

    def _validate_jsonld(self, data: Dict) -> bool:
        # Basic SHACL-like validation
        pass

    def _minimal_valid_jsonld(self, params: Dict) -> Dict:
        # Safe fallback structure
        return {'@context': '...', 'phases': []}

    def _generate_md_twin(self, data: Dict) -> str:
        # Convert to styled MD (trees, bars)
        pass 
```
  ```
  --- END OF FILE scribe/actions\roadmap_populator_action.py ---
  --- START OF FILE scribe/actions\roadmap_populator_action\manifest.json ---
  ```
  ```json
{
  "manifest_version": "2.2",
  "plugin_metadata": {
    "name": "roadmap_populator_action",
    "version": "2.2.0",
    "description": "Populates roadmap documents with structured data and cross-references",
    "author": "Scribe Engine Development Team",
    "license": "MIT",
    "type": "L3-Capability"
  },
  "hma_compliance": {
    "hma_version": "2.2",
    "tier_classification": {
      "mandatory": [
        "json_schema",
        "otel_boundary",
        "mtls"
      ],
      "recommended": [
        "structured_logging",
        "health_checks",
        "kubernetes"
      ],
      "alternative": []
    },
    "boundary_interfaces": [
      {
        "port_type": "PluginExecutionPort",
        "direction": "inbound",
        "validation": "json_schema",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "EventBusPort",
        "direction": "outbound",
        "validation": "event_schema",
        "telemetry": "otel_spans"
      }
    ]
  },
  "runtime_requirements": {
    "python_version": ">=3.8",
    "dependencies": {
      "required": [
        {
          "name": "pyyaml",
          "version": ">=6.0",
          "tier": "mandatory"
        },
        {
          "name": "structlog",
          "version": ">=23.1.0",
          "tier": "recommended"
        }
      ],
      "optional": []
    },
    "platform_support": [
      "windows",
      "linux",
      "macos"
    ],
    "resource_limits": {
      "max_memory_mb": 150,
      "max_cpu_percent": 20,
      "max_file_handles": 25
    }
  },
  "interface_contracts": {
    "action_interface": {
      "entry_point": "roadmap_populator_action.RoadmapPopulatorAction",
      "configuration_schema": {
        "type": "object",
        "properties": {
          "roadmap_template_path": {
            "type": "string",
            "description": "Path to roadmap template file"
          },
          "data_source_paths": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "Paths to data source files for population"
          },
          "output_format": {
            "type": "string",
            "enum": [
              "markdown",
              "yaml",
              "json"
            ],
            "default": "markdown",
            "description": "Output format for populated roadmap"
          }
        }
      },
      "event_patterns": [
        {
          "event_type": "file_created",
          "file_patterns": [
            "*roadmap*.md",
            "*roadmap*.yaml"
          ],
          "priority": 6
        },
        {
          "event_type": "file_modified",
          "file_patterns": [
            "*roadmap*.md",
            "*roadmap*.yaml"
          ],
          "priority": 5
        }
      ],
      "output_schema": {
        "type": "object",
        "properties": {
          "populated": {
            "type": "boolean"
          },
          "sections_updated": {
            "type": "integer"
          },
          "cross_references_added": {
            "type": "integer"
          }
        }
      }
    },
    "lifecycle_hooks": {
      "health_check": "health_check"
    }
  },
  "security": {
    "permissions": [
      "file_read",
      "file_write"
    ],
    "sandbox_compatible": true,
    "mtls_required": true
  }
}
```
  ```
  --- END OF FILE scribe/actions\roadmap_populator_action\manifest.json ---
  --- START OF FILE scribe/actions\run_command_action.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Run Command Action Plugin (HMA v2.2 Compliant)

Executes system commands safely using port-based access.
This action demonstrates the HMA v2.2 ports-and-adapters-only pattern.
"""

import re
from typing import Dict, Any, List
from pathlib import Path

from .base import BaseAction, ActionExecutionError, validate_required_params


class RunCommandAction(BaseAction):
    """
    Action plugin that executes system commands safely using HMA v2.2 ports.
    
    This action uses the CommandExecutionPort to execute commands with proper
    security restrictions and uses list-based command format to prevent
    shell injection attacks.
    
    HMA v2.2 Compliance: All functionality accessed through registered ports.
    """
        
    def get_required_params(self) -> List[str]:
        """Get required parameters for this action."""
        return ["command"]
    
    def get_optional_params(self) -> Dict[str, Any]:
        """Get optional parameters and their defaults."""
        return {
            "cwd": None,
            "timeout": 30,
            "allowed_env_vars": []
        }
    
    def validate_params(self, params: Dict[str, Any]) -> bool:
        """
        Validate parameters for the run command action.
        
        Args:
            params: Parameters to validate
            
        Returns:
            True if parameters are valid
            
        Raises:
            ActionExecutionError: If parameters are invalid
        """
        try:
            # Validate required parameters
            validate_required_params(params, self.get_required_params(), self.action_type)
            
            # Validate command is a list
            command = params.get("command")
            if not isinstance(command, list):
                raise ActionExecutionError(
                    self.action_type, 
                    f"Command must be a list of strings, got {type(command).__name__}"
                )
            
            if len(command) == 0:
                raise ActionExecutionError(
                    self.action_type, 
                    "Command list cannot be empty"
                )
            
            # Validate all command elements are strings
            for i, cmd_part in enumerate(command):
                if not isinstance(cmd_part, str):
                    raise ActionExecutionError(
                        self.action_type,
                        f"Command element {i} must be a string, got {type(cmd_part).__name__}"
                    )
            
            # Validate timeout
            timeout = params.get("timeout", 30)
            if not isinstance(timeout, int) or timeout <= 0:
                raise ActionExecutionError(
                    self.action_type,
                    f"Timeout must be a positive integer, got {timeout}"
                )
            
            # Validate working directory if provided
            cwd = params.get("cwd")
            if cwd is not None:
                if not isinstance(cwd, str):
                    raise ActionExecutionError(
                        self.action_type,
                        f"Working directory must be a string, got {type(cwd).__name__}"
                    )
                
                cwd_path = Path(cwd)
                if not cwd_path.exists():
                    raise ActionExecutionError(
                        self.action_type,
                        f"Working directory does not exist: {cwd}"
                    )
                
                if not cwd_path.is_dir():
                    raise ActionExecutionError(
                        self.action_type,
                        f"Working directory is not a directory: {cwd}"
                    )
            
            return True
            
        except ActionExecutionError:
            raise
        except Exception as e:
            raise ActionExecutionError(
                self.action_type,
                f"Parameter validation failed: {e}"
            )
    
    async def execute(self, 
                      file_content: str, 
                      match: re.Match, 
                      file_path: str, 
                      params: Dict[str, Any]) -> str:
        """
        Execute a system command safely using HMA v2.2 ports.
        
        Args:
            file_content: The full content of the file being processed
            match: The regex match object that triggered the rule
            file_path: The path to the file being processed
            params: Action parameters including command list
            
        Returns:
            The original file content (this action doesn't modify files)
            
        Raises:
            ActionExecutionError: If command execution fails
        """
        try:
            # Validate parameters
            self.validate_params(params)
            
            # Extract parameters
            command_list = params["command"]
            cwd = params.get("cwd")
            timeout = params.get("timeout", 30)
            allowed_env_vars = params.get("allowed_env_vars")
            
            # Log command execution using logging port
            self.log_port.log_info("Executing command via port",
                                  command=command_list,
                                  file_path=file_path,
                                  cwd=cwd,
                                  timeout=timeout,
                                  plugin_id=self.context.get_plugin_id())
            
            # Execute command safely through command execution port
            success, stdout, stderr = await self.execute_command_safely(
                command_list=command_list,
                cwd=cwd,
                timeout=timeout,
                allowed_env_vars=allowed_env_vars
            )
            
            if success:
                self.log_port.log_info("Command executed successfully via port",
                                      command=command_list,
                                      stdout_length=len(stdout),
                                      stderr_length=len(stderr))
                
                # Publish success event
                await self.publish_event(
                    "command_executed",
                    {
                        "command": command_list,
                        "success": True,
                        "file_path": file_path,
                        "plugin_id": self.context.get_plugin_id()
                    }
                )
            else:
                self.log_port.log_warning("Command execution failed via port",
                                         command=command_list,
                                         stdout=stdout,
                                         stderr=stderr)
                
                # Publish failure event
                await self.publish_event(
                    "command_failed",
                    {
                        "command": command_list,
                        "success": False,
                        "error": stderr,
                        "file_path": file_path,
                        "plugin_id": self.context.get_plugin_id()
                    }
                )
                
                # Optionally raise an error on command failure
                raise ActionExecutionError(
                    self.action_type,
                    f"Command failed with stderr: {stderr}"
                )
            
            # Return original content (this action doesn't modify files)
            return file_content
            
        except ActionExecutionError:
            raise
            
        except Exception as e:
            self.log_port.log_error("Unexpected error during command execution",
                                   command=params.get("command"),
                                   error=str(e),
                                   plugin_id=self.context.get_plugin_id())
            
            # Publish error event
            await self.publish_event(
                "command_error",
                {
                    "command": params.get("command"),
                    "error": str(e),
                    "file_path": file_path,
                    "plugin_id": self.context.get_plugin_id()
                }
            )
            
            raise ActionExecutionError(
                self.action_type,
                f"Unexpected error: {e}"
            )
    
    def get_description(self) -> str:
        """Get description of this action."""
        return "Executes system commands safely using list-based command format" 
```
  ```
  --- END OF FILE scribe/actions\run_command_action.py ---
  --- START OF FILE scribe/actions\run_command_action\manifest.json ---
  ```
  ```json
{
  "manifest_version": "2.2",
  "plugin_metadata": {
    "name": "run_command_action",
    "version": "2.2.0",
    "type": "L3-Capability",
    "description": "Executes system commands safely using HMA v2.2 port-based access with shell injection protection",
    "author": "Scribe Engine Development Team",
    "license": "MIT"
  },
  "hma_compliance": {
    "hma_version": "2.2",
    "tier_classification": {
      "mandatory": [
        "json_schema",
        "otel_boundary",
        "mtls"
      ],
      "recommended": [
        "structured_logging",
        "health_checks",
        "kubernetes"
      ],
      "alternative": []
    },
    "boundary_interfaces": [
      {
        "port_type": "PluginExecutionPort",
        "direction": "inbound",
        "validation": "json_schema",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "CommandExecutionPort",
        "direction": "outbound",
        "validation": "security_policy",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "LoggingPort",
        "direction": "outbound",
        "validation": "structured_format",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "EventBusPort",
        "direction": "outbound",
        "validation": "event_schema",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "PluginContextPort",
        "direction": "inbound",
        "validation": "context_schema",
        "telemetry": "otel_spans"
      }
    ]
  },
  "runtime_requirements": {
    "python_version": ">=3.8",
    "dependencies": {
      "required": [
        {
          "name": "structlog",
          "version": ">=23.1.0",
          "tier": "recommended"
        }
      ],
      "optional": []
    },
    "platform_support": [
      "windows",
      "linux",
      "macos"
    ],
    "resource_limits": {
      "max_memory_mb": 100,
      "max_cpu_percent": 25,
      "max_file_handles": 10
    }
  },
  "interface_contracts": {
    "action_interface": {
      "entry_point": "run_command_action.RunCommandAction",
      "configuration_schema": {
        "type": "object",
        "required": [
          "command"
        ],
        "properties": {
          "command": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "minItems": 1,
            "description": "Command and arguments as array to prevent shell injection"
          },
          "working_directory": {
            "type": "string",
            "description": "Working directory for command execution"
          },
          "timeout": {
            "type": "integer",
            "minimum": 1,
            "maximum": 3600,
            "default": 300,
            "description": "Command timeout in seconds"
          },
          "capture_output": {
            "type": "boolean",
            "default": true,
            "description": "Whether to capture command output"
          }
        }
      },
      "event_patterns": [
        {
          "event_type": "file_created",
          "file_patterns": [
            "*.trigger",
            "*.cmd"
          ],
          "priority": 5
        },
        {
          "event_type": "file_modified",
          "file_patterns": [
            "*.trigger",
            "*.cmd"
          ],
          "priority": 5
        }
      ],
      "output_schema": {
        "type": "object",
        "properties": {
          "exit_code": {
            "type": "integer"
          },
          "stdout": {
            "type": "string"
          },
          "stderr": {
            "type": "string"
          },
          "execution_time": {
            "type": "number"
          }
        }
      }
    },
    "lifecycle_hooks": {
      "health_check": "health_check"
    }
  },
  "security": {
    "permissions": [
      "file_read",
      "process_spawn",
      "environment_access"
    ],
    "sandbox_compatible": false,
    "mtls_required": true
  }
}
```
  ```
  --- END OF FILE scribe/actions\run_command_action\manifest.json ---
  --- START OF FILE scribe/actions\view_generation_action.py ---
  ```
  ```py
# tools/scribe/actions/view_generation_action.py
import logging
from pathlib import Path
import json
import yaml # Requires PyYAML
from datetime import datetime
import sys
from typing import Dict, Any, Optional

from .base import BaseAction, ActionExecutionError

# Core logic adapted from tools/view_generator.py
# These can be static methods or part of the class.

def _load_json_file(file_path: Path, logger: logging.Logger) -> Optional[dict]:
    if not file_path.exists():
        logger.error(f"File not found: {file_path}")
        return None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from {file_path}: {e}")
        return None
    except IOError as e:
        logger.error(f"IOError reading {file_path}: {e}")
        return None

def _generate_markdown_view_standard(standard_data: dict, schema_registry: dict, logger: logging.Logger) -> str:
    if not standard_data:
        return "Error: Standard data is empty."

    md_lines = []
    title = standard_data.get("kb:title", "Unknown Standard")
    standard_id_val = standard_data.get("kb:standard_id", standard_data.get("@id", "Unknown ID"))

    md_lines.append(f"# View: {title} ({standard_id_val})")
    md_lines.append(f"Generated: {datetime.now().isoformat()}")
    md_lines.append("\n---")

    md_lines.append("## Fields")
    md_lines.append("| Field Name | Value | Description (from Schema) |")
    md_lines.append("|------------|-------|---------------------------|")

    schema_field_map = {
        defn.get('kb:fieldName'): defn
        for defn in schema_registry.get("kb:fieldDefinitions", [])
    }

    for key, value in standard_data.items():
        if key in ["@id", "@type", "kb:contentHash", "kb:fileSize", "kb:lastModified", "kb:indexed"]: # Skip some internal fields for this view
            continue

        field_desc = "N/A"
        simple_key = key.split("kb:")[-1].replace("_", "-") if key.startswith("kb:") else key
        field_def = schema_field_map.get(simple_key)
        if field_def:
            field_desc = field_def.get("kb:description", "No description in schema.")

        val_str = str(value)
        if isinstance(value, list):
            val_str = "\n".join([f"- {v}" for v in value])
        elif isinstance(value, dict):
            try:
                val_str = f"```yaml\n{yaml.dump(value, indent=2, sort_keys=False, allow_unicode=True)}```"
            except yaml.YAMLError:
                val_str = json.dumps(value, indent=2, ensure_ascii=False)

        # Escape pipes in value string for Markdown table
        if isinstance(val_str, str):
            val_str = val_str.replace("|", "\\|")


        md_lines.append(f"| `{key}` | {val_str} | {field_desc} |")

    # Optionally add raw data if desired, or keep view cleaner
    # md_lines.append("\n## Raw JSON-LD Data")
    # md_lines.append("```json")
    # md_lines.append(json.dumps(standard_data, indent=2, ensure_ascii=False))
    # md_lines.append("```")

    return "\n".join(md_lines)

def _generate_yaml_view_standard(standard_data: dict, logger: logging.Logger) -> str:
    if not standard_data:
        return "# Error: Standard data is empty."
    try:
        # Return a subset of fields for a cleaner "change request" base
        view_data = {k: v for k, v in standard_data.items() if not k.startswith("kb:contentHash") and not k.startswith("kb:fileSize") and not k.startswith("kb:lastModified") and not k.startswith("kb:indexed")}
        return yaml.dump(view_data, sort_keys=False, indent=2, allow_unicode=True)
    except yaml.YAMLError as e:
        logger.error(f"Error encoding YAML: {e}")
        return f"# Error encoding YAML: {e}"


class ViewGenerationAction(BaseAction):
    def __init__(self, action_type: str, params: Dict[str, Any], plugin_context: 'PluginContextPort'):
        super().__init__(action_type, params, plugin_context)
        
        # HMA v2.2 compliant port-based access
        config_port = self.context.get_port("configuration")
        self.log_port = self.context.get_port("logging")
        
        self.schema_registry_path_str = self.params.get("schema_registry_path", "standards/registry/schema-registry.jsonld")
        self.master_index_path_str = self.params.get("master_index_path", "standards/registry/master-index.jsonld")
        self.schema_registry = None
        self.master_index = None
        
        # Get repo root through configuration port
        try:
            self.repo_root = Path(config_port.get_config_value("repo_root", self.context.get_plugin_id(), "."))
        except Exception as e:
            self.log_port.log_warning("Could not get repo_root from config, using current directory", error=str(e))
            self.repo_root = Path(".")


    def setup(self):
        if not super().setup():
            return False

        schema_file = self.repo_root / self.schema_registry_path_str
        master_index_file = self.repo_root / self.master_index_path_str

        self.schema_registry = _load_json_file(schema_file, self.logger)
        if not self.schema_registry:
            self.logger.error(f"Failed to load schema registry: {schema_file}")
            return False

        self.master_index = _load_json_file(master_index_file, self.logger)
        if not self.master_index:
            self.logger.error(f"Failed to load master index: {master_index_file}")
            return False

        self.logger.info("ViewGenerationAction setup complete.")
        return True

    def execute(self, file_content: str, match, file_path: str, params: Dict[str, Any]) -> str:
        self.logger.info(f"Executing ViewGenerationAction. Context: {params}")

        entity_id = params.get("entity_id")
        view_type = params.get("view_type")
        output_path_str = params.get("output_path") # Relative to repo_root or absolute

        if not entity_id or not view_type:
            raise ActionExecutionError(self.action_type, "Missing 'entity_id' or 'view_type' in execution_context.")
        if view_type not in ['md', 'yaml']:
            raise ActionExecutionError(self.action_type, f"Invalid 'view_type': {view_type}. Must be 'md' or 'yaml'.")

        target_standard_data = None
        if self.master_index.get("kb:documents"):
            for doc in self.master_index["kb:documents"]:
                if doc.get("kb:standard_id") == entity_id or doc.get("@id") == entity_id:
                    target_standard_data = doc
                    break
                generated_id_suffix = entity_id.replace('/', '-').replace('.md', '') # Handle if filepath-like ID is passed
                if doc.get("@id") == f"kb:doc-{generated_id_suffix}":
                    target_standard_data = doc
                    break

        if not target_standard_data:
            msg = f"Entity with ID '{entity_id}' not found in master index."
            self.logger.error(msg)
            raise ActionExecutionError(self.action_type, msg)

        self.logger.info(f"Found entity: {target_standard_data.get('kb:title', entity_id)}")

        output_content = ""
        if view_type == 'md':
            self.logger.info("Generating Markdown view...")
            output_content = _generate_markdown_view_standard(target_standard_data, self.schema_registry, self.logger)
        elif view_type == 'yaml':
            self.logger.info("Generating YAML view...")
            output_content = _generate_yaml_view_standard(target_standard_data, self.logger)

        if output_path_str:
            try:
                # output_path should be relative to repo_root if not absolute
                actual_output_path = self.repo_root / output_path_str
                actual_output_path.parent.mkdir(parents=True, exist_ok=True)
                with open(actual_output_path, 'w', encoding='utf-8') as f:
                    f.write(output_content)
                self.logger.info(f"View successfully written to: {actual_output_path}")
                return file_content
            except IOError as e:
                self.logger.exception(f"Error writing to output file {actual_output_path}: {e}")
                raise ActionExecutionError(self.action_type, f"Error writing file: {e}", original_error=e)
        else: # Output to context/stdout
            # This case is tricky in the new model. We can't return arbitrary dicts.
            # We will log the content and return the original file content.
            # A more advanced implementation might involve writing to a temp file
            # or using a different mechanism to pass data between actions.
            self.logger.info("No output path specified. Logging generated view content.")
            self.logger.info(f"--- Generated View for {entity_id} ---\n{output_content}")
            return file_content

# Example usage (for testing)
if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger("ViewGenerationActionTest")

    mock_global_config = {
        "repo_root": str(Path(__file__).resolve().parent.parent.parent)
    }
    mock_action_config = {} # No action-specific config for now beyond defaults

    # For this test to run, master-index.jsonld and schema-registry.jsonld must exist
    # at their default locations relative to the mock_repo_root.
    # And master-index.jsonld should contain a document with standard_id 'TEST-SHACL-001'.

    # Ensure dummy files exist for testing
    registry_dir = Path(mock_global_config["repo_root"]) / "standards" / "registry"
    registry_dir.mkdir(parents=True, exist_ok=True)

    if not (registry_dir / "schema-registry.jsonld").exists():
        with open(registry_dir / "schema-registry.jsonld", "w") as f:
            json.dump({"kb:fieldDefinitions": [{"kb:fieldName": "title", "kb:description": "Test Title Desc"}]}, f)
        logger.info("Created dummy schema-registry.jsonld for test.")

    if not (registry_dir / "master-index.jsonld").exists():
        test_doc = {"@id": "kb:doc-test-shacl-001", "kb:standard_id": "TEST-SHACL-001", "kb:title": "Test Document"}
        with open(registry_dir / "master-index.jsonld", "w") as f:
            json.dump({"kb:documents": [test_doc]}, f)
        logger.info("Created dummy master-index.jsonld for test.")


    action = ViewGenerationAction(action_type="view_generation", params=mock_action_config, config_manager=None, security_manager=None) # Mock ConfigManager and SecurityManager

    if action.setup():
        logger.info("--- MD View to STDOUT Test ---")
        md_context = {"entity_id": "TEST-SHACL-001", "view_type": "md"}
        md_result = action.execute(file_content="", match=None, file_path="", params=md_context)
        logger.info(f"MD Result: {md_result.get('status')}")
        if md_result.get('status') == 'success':
             print("MD Output:\n", md_result.get('output_content'))

        logger.info("--- YAML View to File Test ---")
        yaml_context = {"entity_id": "TEST-SHACL-001", "view_type": "yaml", "output_path": "temp_view_output.yaml"}
        yaml_result = action.execute(file_content="", match=None, file_path="", params=yaml_context)
        logger.info(f"YAML Result: {yaml_result.get('status')}, Path: {yaml_result.get('output_path')}")

        # Cleanup
        output_file = Path(mock_global_config["repo_root"]) / "temp_view_output.yaml"
        if output_file.exists():
            output_file.unlink()
            logger.info(f"Cleaned up: {output_file}")
    else:
        logger.error("Test setup failed.")

```
  ```
  --- END OF FILE scribe/actions\view_generation_action.py ---
  --- START OF FILE scribe/actions\view_generation_action\manifest.json ---
  ```
  ```json
{
  "manifest_version": "2.2",
  "plugin_metadata": {
    "name": "view_generation_action",
    "version": "2.2.0",
    "description": "Generates dynamic views and reports from knowledge base data",
    "author": "Scribe Engine Development Team",
    "license": "MIT",
    "type": "L3-Capability"
  },
  "hma_compliance": {
    "hma_version": "2.2",
    "tier_classification": {
      "mandatory": [
        "json_schema",
        "otel_boundary",
        "mtls"
      ],
      "recommended": [
        "structured_logging",
        "health_checks",
        "kubernetes"
      ],
      "alternative": []
    },
    "boundary_interfaces": [
      {
        "port_type": "PluginExecutionPort",
        "direction": "inbound",
        "validation": "json_schema",
        "telemetry": "otel_spans"
      },
      {
        "port_type": "EventBusPort",
        "direction": "outbound",
        "validation": "event_schema",
        "telemetry": "otel_spans"
      }
    ]
  },
  "runtime_requirements": {
    "python_version": ">=3.8",
    "dependencies": {
      "required": [
        {
          "name": "structlog",
          "version": ">=23.1.0",
          "tier": "recommended"
        },
        {
          "name": "pyyaml",
          "version": ">=6.0",
          "tier": "mandatory"
        }
      ],
      "optional": [
        {
          "name": "jinja2",
          "version": ">=3.0.0",
          "feature": "Advanced template rendering"
        },
        {
          "name": "markdown",
          "version": ">=3.4.0",
          "feature": "Markdown processing"
        }
      ]
    },
    "platform_support": [
      "windows",
      "linux",
      "macos"
    ],
    "resource_limits": {
      "max_memory_mb": 200,
      "max_cpu_percent": 25,
      "max_file_handles": 40
    }
  },
  "interface_contracts": {
    "action_interface": {
      "entry_point": "view_generation_action.ViewGenerationAction",
      "configuration_schema": {
        "type": "object",
        "properties": {
          "template_path": {
            "type": "string",
            "description": "Path to view template file"
          },
          "data_sources": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "path": {
                  "type": "string"
                },
                "type": {
                  "type": "string",
                  "enum": [
                    "yaml",
                    "json",
                    "markdown",
                    "csv"
                  ]
                }
              }
            },
            "description": "Data sources for view generation"
          },
          "output_path": {
            "type": "string",
            "description": "Output path for generated view"
          },
          "refresh_interval": {
            "type": "integer",
            "minimum": 0,
            "description": "Auto-refresh interval in seconds (0 = disabled)"
          }
        }
      },
      "event_patterns": [
        {
          "event_type": "file_modified",
          "file_patterns": [
            "*.template",
            "*view*.yaml"
          ],
          "priority": 5
        }
      ],
      "output_schema": {
        "type": "object",
        "properties": {
          "view_generated": {
            "type": "boolean"
          },
          "data_sources_processed": {
            "type": "integer"
          },
          "output_size_bytes": {
            "type": "integer"
          }
        }
      }
    },
    "lifecycle_hooks": {
      "health_check": "health_check"
    }
  },
  "security": {
    "permissions": [
      "file_read",
      "file_write"
    ],
    "sandbox_compatible": true,
    "mtls_required": true
  }
}
```
  ```
  --- END OF FILE scribe/actions\view_generation_action\manifest.json ---
  --- START OF FILE scribe/adapters\README.md ---
  ```
  ```md
# Scribe Adapters - HMA v2.2 Compliance Bridges

This directory contains compliance adapters that implement the HMA v2.2 Tier 3 compliance pattern. These adapters bridge Tier 3 alternative technologies to Tier 1 mandatory standards, ensuring interoperability while preserving advanced capabilities.

## Overview

HMA v2.2 defines a three-tier technology framework:

- **Tier 1 (Mandatory)**: OpenTelemetry, mTLS, JSON Schema
- **Tier 2 (Recommended)**: NATS, Kubernetes, Vault, Structured Logging  
- **Tier 3 (Alternative)**: Specialized technologies with compliance bridges

Compliance adapters in this directory transform Tier 3 outputs into Tier 1 compatible formats, maintaining the benefits of specialized technologies while ensuring system-wide interoperability.

## Adapter Architecture

### Compliance Bridge Pattern

```python
class ComplianceBridge:
    """Base pattern for Tier 3 to Tier 1 transformation."""
    
    def transform_output(self, tier3_output: Any) -> Tier1StandardFormat:
        """Transform specialized output to standard format."""
        pass
    
    def create_compliance_report(self, data: Any) -> HMAComplianceReport:
        """Generate HMA v2.2 compliant report."""
        pass
```

### Bridge Responsibilities

1. **Format Transformation**: Convert proprietary formats to standard formats
2. **Metadata Preservation**: Maintain semantic information during transformation  
3. **Error Handling**: Graceful handling of malformed or incomplete data
4. **Compliance Reporting**: Generate HMA v2.2 compliant reports with full metadata
5. **Bidirectional Support**: Support workflows starting from either technology tier

## Available Adapters

### SHACL to JSON Schema Bridge

**File**: `shacl_adapter.py`  
**Purpose**: Transforms SHACL (RDF-based) validation reports to JSON Schema validation reports  
**Tier Bridge**: 3 (SHACL) â†’ 1 (JSON Schema)

#### Key Components

- `ValidationResult`: Standardized violation structure
- `ComplianceReport`: HMA v2.2 compliant report format  
- `SHACLToJSONSchemaAdapter`: Main bridge implementation

#### Usage Example

```python
from adapters.shacl_adapter import SHACLToJSONSchemaAdapter

adapter = SHACLToJSONSchemaAdapter()

# Transform existing SHACL report
compliance_report = adapter.transform_shacl_report(shacl_graph, conforms)

# End-to-end validation with compliance bridge
compliance_report = adapter.validate_with_compliance_bridge(
    data, shacl_shapes, report_id="frontmatter_validation"
)

# Generate JSON Schema compatible report
json_report = adapter.create_json_schema_report(compliance_report)
```

#### Features

- **Severity Mapping**: SHACL severities mapped to standard levels (ERROR, WARNING, INFO)
- **Metadata Preservation**: Source locations, constraint components, and focus nodes preserved
- **Report Versioning**: Full report metadata with timestamps and version tracking
- **Error Resilience**: Graceful handling of malformed SHACL reports

## Adapter Development

### Creating New Compliance Adapters

1. **Identify Bridge Requirements**: Determine source (Tier 3) and target (Tier 1) formats
2. **Design Transformation Logic**: Plan how to preserve semantic information
3. **Implement Adapter Class**: Follow the compliance bridge pattern
4. **Add Error Handling**: Ensure graceful failure modes
5. **Create Tests**: Comprehensive testing with real-world data
6. **Document Usage**: Clear examples and integration patterns

### Adapter Template

```python
#!/usr/bin/env python3
"""
MyTechnology to JSON Schema Compliance Adapter

Bridges MyTechnology (Tier 3) outputs to JSON Schema validation reports (Tier 1).
"""

import json
import time
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

from ..core.logging_config import get_scribe_logger

logger = get_scribe_logger(__name__)


@dataclass
class MyTechnologyResult:
    """Standardized result from MyTechnology."""
    valid: bool
    message: str
    # ... other fields


class MyTechnologyToJSONSchemaAdapter:
    """Compliance adapter for MyTechnology to JSON Schema bridge."""
    
    def __init__(self, adapter_version: str = "2.2.0"):
        self.adapter_version = adapter_version
        self.logger = get_scribe_logger(f"{__name__}.{self.__class__.__name__}")
    
    def transform_report(self, 
                        source_data: Any,
                        report_id: Optional[str] = None) -> ComplianceReport:
        """Transform MyTechnology output to HMA v2.2 compliant format."""
        # Implementation here
        pass
    
    def get_adapter_info(self) -> Dict[str, Any]:
        """Get adapter capabilities and metadata."""
        return {
            "adapter_name": "MyTechnologyToJSONSchemaAdapter",
            "adapter_version": self.adapter_version,
            "hma_compliance_version": "2.2",
            "tier_bridge": {
                "from_tier": "3_alternative",
                "to_tier": "1_mandatory",
                "from_technology": "MyTechnology",
                "to_technology": "JSON_Schema"
            }
        }
```

### Testing Adapters

```python
import pytest
from unittest.mock import Mock

class TestMyTechnologyAdapter:
    
    def setup_method(self):
        self.adapter = MyTechnologyToJSONSchemaAdapter()
    
    def test_successful_transformation(self):
        source_data = Mock()  # Mock source data
        result = self.adapter.transform_report(source_data)
        
        assert result.overall_valid is not None
        assert result.hma_compliance_version == "2.2"
        assert len(result.violations) >= 0
    
    def test_error_handling(self):
        invalid_data = None
        result = self.adapter.transform_report(invalid_data)
        
        assert result.overall_valid is False
        assert result.error_count > 0
```

## Integration Patterns

### Plugin Integration

Adapters are typically used within L3 Capability Plugins:

```python
class MyValidationAction(BaseAction):
    def __init__(self, action_type, params, plugin_context):
        super().__init__(action_type, params, plugin_context)
        self.adapter = MyTechnologyToJSONSchemaAdapter()
    
    async def execute(self, file_content, match, file_path, params):
        # Use specialized technology
        specialized_result = my_technology_validate(file_content)
        
        # Transform to HMA compliant format
        compliance_report = self.adapter.transform_report(
            specialized_result, 
            report_id=f"validation_{file_path}"
        )
        
        # Log compliance report
        self.log_port.log_info("Validation completed",
                              report_id=compliance_report.report_id,
                              overall_valid=compliance_report.overall_valid)
        
        return file_content
```

### Event Publishing

Compliance reports can be published as events:

```python
# Publish validation event with compliance report
await self.publish_event(
    "validation_completed",
    {
        "file_path": file_path,
        "compliance_report": compliance_report.to_dict(),
        "adapter_used": "MyTechnologyToJSONSchemaAdapter"
    }
)
```

## Performance Considerations

### Caching Strategies

```python
from functools import lru_cache

class OptimizedAdapter:
    
    @lru_cache(maxsize=128)
    def transform_cached(self, data_hash: str, data: Any) -> ComplianceReport:
        """Cache expensive transformations."""
        return self.transform_report(data)
```

### Streaming Processing

```python
async def transform_large_dataset(self, data_stream):
    """Process large datasets in chunks."""
    async for chunk in data_stream:
        compliance_chunk = self.transform_report(chunk)
        yield compliance_chunk
```

## Security Considerations

- **Input Validation**: Validate all inputs before processing
- **Error Information**: Avoid leaking sensitive information in error messages
- **Resource Limits**: Prevent resource exhaustion with large inputs
- **Audit Logging**: Log all transformation activities

### Secure Transformation

```python
def secure_transform(self, source_data: Any) -> ComplianceReport:
    """Securely transform data with validation and sanitization."""
    
    # Validate input structure
    if not self._validate_input(source_data):
        raise ValidationError("Invalid input structure")
    
    # Sanitize sensitive data
    sanitized_data = self._sanitize_data(source_data)
    
    # Perform transformation
    result = self._transform_internal(sanitized_data)
    
    # Audit log the transformation
    self.logger.info("Transformation completed",
                    input_type=type(source_data).__name__,
                    output_valid=result.overall_valid,
                    violation_count=result.total_violations)
    
    return result
```

## Monitoring and Observability

### Metrics Collection

```python
def transform_with_metrics(self, data: Any) -> ComplianceReport:
    """Transform with comprehensive metrics collection."""
    
    start_time = time.time()
    
    try:
        result = self.transform_report(data)
        
        # Record success metrics
        self.telemetry.emit_metric(
            "adapter_transformations_total", 1.0,
            {
                "adapter_type": self.__class__.__name__,
                "status": "success",
                "valid": str(result.overall_valid)
            }
        )
        
        return result
        
    except Exception as e:
        # Record error metrics
        self.telemetry.emit_metric(
            "adapter_transformations_total", 1.0,
            {
                "adapter_type": self.__class__.__name__,
                "status": "error",
                "error_type": type(e).__name__
            }
        )
        raise
    
    finally:
        # Record duration metrics
        duration = (time.time() - start_time) * 1000
        self.telemetry.emit_metric(
            "adapter_transformation_duration_ms", duration,
            {"adapter_type": self.__class__.__name__}
        )
```

## Troubleshooting

### Common Issues

- **Transformation Failures**: Usually due to unexpected input formats
- **Performance Issues**: Large datasets may require streaming or caching
- **Memory Usage**: Complex transformations may consume significant memory
- **Compatibility Issues**: Source technology version changes may break adapters

### Debug Utilities

```python
def debug_transformation(self, data: Any) -> Dict[str, Any]:
    """Debug utility for troubleshooting transformations."""
    
    return {
        "input_type": type(data).__name__,
        "input_size": len(str(data)) if data else 0,
        "adapter_version": self.adapter_version,
        "transformation_steps": self._get_transformation_steps(data),
        "expected_output_format": "HMAComplianceReport"
    }
```

## Future Enhancements

### Planned Adapters

- **GraphQL to OpenAPI**: Transform GraphQL schemas to OpenAPI specifications
- **Prometheus to OpenTelemetry**: Bridge Prometheus metrics to OpenTelemetry format
- **Custom Schema to JSON Schema**: Transform proprietary schemas to JSON Schema

### Enhancement Opportunities

- **Bidirectional Transformation**: Support reverse transformations (Tier 1 â†’ Tier 3)
- **Chain Adapters**: Support adapter pipelines for complex transformations
- **Performance Optimization**: Implement parallel processing for large datasets
- **Schema Evolution**: Support automated schema migration and version handling

## Related Documentation

- [HMA v2.2 Specification](../../HMA v2.2/) - Architecture specification
- [ADR-003: SHACL JSON Bridge](../docs/decisions/ADR-003-shacl-json-bridge.md) - Design rationale
- [Plugin Development](../actions/README.md) - Integration with plugins
- [Core Architecture](../core/README.md) - Core component integration
```
  ```
  --- END OF FILE scribe/adapters\README.md ---
  --- START OF FILE scribe/adapters\__init__.py ---
  ```
  ```py
# HMA v2.2 Compliance Adapters for Tier 3 Technologies
```
  ```
  --- END OF FILE scribe/adapters\__init__.py ---
  --- START OF FILE scribe/adapters\shacl_adapter.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
SHACL to JSON Schema Compliance Adapter

This adapter implements the HMA v2.2 Tier 3 compliance pattern by bridging
SHACL validation (Tier 3 alternative) to standard JSON Schema validation
reports (Tier 1 mandatory). This ensures interoperability and compliance
while allowing the use of specialized validation technologies.
"""

import json
import time
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass
from enum import Enum

from ..core.logging_config import get_scribe_logger

logger = get_scribe_logger(__name__)


class ValidationSeverity(Enum):
    """Standardized validation severity levels."""
    ERROR = "error"
    WARNING = "warning"
    INFO = "info"


@dataclass
class ValidationResult:
    """Standardized validation result structure."""
    valid: bool
    severity: ValidationSeverity
    message: str
    property_path: Optional[str] = None
    source_location: Optional[str] = None
    constraint_component: Optional[str] = None
    focus_node: Optional[str] = None
    value: Optional[Any] = None


@dataclass
class ComplianceReport:
    """HMA v2.2 compliant validation report."""
    report_id: str
    timestamp: float
    validator_type: str
    validator_version: str
    input_format: str
    output_format: str = "json_schema_validation_report"
    hma_compliance_version: str = "2.2"
    
    # Validation results
    overall_valid: bool = True
    total_violations: int = 0
    error_count: int = 0
    warning_count: int = 0
    info_count: int = 0
    
    # Detailed results
    violations: List[ValidationResult] = None
    
    def __post_init__(self):
        if self.violations is None:
            self.violations = []
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            "report_metadata": {
                "report_id": self.report_id,
                "timestamp": self.timestamp,
                "validator_type": self.validator_type,
                "validator_version": self.validator_version,
                "input_format": self.input_format,
                "output_format": self.output_format,
                "hma_compliance_version": self.hma_compliance_version
            },
            "validation_summary": {
                "overall_valid": self.overall_valid,
                "total_violations": self.total_violations,
                "error_count": self.error_count,
                "warning_count": self.warning_count,
                "info_count": self.info_count
            },
            "violations": [
                {
                    "severity": violation.severity.value,
                    "message": violation.message,
                    "property_path": violation.property_path,
                    "source_location": violation.source_location,
                    "constraint_component": violation.constraint_component,
                    "focus_node": violation.focus_node,
                    "value": violation.value
                }
                for violation in self.violations
            ]
        }


class SHACLToJSONSchemaAdapter:
    """
    Compliance adapter that transforms SHACL validation results into 
    HMA v2.2 compliant JSON Schema validation reports.
    
    This adapter enables the use of SHACL (Tier 3 alternative) while 
    maintaining compliance with JSON Schema validation (Tier 1 mandatory).
    """
    
    def __init__(self, adapter_version: str = "2.2.0"):
        """
        Initialize the SHACL compliance adapter.
        
        Args:
            adapter_version: Version of the adapter
        """
        self.adapter_version = adapter_version
        self.logger = get_scribe_logger(f"{__name__}.{self.__class__.__name__}")
        
        self.logger.info("SHACL compliance adapter initialized",
                        adapter_version=adapter_version,
                        hma_compliance="2.2")
    
    def transform_shacl_report(self, 
                              shacl_graph: Any, 
                              conforms: bool,
                              report_id: Optional[str] = None) -> ComplianceReport:
        """
        Transform a SHACL validation report into HMA v2.2 compliant format.
        
        Args:
            shacl_graph: SHACL validation report graph (from pyshacl)
            conforms: Whether the validation passed
            report_id: Optional custom report ID
            
        Returns:
            ComplianceReport: HMA v2.2 compliant validation report
        """
        if report_id is None:
            report_id = f"shacl_validation_{int(time.time() * 1000)}"
        
        # Create compliance report
        report = ComplianceReport(
            report_id=report_id,
            timestamp=time.time(),
            validator_type="shacl",
            validator_version=self.adapter_version,
            input_format="rdf_turtle",
            overall_valid=conforms
        )
        
        try:
            # Extract violations from SHACL graph
            violations = self._extract_shacl_violations(shacl_graph)
            
            # Transform violations to standardized format
            for violation in violations:
                standardized_violation = self._transform_violation(violation)
                report.violations.append(standardized_violation)
                
                # Update counters
                if standardized_violation.severity == ValidationSeverity.ERROR:
                    report.error_count += 1
                elif standardized_violation.severity == ValidationSeverity.WARNING:
                    report.warning_count += 1
                elif standardized_violation.severity == ValidationSeverity.INFO:
                    report.info_count += 1
            
            report.total_violations = len(report.violations)
            
            self.logger.info("SHACL report transformed successfully",
                           report_id=report_id,
                           overall_valid=conforms,
                           total_violations=report.total_violations,
                           error_count=report.error_count)
            
            return report
            
        except Exception as e:
            self.logger.error("Failed to transform SHACL report",
                            report_id=report_id,
                            error=str(e))
            
            # Return error report
            error_violation = ValidationResult(
                valid=False,
                severity=ValidationSeverity.ERROR,
                message=f"SHACL adapter transformation failed: {str(e)}",
                constraint_component="adapter_error"
            )
            
            report.violations = [error_violation]
            report.overall_valid = False
            report.error_count = 1
            report.total_violations = 1
            
            return report
    
    def _extract_shacl_violations(self, shacl_graph: Any) -> List[Dict[str, Any]]:
        """
        Extract violation information from SHACL validation report graph.
        
        Args:
            shacl_graph: SHACL validation report graph
            
        Returns:
            List of raw violation dictionaries
        """
        violations = []
        
        try:
            from rdflib import Graph, Namespace
            from rdflib.namespace import SH
            
            # Parse the SHACL report graph if it's a string
            if isinstance(shacl_graph, str):
                report_graph = Graph()
                report_graph.parse(data=shacl_graph, format='turtle')
            else:
                report_graph = shacl_graph
            
            # SPARQL query to extract validation results
            query = """
            PREFIX sh: <http://www.w3.org/ns/shacl#>
            SELECT ?result ?severity ?message ?focusNode ?resultPath ?sourceConstraintComponent ?value
            WHERE {
                ?result a sh:ValidationResult ;
                        sh:resultSeverity ?severity ;
                        sh:resultMessage ?message .
                OPTIONAL { ?result sh:focusNode ?focusNode }
                OPTIONAL { ?result sh:resultPath ?resultPath }
                OPTIONAL { ?result sh:sourceConstraintComponent ?sourceConstraintComponent }
                OPTIONAL { ?result sh:value ?value }
            }
            """
            
            # Execute query and extract violations
            for row in report_graph.query(query):
                violation = {
                    "severity": str(row.severity),
                    "message": str(row.message),
                    "focusNode": str(row.focusNode) if row.focusNode else None,
                    "resultPath": str(row.resultPath) if row.resultPath else None,
                    "sourceConstraintComponent": str(row.sourceConstraintComponent) if row.sourceConstraintComponent else None,
                    "value": str(row.value) if row.value else None
                }
                violations.append(violation)
                
            self.logger.info("Extracted SHACL violations",
                           violation_count=len(violations))
                
        except Exception as e:
            self.logger.error("SHACL violation extraction failed",
                            error=str(e))
            # Fallback to empty list rather than crashing
            violations = []
        
        return violations
    
    def _simulate_shacl_extraction(self, shacl_graph: Any) -> List[Dict[str, Any]]:
        """
        Simulate SHACL violation extraction for demonstration.
        In a real implementation, this would use SPARQL queries or RDF parsing.
        """
        # This is a placeholder implementation
        # Real implementation would query the SHACL validation report graph
        return [
            {
                "severity": "sh:Violation",
                "message": "Property value violates constraint",
                "focusNode": "ex:Document1",
                "resultPath": "ex:title",
                "sourceConstraintComponent": "sh:MinLengthConstraintComponent",
                "value": "Short"
            }
        ]
    
    def _transform_violation(self, raw_violation: Dict[str, Any]) -> ValidationResult:
        """
        Transform a raw SHACL violation into standardized format.
        
        Args:
            raw_violation: Raw violation data from SHACL
            
        Returns:
            ValidationResult: Standardized validation result
        """
        # Map SHACL severity to standard severity
        shacl_severity = raw_violation.get("severity", "sh:Violation")
        if shacl_severity in ["sh:Violation", "sh:Error"]:
            severity = ValidationSeverity.ERROR
        elif shacl_severity in ["sh:Warning"]:
            severity = ValidationSeverity.WARNING
        else:
            severity = ValidationSeverity.INFO
        
        # Extract violation details
        message = raw_violation.get("message", "SHACL validation violation")
        property_path = raw_violation.get("resultPath", raw_violation.get("propertyPath"))
        focus_node = raw_violation.get("focusNode")
        constraint_component = raw_violation.get("sourceConstraintComponent")
        value = raw_violation.get("value")
        
        # Create standardized violation
        return ValidationResult(
            valid=False,
            severity=severity,
            message=message,
            property_path=property_path,
            focus_node=focus_node,
            constraint_component=constraint_component,
            value=value,
            source_location=f"SHACL:{focus_node}" if focus_node else None
        )
    
    def create_json_schema_report(self, compliance_report: ComplianceReport) -> str:
        """
        Create a JSON Schema validation report from the compliance report.
        
        Args:
            compliance_report: HMA v2.2 compliant report
            
        Returns:
            JSON string of the validation report
        """
        try:
            report_dict = compliance_report.to_dict()
            return json.dumps(report_dict, indent=2, ensure_ascii=False)
            
        except Exception as e:
            self.logger.error("Failed to create JSON Schema report",
                            report_id=compliance_report.report_id,
                            error=str(e))
            
            # Return minimal error report
            error_report = {
                "report_metadata": {
                    "report_id": compliance_report.report_id,
                    "timestamp": time.time(),
                    "validator_type": "shacl_adapter_error",
                    "hma_compliance_version": "2.2"
                },
                "validation_summary": {
                    "overall_valid": False,
                    "total_violations": 1,
                    "error_count": 1
                },
                "violations": [
                    {
                        "severity": "error",
                        "message": f"JSON Schema report generation failed: {str(e)}",
                        "constraint_component": "adapter_serialization_error"
                    }
                ]
            }
            
            return json.dumps(error_report, indent=2)
    
    def _dict_to_rdf_graph(self, data: Dict[str, Any]) -> Any:
        """
        Convert dictionary data to RDF graph for SHACL validation.
        
        Args:
            data: Dictionary data to convert
            
        Returns:
            RDF Graph object
        """
        try:
            from rdflib import Graph, Literal, URIRef, Namespace
            from rdflib.namespace import XSD
            
            graph = Graph()
            EX = Namespace("http://example.org/")
            
            # Create a basic RDF representation from the dictionary
            subject = EX.Document
            
            for key, value in data.items():
                predicate = EX[key.replace('-', '_').replace(' ', '_')]
                
                if isinstance(value, str):
                    graph.add((subject, predicate, Literal(value)))
                elif isinstance(value, (int, float)):
                    graph.add((subject, predicate, Literal(value)))
                elif isinstance(value, bool):
                    graph.add((subject, predicate, Literal(value, datatype=XSD.boolean)))
                else:
                    # Convert complex objects to string
                    graph.add((subject, predicate, Literal(str(value))))
            
            return graph
            
        except Exception as e:
            self.logger.error("Failed to convert dict to RDF graph", error=str(e))
            # Return empty graph as fallback
            from rdflib import Graph
            return Graph()
    
    def validate_with_compliance_bridge(self, 
                                      data: Any,
                                      shacl_shapes_graph: Any,
                                      report_id: Optional[str] = None) -> ComplianceReport:
        """
        Perform SHACL validation and return HMA v2.2 compliant report.
        
        This method provides a complete bridge from SHACL validation to
        JSON Schema compliance reporting.
        
        Args:
            data: Data to validate (RDF graph or dict)
            shacl_shapes_graph: SHACL shapes for validation
            report_id: Optional custom report ID
            
        Returns:
            ComplianceReport: HMA v2.2 compliant validation report
        """
        try:
            # Production SHACL validation using pyshacl
            from pyshacl import validate
            from rdflib import Graph
            
            # Convert data to RDF graph if needed
            if isinstance(data, dict):
                data_graph = self._dict_to_rdf_graph(data)
            elif isinstance(data, str):
                data_graph = Graph()
                data_graph.parse(data=data, format='turtle')
            else:
                data_graph = data
            
            # Convert shapes to RDF graph if needed
            if isinstance(shacl_shapes_graph, str):
                shapes_graph = Graph()
                shapes_graph.parse(data=shacl_shapes_graph, format='turtle')
            else:
                shapes_graph = shacl_shapes_graph
            
            # Perform SHACL validation
            conforms, results_graph, results_text = validate(
                data_graph, 
                shacl_graph=shapes_graph,
                inference='rdfs',
                serialize_report_graph='turtle'
            )
            
            self.logger.info("SHACL validation completed",
                           conforms=conforms,
                           data_triples=len(data_graph),
                           shapes_triples=len(shapes_graph))
            
            # Transform to compliance report
            return self.transform_shacl_report(results_graph, conforms, report_id)
            
        except Exception as e:
            self.logger.error("SHACL compliance validation failed",
                            error=str(e))
            
            # Return error report
            error_report = ComplianceReport(
                report_id=report_id or f"error_{int(time.time() * 1000)}",
                timestamp=time.time(),
                validator_type="shacl_adapter_error",
                validator_version=self.adapter_version,
                input_format="unknown",
                overall_valid=False
            )
            
            error_violation = ValidationResult(
                valid=False,
                severity=ValidationSeverity.ERROR,
                message=f"SHACL validation failed: {str(e)}",
                constraint_component="validation_error"
            )
            
            error_report.violations = [error_violation]
            error_report.error_count = 1
            error_report.total_violations = 1
            
            return error_report
    
    def _simulate_shacl_validation(self, data: Any, shapes: Any) -> bool:
        """Simulate SHACL validation for demonstration."""
        # In a real implementation, this would use pyshacl
        # For now, assume validation passes if data is present
        return data is not None and shapes is not None
    
    def _create_mock_results_graph(self, conforms: bool) -> List[Dict[str, Any]]:
        """Create mock results for demonstration."""
        if conforms:
            return []
        else:
            return [
                {
                    "severity": "sh:Violation",
                    "message": "Mock validation violation for demonstration",
                    "focusNode": "ex:TestNode",
                    "resultPath": "ex:testProperty",
                    "sourceConstraintComponent": "sh:MockConstraintComponent"
                }
            ]
    
    def get_adapter_info(self) -> Dict[str, Any]:
        """Get adapter information and capabilities."""
        return {
            "adapter_name": "SHACLToJSONSchemaAdapter",
            "adapter_version": self.adapter_version,
            "hma_compliance_version": "2.2",
            "tier_bridge": {
                "from_tier": "3_alternative",
                "to_tier": "1_mandatory",
                "from_technology": "SHACL",
                "to_technology": "JSON_Schema"
            },
            "capabilities": [
                "shacl_validation_report_transformation",
                "json_schema_compliance_reporting",
                "validation_result_standardization",
                "hma_compliant_output_generation"
            ],
            "supported_input_formats": [
                "rdf_turtle",
                "rdf_xml",
                "json_ld"
            ],
            "output_format": "json_schema_validation_report"
        }
```
  ```
  --- END OF FILE scribe/adapters\shacl_adapter.py ---
  --- START OF FILE scribe/config\README.md ---
  ```
  ```md
# Scribe Configuration - HMA v2.2 Compliant Settings

This directory contains all configuration files for the Scribe engine. All configurations follow HMA v2.2 standards with JSON Schema validation and externalized security policies.

## Configuration Files

### Core Configuration

| File | Purpose | Schema |
|------|---------|--------|
| **config.json** | Main engine configuration | `../schemas/scribe_config.schema.json` |
| **security_policy.yaml** | Externalized security policies | Custom YAML format |

## Main Configuration (config.json)

The main configuration file defines all engine settings with HMA v2.2 compliance:

### Configuration Structure

```json
{
  "config_version": "2.2",
  "hma_compliance": {
    "version": "2.2",
    "tier_1_technologies": {
      "telemetry": "opentelemetry",
      "security": "mtls",
      "validation": "json_schema"
    },
    "tier_2_technologies": {
      "messaging": "nats",
      "logging": "structured",
      "orchestration": "kubernetes"
    }
  },
  "engine": {
    "watch_paths": ["."],
    "file_patterns": ["*.md", "*.txt", "*.json"],
    "max_workers": 4,
    "enable_hot_reload": true,
    "health_check_port": 9090
  },
  "plugins": {
    "manifest_required": true,
    "validation_strict": true,
    "load_order": [
      "enhanced_frontmatter_action",
      "naming_enforcement_action",
      "graph_validation_action",
      "reconciliation_action"
    ]
  },
  "security": {
    "enable_mtls": true,
    "audit_enabled": true,
    "allowed_commands": ["git", "python", "echo"],
    "policy_file": "config/security_policy.yaml"
  },
  "telemetry": {
    "enabled": true,
    "service_name": "scribe-engine",
    "export_endpoint": "http://jaeger:14268/api/traces",
    "metrics_port": 9090,
    "trace_sampling_rate": "1.0"
  },
  "nats": {
    "url": "nats://localhost:4222",
    "max_reconnect_attempts": -1,
    "reconnect_time_wait": 2,
    "connection_timeout": 30
  },
  "performance": {
    "async_processing": {
      "enabled": true,
      "max_queue_size": 1000,
      "worker_count": 4,
      "timeout_seconds": 300
    },
    "caching": {
      "enabled": true,
      "max_size": 1000,
      "ttl_seconds": 3600
    }
  },
  "logging": {
    "level": "INFO",
    "format": "structured",
    "include_stdlib_logs": true,
    "correlation_ids": true
  }
}
```

### Configuration Sections

#### HMA Compliance

Defines HMA v2.2 compliance settings and technology tier selections:

```json
{
  "hma_compliance": {
    "version": "2.2",
    "tier_1_technologies": {
      "telemetry": "opentelemetry",
      "security": "mtls", 
      "validation": "json_schema"
    },
    "tier_2_technologies": {
      "messaging": "nats",
      "logging": "structured"
    },
    "tier_3_adapters": {
      "shacl_adapter": "enabled"
    }
  }
}
```

#### Engine Settings

Core engine behavior and file watching configuration:

```json
{
  "engine": {
    "watch_paths": ["."],                    // Directories to monitor
    "file_patterns": ["*.md", "*.txt"],     // File patterns to process
    "exclude_patterns": ["*.tmp", "*.bak"], // Patterns to exclude
    "max_workers": 4,                       // Maximum concurrent workers
    "enable_hot_reload": true,              // Plugin hot-reloading
    "health_check_port": 9090,              // Health endpoint port
    "graceful_shutdown_timeout": 30         // Shutdown timeout in seconds
  }
}
```

#### Plugin Configuration

Plugin loading and management settings:

```json
{
  "plugins": {
    "manifest_required": true,              // Require manifest.json for all plugins
    "validation_strict": true,              // Strict manifest validation
    "load_order": [                         // Plugin loading order
      "enhanced_frontmatter_action",
      "naming_enforcement_action"
    ],
    "directories": ["actions"],             // Plugin search directories
    "auto_reload": true,                    // Automatic plugin reloading
    "timeout_seconds": 60                   // Plugin execution timeout
  }
}
```

#### Security Configuration

Security policies and enforcement settings:

```json
{
  "security": {
    "enable_mtls": true,                    // Enable mutual TLS
    "audit_enabled": true,                  // Enable security auditing
    "allowed_commands": ["git", "python"],  // Whitelist of allowed commands
    "policy_file": "config/security_policy.yaml", // External policy file
    "max_command_timeout": 300,             // Command execution timeout
    "env_var_whitelist": ["PATH", "HOME"]   // Allowed environment variables
  }
}
```

#### Telemetry Configuration

OpenTelemetry and observability settings:

```json
{
  "telemetry": {
    "enabled": true,
    "service_name": "scribe-engine",
    "service_version": "2.2.0",
    "export_endpoint": "http://jaeger:14268/api/traces",
    "metrics_endpoint": "http://prometheus:9090/metrics",
    "trace_sampling_rate": "1.0",           // Sample 100% of traces
    "resource_attributes": {                // Custom resource attributes
      "environment": "production",
      "deployment.zone": "us-west-2"
    }
  }
}
```

#### NATS Configuration

Message broker settings for event communication:

```json
{
  "nats": {
    "url": "nats://localhost:4222",         // NATS server URL
    "cluster_urls": [                       // Cluster URLs for HA
      "nats://nats-1:4222",
      "nats://nats-2:4222"
    ],
    "max_reconnect_attempts": -1,           // Unlimited reconnects
    "reconnect_time_wait": 2,               // Seconds between reconnect attempts
    "connection_timeout": 30,               // Connection timeout
    "credentials_file": "nats.creds",       // NATS credentials file
    "tls_enabled": true                     // Enable TLS
  }
}
```

## Security Policy (security_policy.yaml)

Externalized security policies that can be updated without code changes:

```yaml
# Security Policy v2.2
policy_version: "2.2"
last_updated: "2025-07-24T00:00:00Z"

# Dangerous command patterns to block
dangerous_patterns:
  - "rm -rf"
  - "del /s"
  - "format c:"
  - "shutdown"
  - "reboot"
  - "mkfs"
  - "fdisk"

# Environment variables to always scrub from logs
dangerous_env_keys_to_always_scrub:
  - "PASSWORD"
  - "SECRET"
  - "TOKEN"
  - "KEY"
  - "CREDENTIAL"
  - "AUTH"
  - "API_KEY"
  - "PRIVATE_KEY"

# File path restrictions
restricted_paths:
  - "/etc/passwd"
  - "/etc/shadow"
  - "C:\\Windows\\System32"
  - "/proc"
  - "/sys"

# Network restrictions
network_restrictions:
  allowed_domains:
    - "api.github.com"
    - "registry.npmjs.org"
  blocked_ips:
    - "169.254.169.254"  # AWS metadata
    - "metadata.google.internal"

# Command execution limits
execution_limits:
  max_execution_time: 300      # 5 minutes
  max_memory_mb: 1024         # 1GB
  max_file_descriptors: 100   # File handles
  max_processes: 10           # Child processes
```

## Environment-Specific Configuration

### Development Configuration

```json
{
  "config_version": "2.2",
  "engine": {
    "watch_paths": ["./dev-content"],
    "enable_hot_reload": true,
    "health_check_port": 9090
  },
  "security": {
    "enable_mtls": false,
    "audit_enabled": false
  },
  "telemetry": {
    "enabled": true,
    "export_endpoint": "http://localhost:14268/api/traces",
    "trace_sampling_rate": "0.1"
  },
  "logging": {
    "level": "DEBUG",
    "include_stdlib_logs": true
  }
}
```

### Production Configuration

```json
{
  "config_version": "2.2",
  "engine": {
    "watch_paths": ["/app/content"],
    "enable_hot_reload": false,
    "health_check_port": 9090
  },
  "security": {
    "enable_mtls": true,
    "audit_enabled": true,
    "policy_file": "/etc/scribe/security_policy.yaml"
  },
  "telemetry": {
    "enabled": true,
    "export_endpoint": "https://jaeger.example.com/api/traces",
    "trace_sampling_rate": "0.1"
  },
  "logging": {
    "level": "INFO",
    "include_stdlib_logs": false
  }
}
```

## Configuration Validation

All configuration files are validated against JSON Schema:

### Validation Process

1. **Schema Loading**: Load schema from `../schemas/scribe_config.schema.json`
2. **Structure Validation**: Validate JSON structure and data types
3. **Business Logic Validation**: Check cross-field dependencies and constraints
4. **Security Validation**: Ensure secure defaults and no dangerous settings

### Validation Example

```python
import json
import jsonschema

def validate_config(config_path: str) -> bool:
    """Validate configuration against schema."""
    
    # Load configuration
    with open(config_path) as f:
        config = json.load(f)
    
    # Load schema
    with open("../schemas/scribe_config.schema.json") as f:
        schema = json.load(f)
    
    # Validate
    try:
        jsonschema.validate(config, schema)
        return True
    except jsonschema.ValidationError as e:
        print(f"Configuration validation failed: {e}")
        return False
```

## Configuration Management

### Dynamic Configuration Updates

Some settings can be updated at runtime:

```python
# Update logging level
config_manager.update_setting("logging.level", "DEBUG")

# Update plugin load order
config_manager.update_setting("plugins.load_order", ["new_plugin", "existing_plugin"])
```

### Configuration Hot-Reloading

The engine can reload configuration without restart for certain settings:

- Logging levels
- Plugin load order
- Security policies (external file)
- Telemetry sampling rates

### Configuration Sources

Configuration can be loaded from multiple sources (in order of precedence):

1. **Environment Variables**: `SCRIBE_WATCH_PATHS`, `SCRIBE_LOG_LEVEL`
2. **Command Line Arguments**: `--config-file`, `--log-level`
3. **Configuration File**: `config.json`
4. **Default Values**: Built-in defaults

## Troubleshooting

### Common Configuration Issues

#### Invalid JSON Format

```bash
# Validate JSON syntax
python -m json.tool config.json
```

#### Schema Validation Errors

```bash
# Validate against schema
python -c "
import json, jsonschema
config = json.load(open('config.json'))
schema = json.load(open('../schemas/scribe_config.schema.json'))
jsonschema.validate(config, schema)
print('Configuration is valid')
"
```

#### Permission Issues

```bash
# Check file permissions
ls -la config/
chmod 644 config.json
chmod 600 security_policy.yaml  # More restrictive for security file
```

### Debug Configuration Loading

```python
import json
from core.config_manager import ConfigManager

# Load with debug information
config_manager = ConfigManager("config/config.json", debug=True)
print(f"Loaded configuration: {json.dumps(config_manager.config, indent=2)}")
```

## Security Considerations

### File Permissions

- **config.json**: `644` (readable by all, writable by owner)
- **security_policy.yaml**: `600` (readable/writable by owner only)

### Sensitive Data

- **Never store secrets** in configuration files
- Use environment variables or secret management systems
- Scrub sensitive data from logs using security policies

### Configuration Validation

- All inputs validated against schemas
- Business logic validation for cross-field dependencies
- Security validation to prevent dangerous configurations

## Related Documentation

- [Schema Definitions](../schemas/) - JSON Schema validation files
- [Security Manager](../core/security_manager.py) - Security policy enforcement
- [Config Manager](../core/config_manager.py) - Configuration loading and validation
- [Deployment Guide](../deployment/) - Environment-specific deployment configurations
```
  ```
  --- END OF FILE scribe/config\README.md ---
  --- START OF FILE scribe/config\config.json ---
  ```
  ```json
{
  "config_version": "2.0",
  "engine": {
    "watch_paths": ["."],
    "file_patterns": ["*.md", "*.txt", "*.json"],
    "max_workers": 4,
    "enable_hot_reload": true,
    "health_check_port": 9469
  },
  "logging": {
    "level": "INFO",
    "format": "structured",
    "output_path": "tools/reports/scribe-engine.log",
    "max_file_size": "10MB",
    "backup_count": 5
  },
  "security": {
    "enable_mtls": false,
    "cert_path": "",
    "key_path": "",
    "ca_path": "",
    "audit_enabled": true,
    "allowed_commands": ["git", "python"],
    "restricted_paths": [".git/", ".vscode/", "node_modules/", "archive/"]
  },
  "plugins": {
    "directories": ["actions/"],
    "auto_reload": false,
    "load_order": [],
    "manifest_required": true
  },
  "performance": {
    "async_processing": {
      "enabled": true,
      "max_queue_size": 1000,
      "worker_count": 4,
      "batch_size": 10
    },
    "caching": {
      "enabled": true,
      "max_size": 1000,
      "ttl_seconds": 3600
    },
    "file_optimization": {
      "use_memory_mapping": true,
      "streaming_threshold": "1MB"
    }
  },
  "circuit_breaker": {
    "failure_threshold": 5,
    "recovery_timeout_seconds": 300,
    "success_threshold": 3
  },
  "telemetry": {
    "enabled": true,
    "endpoint": "http://localhost:4318/v1/traces",
    "service_name": "scribe-engine",
    "sample_rate": 1.0
  },
  "rules": []
}
```
  ```
  --- END OF FILE scribe/config\config.json ---
  --- START OF FILE scribe/config\security_policy.yaml ---
  ```
  ```yaml
# HMA v2.2 Compliant Security Policy Configuration
# This file contains externalized security policies that were previously hard-coded

# Dangerous environment variables that should always be scrubbed for security
dangerous_env_keys_to_always_scrub:
  - 'LD_PRELOAD'
  - 'LD_LIBRARY_PATH'
  - 'DYLD_INSERT_LIBRARIES'
  - 'DYLD_LIBRARY_PATH'
  - 'PYTHONPATH'
  - 'PATH'  # We'll set a safe PATH
  - 'SHELL'
  - 'IFS'
  - 'PS1'
  - 'PS2'
  - 'PS4'
  - 'PROMPT_COMMAND'
  - 'BASH_ENV'
  - 'ENV'
  - 'FPATH'
  - 'CDPATH'

# Dangerous patterns to check for in commands and parameters
dangerous_patterns:
  - 'rm\s+-rf\s+/'      # Dangerous rm commands
  - 'sudo\s+'           # Sudo commands  
  - 'su\s+'             # Su commands
  - 'chmod\s+777'       # Overly permissive chmod
  - 'eval\s*\('         # Eval functions
  - 'exec\s*\('         # Exec functions
  - '__import__'        # Python imports
  - 'open\s*\('         # File operations
  - 'file\s*\('         # File operations
  - 'subprocess'        # Subprocess calls
  - 'os\.system'        # OS system calls
  - 'shell=True'        # Shell execution

# Security policy metadata
policy_metadata:
  version: "2.2.0"
  last_updated: "2025-07-24"
  compliance_framework: "HMA v2.2"
  review_frequency: "quarterly"
```
  ```
  --- END OF FILE scribe/config\security_policy.yaml ---
  --- START OF FILE scribe/core\README.md ---
  ```
  ```md
# Scribe Core - L2 Microkernel Components

This directory contains the L2 Microkernel Core components that implement the central logic and coordination for the Scribe engine. All components are HMA v2.2 compliant and follow strict architectural patterns.

## Architecture Overview

The core implements the HMA v2.2 L2 Microkernel pattern with these key principles:

- **Minimalist Core**: `engine.py` contains only routing and lifecycle management
- **Factory Pattern**: `engine_factory.py` handles all component instantiation
- **Ports-Only Interactions**: All external communication through registered ports
- **Dependency Injection**: Components receive dependencies via constructor injection
- **Boundary Telemetry**: OpenTelemetry spans on all boundary crossings

## Core Components

### Engine and Lifecycle

| Component | Purpose | HMA Layer |
|-----------|---------|-----------|
| **engine.py** | Minimalist L2 Core with routing and lifecycle only | L2-Core |
| **engine_factory.py** | Component creation and dependency injection | L2-Infrastructure |
| **minimal_core.py** | HMA-compliant minimal core implementation | L2-Core |

### Port System

| Component | Purpose | HMA Layer |
|-----------|---------|-----------|
| **hma_ports.py** | Technology-agnostic port interface definitions | L2-Interfaces |
| **port_adapters.py** | Concrete port implementations with adapters | L2-Infrastructure |
| **adapters/** | Specialized adapters (NATS, SHACL, etc.) | L2-Infrastructure |

### Plugin Management

| Component | Purpose | HMA Layer |
|-----------|---------|-----------|
| **plugin_loader.py** | HMA v2.2 plugin discovery and validation | L2-Infrastructure |
| **action_dispatcher.py** | Plugin execution coordination | L2-Core |

### Configuration and Security

| Component | Purpose | HMA Layer |
|-----------|---------|-----------|
| **config_manager.py** | Centralized configuration management | L2-Infrastructure |
| **security_manager.py** | Security policy enforcement and validation | L2-Infrastructure |
| **mtls.py** | Mutual TLS implementation for secure communication | L2-Infrastructure |

### Observability and Telemetry

| Component | Purpose | HMA Layer |
|-----------|---------|-----------|
| **hma_telemetry.py** | HMA v2.2 compliant OpenTelemetry implementation | L2-Infrastructure |
| **logging_config.py** | Structured logging configuration | L2-Infrastructure |
| **health_monitor.py** | Health check endpoints and monitoring | L2-Infrastructure |

### Processing and Coordination

| Component | Purpose | HMA Layer |
|-----------|---------|-----------|
| **async_processor.py** | Asynchronous task processing with queues | L2-Infrastructure |
| **rule_processor.py** | File pattern matching and rule evaluation | L2-Core |
| **boundary_validator.py** | HMA boundary validation and enforcement | L2-Infrastructure |

### Utility Components

| Component | Purpose | HMA Layer |
|-----------|---------|-----------|
| **atomic_write.py** | Cross-platform atomic file operations | L2-Infrastructure |
| **cache_manager.py** | Caching and memoization utilities | L2-Infrastructure |
| **circuit_breaker.py** | Circuit breaker pattern implementation | L2-Infrastructure |
| **error_recovery.py** | Error handling and recovery mechanisms | L2-Infrastructure |

## HMA v2.2 Compliance

### Mandatory Tier 1 Technologies

- âœ… **OpenTelemetry**: Boundary telemetry on all port interactions (`hma_telemetry.py`)
- âœ… **mTLS**: Mutual TLS for inter-component communication (`mtls.py`)
- âœ… **JSON Schema**: Validation for all configuration and manifests

### Recommended Tier 2 Technologies

- âœ… **NATS**: Message broker replacing in-memory queues (`adapters/nats_adapter.py`)
- âœ… **Structured Logging**: Contextual logging with correlation IDs
- âœ… **Health Checks**: Comprehensive health monitoring endpoints

### Architecture Patterns

- âœ… **Minimalist Core**: Engine contains only essential routing logic
- âœ… **Factory Pattern**: All instantiation externalized from core
- âœ… **Ports-and-Adapters**: Strict boundary enforcement with port registry
- âœ… **Dependency Injection**: Clean dependency management

## Key Design Patterns

### Factory Pattern

The `engine_factory.py` implements the factory pattern to eliminate god objects:

```python
def create_engine_components() -> EngineComponents:
    """Create all engine components with proper dependency injection."""
    components = EngineComponents()
    
    # Create components in correct order
    components.config_manager = ConfigManager(config_path)
    components.telemetry = create_hma_telemetry(...)
    components.security_manager = SecurityManager(components.config_manager)
    
    # Create and register port adapters
    _create_port_adapters(components)
    
    return components
```

### Port Registry Pattern

All external interactions go through registered ports:

```python
# Register ports
port_registry = PortRegistry()
port_registry.register_port("command_execution", command_adapter)
port_registry.register_port("event_bus", nats_adapter)

# Access through registry
command_port = port_registry.get_port("command_execution")
result = await command_port.execute_command_safely(["git", "status"])
```

### Boundary Telemetry Pattern

All port interactions include mandatory OpenTelemetry spans:

```python
async def execute_plugin(self, plugin_id: str, input_data: Dict[str, Any]):
    with self.telemetry.start_span("plugin_execution_boundary", plugin_id) as span:
        span.set_attribute("hma.boundary.type", "plugin_execution")
        span.set_attribute("hma.operation", "execute_plugin")
        # ... execution logic
```

## Component Interactions

### Engine Startup Sequence

1. **Factory Creation**: `create_engine_components()` instantiates all components
2. **Port Registration**: All adapters registered in port registry
3. **Plugin Loading**: `plugin_loader.load_all_plugins()` discovers and validates plugins
4. **Core Initialization**: `minimal_core.start()` initializes HMA core
5. **Service Startup**: Health server and NATS adapter started

### Plugin Execution Flow

1. **File Event**: Watcher detects file change
2. **Rule Processing**: `rule_processor` matches patterns and determines actions
3. **Plugin Dispatch**: `action_dispatcher` coordinates plugin execution
4. **Port Access**: Plugin accesses functionality through port registry
5. **Boundary Telemetry**: All interactions traced with OpenTelemetry
6. **Result Processing**: Results processed and events published

## Configuration

### Core Configuration

```json
{
  "engine": {
    "watch_paths": ["."],
    "max_workers": 4,
    "health_check_port": 9090
  },
  "telemetry": {
    "enabled": true,
    "service_name": "scribe-engine",
    "export_endpoint": "http://jaeger:14268/api/traces"
  },
  "nats": {
    "url": "nats://localhost:4222",
    "max_reconnect_attempts": -1
  }
}
```

### Security Configuration

External security policies in `../config/security_policy.yaml`:

```yaml
dangerous_patterns:
  - "rm -rf"
  - "del /s"
  - "format c:"

dangerous_env_keys_to_always_scrub:
  - "PASSWORD"
  - "SECRET"
  - "TOKEN"
```

## Development Guidelines

### Adding New Components

1. **Interface First**: Define port interface in `hma_ports.py` if needed
2. **Implementation**: Create concrete implementation with proper error handling
3. **Factory Integration**: Add to `engine_factory.py` with dependency injection
4. **Telemetry**: Add OpenTelemetry boundary spans
5. **Testing**: Unit tests with proper mocking
6. **Documentation**: Update this README and add docstrings

### Error Handling

```python
try:
    result = await some_operation()
    logger.info("Operation completed", operation="some_operation", result=result)
    return result
except Exception as e:
    logger.error("Operation failed", operation="some_operation", error=str(e))
    self.telemetry.record_error("operation_failed", "component_name", str(e))
    raise
```

### Async Patterns

```python
# Proper async method with timeout
async def process_with_timeout(self, data: Any, timeout: int = 30) -> Any:
    try:
        return await asyncio.wait_for(
            self._actual_processing(data),
            timeout=timeout
        )
    except asyncio.TimeoutError:
        raise ProcessingTimeoutError(f"Processing timed out after {timeout}s")
```

## Testing

### Unit Testing

```python
import pytest
from unittest.mock import Mock, AsyncMock

@pytest.fixture
def mock_components():
    components = Mock()
    components.config_manager = Mock()
    components.telemetry = Mock()
    return components

async def test_component_function(mock_components):
    component = MyComponent(mock_components.config_manager, mock_components.telemetry)
    result = await component.process_data("test")
    assert result is not None
```

### Integration Testing

```python
async def test_engine_startup():
    components = create_engine_components("test_config.json")
    engine = ScribeEngine(components)
    
    # Test startup
    engine.start()
    assert engine.is_running
    
    # Test shutdown
    engine.stop()
    assert not engine.is_running
```

## Performance Considerations

- **Async Operations**: All I/O operations use async/await
- **Connection Pooling**: Reuse connections where possible
- **Caching**: Cache expensive operations with TTL
- **Resource Limits**: Respect memory and CPU constraints
- **Batch Processing**: Process multiple items when beneficial

## Security Considerations

- **mTLS**: All inter-component communication secured
- **Input Validation**: All inputs validated against schemas
- **Permission Checks**: Components verify permissions before operations
- **Audit Logging**: All security-relevant operations logged
- **Secret Management**: Secrets externalized and encrypted

## Troubleshooting

### Common Issues

- **Port Not Found**: Check port registration in `engine_factory.py`
- **Plugin Load Errors**: Validate manifest against v2.2 schema
- **Telemetry Issues**: Verify OpenTelemetry configuration
- **NATS Connection**: Check NATS server availability and configuration

### Debug Commands

```bash
# Check component health
curl http://localhost:9090/health

# View telemetry
curl http://localhost:9090/metrics

# Test plugin loading
python -c "from core.plugin_loader import PluginLoader; PluginLoader().load_all_plugins()"
```

## Related Documentation

- [HMA v2.2 Specification](../../HMA v2.2/) - Architecture specification
- [Architecture Decision Records](../docs/decisions/) - Design decisions
- [Plugin Development](../actions/README.md) - Plugin development guide
- [Deployment Guide](../deployment/) - Production deployment instructions
```
  ```
  --- END OF FILE scribe/core\README.md ---
  --- START OF FILE scribe/core\__init__.py ---
  ```
  ```py
"""
Scribe Core Components

Core functionality for the Scribe automation engine including:
- Configuration management
- Security management  
- Logging configuration
- Action dispatching
- Plugin loading
"""

from .config_manager import ConfigManager
from .security_manager import SecurityManager, SecurityViolation
from .logging_config import configure_structured_logging, get_scribe_logger

__all__ = [
    "ConfigManager",
    "SecurityManager",
    "SecurityViolation", 
    "configure_structured_logging",
    "get_scribe_logger",
] 
```
  ```
  --- END OF FILE scribe/core\__init__.py ---
  --- START OF FILE scribe/core\action_dispatcher.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Action Dispatcher

Orchestrates the execution of actions when rules match.
Handles action loading, parameter validation, execution, and error recovery.
"""

import re
import time
import shutil
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import structlog

from .logging_config import get_scribe_logger
from .plugin_loader import PluginLoader, PluginInfo
from .rule_processor import RuleMatch
from .circuit_breaker import CircuitBreakerManager, CircuitBreakerError
from tools.scribe.actions.base import BaseAction, ActionExecutionError, ValidationError
from .config_manager import ConfigManager
from .security_manager import SecurityManager, SecurityViolation


logger = get_scribe_logger(__name__)


class ActionChainFailedError(Exception):
    """Exception raised when one or more actions fail in a chain."""
    
    def __init__(self, message: str, rule_id: str = None, failed_actions: int = 0, total_actions: int = 0):
        """
        Initialize the exception.
        
        Args:
            message: Error message
            rule_id: ID of the rule that failed
            failed_actions: Number of failed actions
            total_actions: Total number of actions in the chain
        """
        self.rule_id = rule_id
        self.failed_actions = failed_actions
        self.total_actions = total_actions
        super().__init__(message)


class ActionResult:
    """Result of executing an action."""
    
    def __init__(self, 
                 action_type: str,
                 success: bool,
                 modified_content: Optional[str] = None,
                 error: Optional[Exception] = None,
                 execution_time: float = 0.0,
                 metadata: Optional[Dict[str, Any]] = None,
                 error_message: Optional[str] = None,  # Backward compatibility
                 execution_time_ms: Optional[float] = None):  # Backward compatibility
        """
        Initialize action result.
        
        Args:
            action_type: Type of action that was executed
            success: Whether the action succeeded
            modified_content: The modified file content (if successful)
            error: Exception if the action failed
            execution_time: Time taken to execute the action in seconds
            metadata: Additional metadata about the execution
        """
        self.action_type = action_type
        self.success = success
        self.modified_content = modified_content
        
        # Handle backward compatibility for error_message parameter
        if error_message is not None and error is None:
            self.error = Exception(error_message)
            self.error_message = error_message
        elif error is not None:
            self.error = error
            self.error_message = str(error)
        else:
            self.error = error
            self.error_message = None
        
        # Handle backward compatibility for execution_time_ms parameter
        if execution_time_ms is not None:
            self.execution_time = execution_time_ms / 1000.0  # Convert ms to seconds
            self.execution_time_ms = execution_time_ms
        else:
            self.execution_time = execution_time
            self.execution_time_ms = execution_time * 1000.0  # Convert seconds to ms
            
        self.metadata = metadata or {}
        self.timestamp = time.time()
    
    def __str__(self) -> str:
        status = "SUCCESS" if self.success else "FAILED"
        return f"ActionResult({self.action_type}: {status})"
    
    def __repr__(self) -> str:
        return f"ActionResult(action_type='{self.action_type}', success={self.success}, execution_time={self.execution_time:.3f}s)"


class DispatchResult:
    """Result of dispatching all actions for a rule match."""
    
    def __init__(self, 
                 rule_id: str,
                 file_path: str,
                 final_content: str,
                 action_results: List[ActionResult]):
        """
        Initialize dispatch result.
        
        Args:
            rule_id: ID of the rule that was processed
            file_path: Path to the file that was processed
            final_content: Final content after all actions
            action_results: Results of individual actions
        """
        self.rule_id = rule_id
        self.file_path = file_path
        self.final_content = final_content
        self.action_results = action_results
        self.timestamp = time.time()
        
        # Calculate summary statistics
        self.total_actions = len(action_results)
        self.successful_actions = sum(1 for r in action_results if r.success)
        self.failed_actions = self.total_actions - self.successful_actions
        self.total_execution_time = sum(r.execution_time for r in action_results)
        self.success = self.failed_actions == 0
    
    def get_failed_actions(self) -> List[ActionResult]:
        """Get list of failed action results."""
        return [r for r in self.action_results if not r.success]
    
    def get_successful_actions(self) -> List[ActionResult]:
        """Get list of successful action results."""
        return [r for r in self.action_results if r.success]
    
    def __str__(self) -> str:
        status = "SUCCESS" if self.success else "PARTIAL/FAILED"
        return f"DispatchResult({self.rule_id}: {status}, {self.successful_actions}/{self.total_actions} actions)"
    
    def __repr__(self) -> str:
        return f"DispatchResult(rule_id='{self.rule_id}', success={self.success}, actions={self.successful_actions}/{self.total_actions})"


class ActionDispatcher:
    """
    Dispatches and executes actions for rule matches.
    
    Handles action loading, parameter validation, execution orchestration,
    and error recovery with comprehensive logging.
    """
    
    def __init__(self,
                 plugin_loader: PluginLoader,
                 config_manager: ConfigManager,
                 security_manager: SecurityManager,
                 quarantine_path: Optional[str] = None):
        """
        Initialize the action dispatcher.
        
        Args:
            plugin_loader: Plugin loader for action plugins.
            config_manager: ConfigManager instance.
            security_manager: SecurityManager instance.
            quarantine_path: Path to quarantine directory for failed files.
        """
        self.plugin_loader = plugin_loader
        self.config_manager = config_manager
        self.security_manager = security_manager
        self.quarantine_path = quarantine_path or "archive/scribe/quarantine/"
        
        # Circuit breaker manager for rule failure isolation
        self.circuit_breaker_manager = CircuitBreakerManager()
        
        # Action instances are created per execution for thread safety
        
        # Execution statistics
        self._execution_stats = {
            'total_dispatches': 0,
            'successful_dispatches': 0,
            'failed_dispatches': 0,
            'total_actions_executed': 0,
            'total_execution_time': 0.0,
            'circuit_breaker_blocks': 0,
            'files_quarantined': 0
        }
        
        logger.info("ActionDispatcher initialized",
                   # Log security_manager presence if needed, e.g. security_enabled=bool(self.security_manager)
                   circuit_breaker_enabled=True,
                   quarantine_path=self.quarantine_path)

    
    def validate_action_params(self, action: BaseAction, params: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
        """
        Validate action parameters.
        
        Args:
            action: The action instance
            params: Parameters to validate
            
        Returns:
            Tuple of (is_valid, error_message)
        """
        try:
            # Check required parameters
            required_params = action.get_required_params()
            for param_name in required_params:
                if param_name not in params:
                    return False, f"Required parameter '{param_name}' is missing"
                if params[param_name] is None:
                    return False, f"Required parameter '{param_name}' cannot be None"
            
            # Use action's validation method
            if not action.validate_params(params):
                return False, "Action parameter validation failed"
            
            return True, None
            
        except Exception as e:
            return False, f"Parameter validation error: {e}"
    

    def execute_action(self, action: BaseAction, file_content: str, match: re.Match, file_path: str, params: Dict[str, Any], event_id: Optional[str] = None) -> ActionResult:
        start_time = time.time()
        action_type = action.action_type
        max_retries = 3
        for attempt in range(max_retries + 1):
            try:
                logger.debug("Executing action", event_id=event_id, action_type=action_type, file_path=file_path, params=params, attempt=attempt)
                action.pre_execute(file_content, match, file_path, params)
                modified_content = action.execute(file_content, match, file_path, params)
                action.post_execute(file_content, modified_content, match, file_path, params)
                execution_time = time.time() - start_time
                if not isinstance(modified_content, str):
                    raise ActionExecutionError(action_type, f"Action returned {type(modified_content)}, expected str")
                logger.info("Action executed successfully", event_id=event_id, action_type=action_type, file_path=file_path, execution_time=execution_time, content_changed=modified_content != file_content)
                return ActionResult(action_type=action_type, success=True, modified_content=modified_content, execution_time=execution_time, metadata={'content_changed': modified_content != file_content, 'content_length_before': len(file_content), 'content_length_after': len(modified_content)})
            except (ActionExecutionError, ValidationError) as e:
                execution_time = time.time() - start_time
                logger.error("Action execution failed", event_id=event_id, action_type=action_type, file_path=file_path, error=str(e), execution_time=execution_time, attempt=attempt)
                if attempt < max_retries:
                    time.sleep(2 ** attempt)  # Exponential backoff
                    continue
                return ActionResult(action_type=action_type, success=False, error=e, execution_time=execution_time)
            except Exception as e:
                execution_time = time.time() - start_time
                logger.error("Unexpected error during action execution", event_id=event_id, action_type=action_type, file_path=file_path, error=str(e), execution_time=execution_time, exc_info=True, attempt=attempt)
                wrapped_error = ActionExecutionError(action_type, "Unexpected error", e)
                if attempt < max_retries:
                    time.sleep(2 ** attempt)
                    continue
                return ActionResult(action_type=action_type, success=False, error=wrapped_error, execution_time=execution_time)
    
    def dispatch_actions(self, rule_match: RuleMatch) -> DispatchResult:
        """
        Dispatch all actions for a rule match with circuit breaker protection.
        
        Args:
            rule_match: The rule match to process
            
        Returns:
            DispatchResult with execution details
        """
        start_time = time.time()
        rule_id = rule_match.rule.id
        file_path = rule_match.file_path
        current_content = rule_match.file_content
        event_id = rule_match.event_id
        
        logger.info("Dispatching actions for rule match",
                   event_id=event_id,
                   rule_id=rule_id,
                   file_path=file_path,
                   actions_count=len(rule_match.rule.actions))
        
        # Update statistics
        self._execution_stats['total_dispatches'] += 1
        
        # Get circuit breaker configuration from rule
        circuit_breaker_config = self._get_circuit_breaker_config(rule_match.rule)
        circuit_breaker = self.circuit_breaker_manager.get_breaker(
            rule_id=rule_id,
            **circuit_breaker_config
        )
        
        # Check if circuit breaker allows execution
        try:
            # Wrap the action execution logic with circuit breaker
            def execute_actions():
                return self._execute_actions_internal(rule_match, current_content)
            
            # Define fallback callback for circuit breaker errors
            def circuit_breaker_fallback(error):
                """Fallback handler when circuit breaker is open"""
                self._execution_stats['circuit_breaker_blocks'] += 1
                self._execution_stats['failed_dispatches'] += 1
                
                logger.warning("Circuit breaker fallback triggered",
                              event_id=event_id,
                              rule_id=rule_id,
                              file_path=file_path,
                              error_type=type(error).__name__,
                              error_message=str(error))
                
                # Quarantine the file when circuit breaker trips
                quarantine_result = self.quarantine_file(file_path, rule_id, "circuit_breaker_open")
                
                # Return a DispatchResult indicating circuit breaker block
                circuit_breaker_error = ActionResult(
                    action_type="circuit_breaker",
                    success=False,
                    error_message=str(error),
                    execution_time_ms=0,
                    metadata={
                        "blocked_by_circuit_breaker": True,
                        "circuit_breaker_state": "open",
                        "fallback_triggered": True,
                        "quarantine_reason": "circuit_breaker_open"
                    }
                )
                
                return DispatchResult(
                    success=False,
                    rule_id=rule_id,
                    final_content=current_content,
                    action_results=[circuit_breaker_error]
                )
            
            dispatch_result = circuit_breaker.execute(execute_actions, circuit_breaker_fallback)
            
            # Update statistics for successful dispatch
            self._execution_stats['successful_dispatches'] += 1
            total_time = time.time() - start_time
            self._execution_stats['total_execution_time'] += total_time
            
            logger.info("Action dispatch complete (circuit breaker: CLOSED)",
                       event_id=event_id,
                       rule_id=rule_id,
                       file_path=file_path,
                       total_actions=dispatch_result.total_actions,
                       successful_actions=dispatch_result.successful_actions,
                       failed_actions=dispatch_result.failed_actions,
                       total_time=total_time,
                       content_changed=dispatch_result.final_content != rule_match.file_content)
            
            return dispatch_result
            
        except Exception as e:
            # Unexpected error during circuit breaker execution
            self._execution_stats['failed_dispatches'] += 1
            
            logger.error("Unexpected error during circuit breaker execution",
                        event_id=event_id,
                        rule_id=rule_id,
                        file_path=file_path,
                        error=str(e),
                        exc_info=True)
            
            # Return result with a synthetic failure to indicate system error
            system_error = ActionResult(
                action_type="system_error",
                success=False,
                error=e,
                execution_time=0.0,
                metadata={"system_level_failure": True}
            )
            
            return DispatchResult(
                rule_id=rule_id,
                file_path=file_path,
                final_content=current_content,
                action_results=[system_error]
            )
    
    def _get_circuit_breaker_config(self, rule) -> Dict[str, int]:
        """
        Extract circuit breaker configuration from rule.
        
        Args:
            rule: Rule object with potential circuit breaker config
            
        Returns:
            Dictionary with circuit breaker parameters
        """
        # Default circuit breaker configuration
        default_config = {
            'failure_threshold': 5,
            'recovery_timeout_seconds': 60,
            'success_threshold': 3
        }
        
        # Check if rule has error handling configuration
        if hasattr(rule, 'error_handling') and rule.error_handling:
            circuit_breaker_config = rule.error_handling.get('circuit_breaker', {})
            
            # Override defaults with rule-specific config
            for key in default_config:
                if key in circuit_breaker_config:
                    default_config[key] = circuit_breaker_config[key]
        
        return default_config
    
    def _execute_actions_internal(self, rule_match: RuleMatch, current_content: str) -> DispatchResult:
        """
        Internal method to execute actions for a rule match.
        
        This method contains the actual action execution logic and is wrapped
        by the circuit breaker in dispatch_actions().
        
        Args:
            rule_match: The rule match to process
            current_content: Current file content
            
        Returns:
            DispatchResult with execution details
            
        Raises:
            Exception: Any exception during action execution (will be caught by circuit breaker)
        """
        rule_id = rule_match.rule.id
        file_path = rule_match.file_path
        event_id = rule_match.event_id
        action_results = []
        
        # Process each action in sequence
        for action_config in rule_match.rule.actions:
            action_type = action_config['type']
            params = action_config.get('params', {})
            
            # Get PluginInfo first
            plugin_info = self.plugin_loader.get_plugin(action_type)
            if not plugin_info:
                error = ActionExecutionError(action_type, "Action plugin type not found in PluginLoader")
                action_results.append(ActionResult(
                    action_type=action_type,
                    success=False,
                    error=error
                ))
                logger.error("Action plugin type not found in PluginLoader", action_type=action_type)
                continue

            # Instantiate action with dependencies
            try:
                action = plugin_info.action_class( # Directly instantiate using the class from PluginInfo
                    action_type=plugin_info.action_type, # Use type from PluginInfo
                    params=params,
                    config_manager=self.config_manager,
                    security_manager=self.security_manager
                )
            except Exception as e_inst:
                error = ActionExecutionError(action_type, f"Failed to instantiate action: {str(e_inst)}", original_error=e_inst)
                action_results.append(ActionResult(
                    action_type=action_type,
                    success=False,
                    error=error
                ))
                logger.error("Action plugin instantiation failed", action_type=action_type, params=params, error=str(e_inst), exc_info=True)
                continue

            # Validate parameters using action's own method AND SecurityManager
            # Action's own validation (e.g. required keys, types)
            action_params_valid = True  # Assume true initially
            action_validation_error = "Unknown validation error" # Default error message
            if hasattr(action, 'validate_params') and callable(getattr(action, 'validate_params')):
                try:
                    if not action.validate_params(params): # Expects bool now
                        action_params_valid = False
                        action_validation_error = "Action's validate_params() returned False"
                except ActionExecutionError as e_val: # Catch if action's validation raises error
                    action_params_valid = False
                    action_validation_error = str(e_val)

            if not action_params_valid:
                error = ValidationError(action_type, "params", action_validation_error)
                action_results.append(ActionResult(
                        action_type=action_type,
                        success=False,
                        error=error
                    ))
                logger.error("Action's own parameter validation failed", # Correct indentation
                               action_type=action_type,
                               error=action_validation_error)
                continue # Correct indentation

            # SecurityManager validation (e.g. dangerous values)
            sec_params_valid, sec_validation_error = self.security_manager.validate_action_params(action_type, params)
            if not sec_params_valid:
                error = SecurityViolation("param_validation", sec_validation_error or "SecurityManager validate_action_params() failed", {'action_type': action_type, 'params': params})
                action_results.append(ActionResult(
                    action_type=action_type,
                    success=False,
                    error=error
                ))
                logger.error("Action parameter validation by SecurityManager failed", # Corrected log message
                           action_type=action_type,
                           error=sec_validation_error)
                continue
            
            # Security validation handled by security_manager
            # Other checks like whitelisted action types could be added here if needed,
            # based on a configuration setting (e.g., from self.config_manager).

            # Execute the action
            result = self.execute_action(action, current_content, rule_match.match, file_path, params, event_id)
            action_results.append(result)
            
            # Update statistics
            self._execution_stats['total_actions_executed'] += 1
            
            # If action succeeded, use its output as input for next action
            if result.success and result.modified_content is not None:
                current_content = result.modified_content
            else:
                # If action failed, log but continue with other actions
                logger.warning("Action failed, continuing with remaining actions",
                             action_type=action_type,
                             rule_id=rule_id)
        
        # Create dispatch result
        dispatch_result = DispatchResult(
            rule_id=rule_id,
            file_path=file_path,
            final_content=current_content,
            action_results=action_results
        )
        
        # Check if action chain should trigger circuit breaker
        # Circuit breaker should trip on persistent action failures, not just system failures
        failed_actions = dispatch_result.get_failed_actions()
        total_actions = len(action_results)
        
        if total_actions > 0:
            failure_rate = len(failed_actions) / total_actions
            
            # Trip circuit breaker if:
            # 1. All actions failed, OR
            # 2. More than 50% of actions failed AND there are multiple actions
            should_trip_breaker = (
                failure_rate >= 1.0 or  # All actions failed
                (failure_rate > 0.5 and total_actions > 1)  # >50% failed with multiple actions
            )
            
            if should_trip_breaker:
                # Create a summary of failures for the exception
                failure_summary = []
                for failed_action in failed_actions:
                    failure_summary.append(f"{failed_action.action_type}: {failed_action.error}")
                
                error_message = (
                    f"Action chain failure in rule '{rule_id}': "
                    f"{len(failed_actions)}/{total_actions} actions failed. "
                    f"Failures: {'; '.join(failure_summary)}"
                )
                
                logger.error("Action chain failure triggering circuit breaker",
                           event_id=event_id,
                           rule_id=rule_id,
                           file_path=file_path,
                           failed_actions=len(failed_actions),
                           total_actions=total_actions,
                           failure_rate=failure_rate)
                
                # Raise exception to trigger circuit breaker
                raise ActionChainFailedError(
                    error_message,
                    rule_id=rule_id,
                    failed_actions=len(failed_actions),
                    total_actions=total_actions
                )
        
        return dispatch_result
    
    def quarantine_file(self, file_path: str, rule_id: str, reason: str) -> Dict[str, Any]:
        """
        Quarantine a problematic file by moving it to the quarantine directory.
        
        Args:
            file_path: Path to the file to quarantine
            rule_id: ID of the rule that caused the quarantine
            reason: Reason for quarantine (e.g., "circuit_breaker_open")
            
        Returns:
            Dictionary with quarantine operation result
        """
        try:
            # Convert to Path objects for easier manipulation
            source_path = Path(file_path)
            
            # Check if source file exists
            if not source_path.exists():
                logger.warning("Cannot quarantine file - file does not exist",
                             file_path=file_path,
                             rule_id=rule_id,
                             reason=reason)
                return {
                    "success": False,
                    "error": "File does not exist",
                    "file_path": file_path
                }
            
            # Create quarantine directory structure
            quarantine_base = Path(self.quarantine_path)
            
            # Preserve relative path structure in quarantine
            if source_path.is_absolute():
                # For absolute paths, use relative path from current working directory
                try:
                    relative_path = source_path.relative_to(Path.cwd())
                except ValueError:
                    # If can't make relative, preserve the full path structure
                    # Handle Windows drive letters for cross-platform compatibility
                    path_parts = source_path.parts
                    if len(path_parts) > 1 and ':' in path_parts[0]:
                        # Windows path with drive letter
                        relative_path = Path(*path_parts[1:])
                    else:
                        # Unix-style absolute path
                        relative_path = Path(*path_parts[1:]) if path_parts[0] == '/' else source_path
            else:
                relative_path = source_path
            
            # Ensure relative_path is a Path object
            if isinstance(relative_path, str):
                relative_path = Path(relative_path)
            
            # Create timestamped filename to avoid conflicts
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            stem = relative_path.stem
            suffix = relative_path.suffix
            quarantine_filename = f"{stem}_{timestamp}{suffix}"
            
            # Build full quarantine path
            quarantine_dir = quarantine_base / relative_path.parent
            quarantine_path = quarantine_dir / quarantine_filename
            
            # Create quarantine directory if it doesn't exist
            quarantine_dir.mkdir(parents=True, exist_ok=True)
            
            # Copy file to quarantine (preserving original)
            shutil.copy2(source_path, quarantine_path)
            
            # Create metadata file alongside quarantined file
            metadata_path = quarantine_path.with_suffix(quarantine_path.suffix + ".quarantine_info")
            metadata = {
                "original_path": str(source_path),
                "quarantine_time": datetime.now().isoformat(),
                "rule_id": rule_id,
                "reason": reason,
                "quarantine_path": str(quarantine_path)
            }
            
            with open(metadata_path, 'w') as f:
                import json
                json.dump(metadata, f, indent=2)
            
            # Clean up original file after successful quarantine
            source_path.unlink()
            
            # Update statistics
            self._execution_stats['files_quarantined'] += 1
            
            logger.info("File quarantined successfully",
                       original_path=str(source_path),
                       quarantine_path=str(quarantine_path),
                       rule_id=rule_id,
                       reason=reason)
            
            return {
                "success": True,
                "original_path": str(source_path),
                "quarantine_path": str(quarantine_path),
                "metadata_path": str(metadata_path),
                "rule_id": rule_id,
                "reason": reason
            }
            
        except Exception as e:
            logger.error("Failed to quarantine file",
                        file_path=file_path,
                        rule_id=rule_id,
                        reason=reason,
                        error=str(e),
                        exc_info=True)
            
            return {
                "success": False,
                "error": str(e),
                "file_path": file_path,
                "rule_id": rule_id,
                "reason": reason
            }
    
    def clear_action_cache(self) -> None:
        """Clear action cache - no-op in v2.0 as actions are created per execution."""
        logger.debug("Action cache cleared (no-op in v2.0)")
    
    def get_execution_stats(self) -> Dict[str, Any]:
        """
        Get execution statistics.
        
        Returns:
            Dictionary with execution statistics
        """
        stats = self._execution_stats.copy()
        
        # Calculate derived statistics
        if stats['total_dispatches'] > 0:
            stats['success_rate'] = stats['successful_dispatches'] / stats['total_dispatches']
            stats['average_execution_time'] = stats['total_execution_time'] / stats['total_dispatches']
        else:
            stats['success_rate'] = 0.0
            stats['average_execution_time'] = 0.0
        
        stats['available_action_types'] = list(self.plugin_loader.get_all_plugins().keys())
        
        # Add circuit breaker statistics
        stats['circuit_breaker_stats'] = self.get_circuit_breaker_stats()
        
        # Add quarantine statistics
        stats['quarantine_stats'] = {
            'files_quarantined': stats['files_quarantined'],
            'quarantine_path': self.quarantine_path
        }
        
        return stats
    
    def get_circuit_breaker_stats(self) -> Dict[str, Any]:
        """
        Get circuit breaker statistics.
        
        Returns:
            Dictionary with circuit breaker statistics
        """
        return self.circuit_breaker_manager.get_manager_stats()
    
    def reset_stats(self) -> None:
        """Reset execution statistics."""
        self._execution_stats = {
            'total_dispatches': 0,
            'successful_dispatches': 0,
            'failed_dispatches': 0,
            'total_actions_executed': 0,
            'total_execution_time': 0.0,
            'circuit_breaker_blocks': 0,
            'files_quarantined': 0
        }
        logger.debug("Execution statistics reset") 
```
  ```
  --- END OF FILE scribe/core\action_dispatcher.py ---
  --- START OF FILE scribe/core\adapters\__init__.py ---
  ```
  ```py
# HMA v2.2 Compliant Adapters Module
```
  ```
  --- END OF FILE scribe/core\adapters\__init__.py ---
  --- START OF FILE scribe/core\adapters\nats_adapter.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
NATS EventBus Adapter - HMA v2.2 Tier 2 Recommended Implementation

This adapter implements the EventBusPort interface using NATS as the underlying
message broker, replacing the legacy in-memory queue implementation.
"""

import asyncio
import json
import time
import threading
from typing import Dict, Any, Optional, List, Callable
from collections import defaultdict
import nats
from nats.aio.client import Client as NATSClient
from nats.aio.errors import ErrConnectionClosed, ErrTimeout, ErrNoServers

from ..hma_ports import EventBusPort
from ..hma_telemetry import HMATelemetry
from ..logging_config import get_scribe_logger

logger = get_scribe_logger(__name__)


class NatsEventBusAdapter(EventBusPort):
    """
    NATS-based EventBus implementation as recommended by HMA v2.2 Tier 2 technologies.
    
    This adapter replaces the legacy in-memory queue with a production-grade
    NATS message broker for improved scalability and resilience.
    """
    
    def __init__(self, 
                 telemetry: HMATelemetry,
                 nats_url: str = "nats://localhost:4222",
                 max_reconnect_attempts: int = -1,
                 reconnect_time_wait: int = 2):
        """
        Initialize NATS EventBus adapter.
        
        Args:
            telemetry: HMA telemetry instance
            nats_url: NATS server URL
            max_reconnect_attempts: Maximum reconnection attempts (-1 for unlimited)
            reconnect_time_wait: Wait time between reconnection attempts
        """
        self.telemetry = telemetry
        self.nats_url = nats_url
        self.max_reconnect_attempts = max_reconnect_attempts
        self.reconnect_time_wait = reconnect_time_wait
        
        # NATS client and connection management
        self.nats_client: Optional[NATSClient] = None
        self.connected = False
        self.running = False
        
        # Subscription management
        self.subscribers: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        self.nats_subscriptions: Dict[str, Any] = {}
        self.lock = threading.RLock()
        
        # Statistics tracking
        self.events_published = 0
        self.events_delivered = 0
        self.events_dropped = 0
        self.connection_errors = 0
        
        logger.info("NATS EventBus adapter initialized",
                   nats_url=nats_url,
                   max_reconnect_attempts=max_reconnect_attempts)
    
    async def start(self) -> bool:
        """Start the NATS EventBus adapter."""
        if self.running:
            logger.warning("NATS EventBus adapter already running")
            return True
        
        try:
            # Create NATS client
            self.nats_client = nats.connect(
                servers=[self.nats_url],
                max_reconnect_attempts=self.max_reconnect_attempts,
                reconnect_time_wait=self.reconnect_time_wait,
                disconnected_cb=self._on_disconnected,
                reconnected_cb=self._on_reconnected,
                error_cb=self._on_error,
                closed_cb=self._on_closed
            )
            
            # Connect to NATS server
            await self.nats_client
            self.connected = True
            self.running = True
            
            logger.info("NATS EventBus adapter started successfully",
                       server_info=self.nats_client.server_info)
            
            # Record telemetry
            self.telemetry.emit_metric(
                "hma_nats_connection_total", 1.0,
                {"status": "connected", "server": self.nats_url}
            )
            
            return True
            
        except Exception as e:
            logger.error("Failed to start NATS EventBus adapter",
                        error=str(e),
                        nats_url=self.nats_url)
            self.connection_errors += 1
            self.telemetry.record_error("nats_connection_failed", "nats_adapter", str(e))
            return False
    
    async def stop(self) -> None:
        """Stop the NATS EventBus adapter."""
        if not self.running:
            return
        
        try:
            self.running = False
            
            # Unsubscribe from all subjects
            for subject, subscription in self.nats_subscriptions.items():
                try:
                    await subscription.unsubscribe()
                    logger.debug("Unsubscribed from NATS subject", subject=subject)
                except Exception as e:
                    logger.warning("Failed to unsubscribe from subject",
                                 subject=subject,
                                 error=str(e))
            
            self.nats_subscriptions.clear()
            
            # Close NATS connection
            if self.nats_client and self.connected:
                await self.nats_client.close()
                self.connected = False
                logger.info("NATS connection closed")
            
            # Record final statistics
            self._log_final_stats()
            
            logger.info("NATS EventBus adapter stopped successfully")
            
        except Exception as e:
            logger.error("Error stopping NATS EventBus adapter", error=str(e))
    
    async def publish_event(self, 
                          event_type: str,
                          event_data: Dict[str, Any], 
                          target: Optional[str] = None,
                          correlation_id: Optional[str] = None) -> bool:
        """Publish event to NATS broker."""
        
        # HMA v2.2 mandatory OTEL boundary telemetry
        with self.telemetry.start_span("nats_publish_event_boundary", event_type) as span:
            if hasattr(span, 'set_attribute'):
                span.set_attribute("hma.boundary.type", "event_bus")
                span.set_attribute("hma.operation", "publish_event")
                span.set_attribute("hma.event.type", event_type)
                span.set_attribute("hma.event.target", target or "broadcast")
                span.set_attribute("hma.message_broker", "nats")
        
        if not self.connected or not self.nats_client:
            logger.error("Cannot publish event - NATS not connected",
                        event_type=event_type)
            self.events_dropped += 1
            return False
        
        try:
            # Create HMA-compliant event structure
            event = {
                "eventId": correlation_id or f"event_{int(time.time() * 1000000)}",
                "eventType": event_type,
                "eventVersion": "2.2",
                "source": "scribe-core",
                "timestamp": time.time(),
                "data": event_data
            }
            
            # Determine NATS subject
            subject = f"scribe.events.{event_type}"
            if target:
                subject = f"scribe.events.{event_type}.{target}"
            
            # Serialize event data
            event_json = json.dumps(event).encode('utf-8')
            
            # Publish to NATS
            await self.nats_client.publish(subject, event_json)
            
            self.events_published += 1
            
            # Record telemetry
            self.telemetry.emit_metric(
                "hma_events_published_total", 1.0,
                {"event_type": event_type, "target": target or "broadcast", "broker": "nats"}
            )
            
            logger.debug("Event published to NATS",
                        event_type=event_type,
                        subject=subject,
                        target=target,
                        correlation_id=correlation_id)
            
            return True
            
        except Exception as e:
            logger.error("Failed to publish event to NATS",
                        event_type=event_type,
                        error=str(e))
            self.events_dropped += 1
            self.telemetry.emit_metric(
                "hma_events_dropped_total", 1.0,
                {"event_type": event_type, "reason": "nats_publish_error"}
            )
            return False
    
    async def subscribe_to_events(self, 
                                event_types: List[str], 
                                callback: Callable,
                                subscriber_id: str) -> bool:
        """Subscribe to specific event types via NATS."""
        
        # HMA v2.2 mandatory OTEL boundary telemetry
        with self.telemetry.start_span("nats_subscribe_boundary", subscriber_id) as span:
            if hasattr(span, 'set_attribute'):
                span.set_attribute("hma.boundary.type", "event_bus")
                span.set_attribute("hma.operation", "subscribe")
                span.set_attribute("hma.subscriber.id", subscriber_id)
                span.set_attribute("hma.message_broker", "nats")
        
        if not self.connected or not self.nats_client:
            logger.error("Cannot subscribe to events - NATS not connected",
                        subscriber_id=subscriber_id,
                        event_types=event_types)
            return False
        
        try:
            with self.lock:
                for event_type in event_types:
                    # Add to local subscribers tracking
                    self.subscribers[event_type].append({
                        "callback": callback,
                        "subscriber_id": subscriber_id
                    })
                    
                    # Create NATS subscription if not exists
                    subject = f"scribe.events.{event_type}"
                    if subject not in self.nats_subscriptions:
                        subscription = await self.nats_client.subscribe(
                            subject,
                            cb=self._create_nats_callback(event_type)
                        )
                        self.nats_subscriptions[subject] = subscription
                        
                        logger.debug("Created NATS subscription",
                                   subject=subject,
                                   event_type=event_type)
            
            logger.info("Subscribed to NATS events",
                       subscriber_id=subscriber_id,
                       event_types=event_types)
            return True
            
        except Exception as e:
            logger.error("Failed to subscribe to NATS events",
                        subscriber_id=subscriber_id,
                        event_types=event_types,
                        error=str(e))
            return False
    
    async def unsubscribe_from_events(self, 
                                    event_types: List[str],
                                    subscriber_id: str) -> bool:
        """Unsubscribe from event types."""
        
        try:
            with self.lock:
                for event_type in event_types:
                    # Remove from local subscribers
                    self.subscribers[event_type] = [
                        sub for sub in self.subscribers[event_type]
                        if sub["subscriber_id"] != subscriber_id
                    ]
                    
                    # If no more local subscribers, remove NATS subscription
                    if not self.subscribers[event_type]:
                        subject = f"scribe.events.{event_type}"
                        if subject in self.nats_subscriptions:
                            await self.nats_subscriptions[subject].unsubscribe()
                            del self.nats_subscriptions[subject]
                            
                            logger.debug("Removed NATS subscription",
                                       subject=subject,
                                       event_type=event_type)
            
            logger.info("Unsubscribed from NATS events",
                       subscriber_id=subscriber_id,
                       event_types=event_types)
            return True
            
        except Exception as e:
            logger.error("Failed to unsubscribe from NATS events",
                        subscriber_id=subscriber_id,
                        event_types=event_types,
                        error=str(e))
            return False
    
    def _create_nats_callback(self, event_type: str) -> Callable:
        """Create NATS message callback for event type."""
        
        async def nats_callback(msg):
            """Handle NATS message and deliver to subscribers."""
            try:
                # Parse event data
                event_data = json.loads(msg.data.decode('utf-8'))
                
                # Validate HMA event structure
                if not self._validate_hma_event(event_data):
                    logger.warning("Received invalid HMA event structure",
                                 event_type=event_type,
                                 subject=msg.subject)
                    return
                
                # Deliver to all subscribers for this event type
                with self.lock:
                    subscribers = self.subscribers.get(event_type, [])
                
                delivered = 0
                for subscriber in subscribers:
                    try:
                        # Call subscriber callback
                        callback = subscriber["callback"]
                        if asyncio.iscoroutinefunction(callback):
                            await callback(event_data)
                        else:
                            callback(event_data)
                        
                        delivered += 1
                        
                    except Exception as e:
                        logger.error("Error delivering NATS event to subscriber",
                                   event_type=event_type,
                                   subscriber_id=subscriber["subscriber_id"],
                                   error=str(e))
                
                self.events_delivered += delivered
                
                logger.debug("NATS event delivered",
                           event_type=event_type,
                           subscribers_delivered=delivered,
                           event_id=event_data.get("eventId"))
                
            except Exception as e:
                logger.error("Error processing NATS message",
                           event_type=event_type,
                           subject=msg.subject,
                           error=str(e))
        
        return nats_callback
    
    def _validate_hma_event(self, event_data: Dict[str, Any]) -> bool:
        """Validate HMA v2.2 event structure."""
        required_fields = ["eventId", "eventType", "eventVersion", "source", "timestamp", "data"]
        
        for field in required_fields:
            if field not in event_data:
                logger.warning("Missing required HMA event field", field=field)
                return False
        
        return True
    
    def get_event_statistics(self) -> Dict[str, Any]:
        """Get NATS event bus statistics."""
        return {
            "events_published": self.events_published,
            "events_delivered": self.events_delivered,
            "events_dropped": self.events_dropped,
            "connection_errors": self.connection_errors,
            "connected": self.connected,
            "running": self.running,
            "nats_url": self.nats_url,
            "subscriber_count": sum(len(subs) for subs in self.subscribers.values()),
            "event_types": list(self.subscribers.keys()),
            "nats_subscriptions": len(self.nats_subscriptions),
            "broker_type": "nats"
        }
    
    # Backward compatibility methods for legacy tests
    def put(self, event, timeout=None):
        """Legacy method for putting events (backward compatibility)."""
        logger.warning("Using legacy put() method - consider migrating to publish_event()")
        
        # Convert legacy event to modern format
        if isinstance(event, dict):
            event_type = event.get('type', event.get('event_type', 'unknown'))
            event_data = event.get('data', event)
        else:
            event_type = 'legacy_event'
            event_data = {"legacy_event": event}
        
        # Use asyncio to run the async publish method
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If we're already in an async context, schedule the coroutine
                asyncio.create_task(self.publish_event(event_type, event_data))
                return True
            else:
                # If not in async context, run until complete
                return loop.run_until_complete(self.publish_event(event_type, event_data))
        except Exception as e:
            logger.error("Legacy put() method failed", error=str(e))
            return False
    
    def qsize(self) -> int:
        """Legacy method for getting queue size (NATS doesn't have traditional queue size)."""
        # For NATS, we return the number of active subscriptions as a proxy
        return len(self.nats_subscriptions)
    
    def _log_final_stats(self) -> None:
        """Log final NATS adapter statistics."""
        if self.events_published > 0 or self.events_delivered > 0:
            stats = self.get_event_statistics()
            logger.info("NATS EventBus final statistics", **stats)
    
    # NATS connection event handlers
    async def _on_disconnected(self):
        """Handle NATS disconnection."""
        logger.warning("NATS connection lost")
        self.connected = False
        self.connection_errors += 1
        self.telemetry.emit_metric(
            "hma_nats_disconnections_total", 1.0,
            {"server": self.nats_url}
        )
    
    async def _on_reconnected(self):
        """Handle NATS reconnection."""
        logger.info("NATS connection restored")
        self.connected = True
        self.telemetry.emit_metric(
            "hma_nats_reconnections_total", 1.0,
            {"server": self.nats_url}
        )
    
    async def _on_error(self, error):
        """Handle NATS errors."""
        logger.error("NATS error occurred", error=str(error))
        self.connection_errors += 1
        self.telemetry.record_error("nats_error", "nats_adapter", str(error))
    
    async def _on_closed(self):
        """Handle NATS connection closed."""
        logger.info("NATS connection closed")
        self.connected = False
```
  ```
  --- END OF FILE scribe/core\adapters\nats_adapter.py ---
  --- START OF FILE scribe/core\async_processor.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Engine Async Processing Pipeline

Implements high-performance asynchronous processing for file events and actions
with concurrent execution, backpressure handling, and resource management.
"""

import asyncio
import threading
import time
from typing import Dict, Any, Optional, List, Callable, Union, Awaitable
from dataclasses import dataclass
from enum import Enum
import structlog
from concurrent.futures import ThreadPoolExecutor

from .logging_config import get_scribe_logger
from .telemetry import get_telemetry_manager
from .error_recovery import handle_error

logger = get_scribe_logger(__name__)


class TaskPriority(Enum):
    """Task priority levels for processing order."""
    LOW = 1
    NORMAL = 5
    HIGH = 8
    CRITICAL = 10


class TaskStatus(Enum):
    """Task execution status."""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class AsyncTask:
    """Represents an asynchronous task in the processing pipeline."""
    task_id: str
    task_type: str
    priority: TaskPriority
    payload: Dict[str, Any]
    callback: Optional[Callable] = None
    created_at: float = None
    started_at: Optional[float] = None
    completed_at: Optional[float] = None
    status: TaskStatus = TaskStatus.PENDING
    retry_count: int = 0
    max_retries: int = 3
    error: Optional[Exception] = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = time.time()
    
    @property
    def duration(self) -> Optional[float]:
        """Get task execution duration."""
        if self.started_at and self.completed_at:
            return self.completed_at - self.started_at
        return None
    
    @property
    def wait_time(self) -> Optional[float]:
        """Get time spent waiting in queue."""
        if self.started_at:
            return self.started_at - self.created_at
        return None


class AsyncTaskQueue:
    """Priority queue for async tasks with backpressure handling."""
    
    def __init__(self, max_size: int = 1000):
        """
        Initialize async task queue.
        
        Args:
            max_size: Maximum queue size for backpressure
        """
        self.max_size = max_size
        self._queue = asyncio.PriorityQueue(maxsize=max_size)
        self._lock = asyncio.Lock()
        self._task_counter = 0
        
        logger.debug("AsyncTaskQueue initialized", max_size=max_size)
    
    async def put(self, task: AsyncTask) -> bool:
        """
        Add task to queue with backpressure handling.
        
        Args:
            task: Task to add
            
        Returns:
            True if task was added, False if queue is full
        """
        try:
            # Check queue size for backpressure
            if self._queue.qsize() >= self.max_size:
                logger.warning("Task queue full, dropping task",
                             task_id=task.task_id,
                             queue_size=self._queue.qsize())
                return False
            
            # Priority queue item: (priority, insertion_order, task)
            priority_value = -task.priority.value  # Negative for descending order
            async with self._lock:
                self._task_counter += 1
                await self._queue.put((priority_value, self._task_counter, task))
            
            logger.debug("Task queued",
                        task_id=task.task_id,
                        priority=task.priority.value,
                        queue_size=self._queue.qsize())
            return True
            
        except Exception as e:
            logger.error("Error queuing task",
                        task_id=task.task_id,
                        error=str(e))
            return False
    
    async def get(self) -> Optional[AsyncTask]:
        """Get next task from queue."""
        try:
            priority, order, task = await self._queue.get()
            
            logger.debug("Task dequeued",
                        task_id=task.task_id,
                        priority=task.priority.value,
                        queue_size=self._queue.qsize())
            
            return task
            
        except Exception as e:
            logger.error("Error dequeuing task", error=str(e))
            return None
    
    def qsize(self) -> int:
        """Get current queue size."""
        return self._queue.qsize()
    
    def empty(self) -> bool:
        """Check if queue is empty."""
        return self._queue.empty()
    
    def full(self) -> bool:
        """Check if queue is full."""
        return self._queue.qsize() >= self.max_size


class AsyncProcessor:
    """
    High-performance async processor for Scribe Engine.
    
    Implements concurrent processing with configurable worker pools,
    priority queues, backpressure handling, and resource management.
    """
    
    def __init__(self,
                 max_workers: int = 10,
                 max_queue_size: int = 1000,
                 worker_timeout: float = 300.0):
        """
        Initialize async processor.
        
        Args:
            max_workers: Maximum number of async workers
            max_queue_size: Maximum task queue size
            worker_timeout: Timeout for worker tasks
        """
        self.max_workers = max_workers
        self.max_queue_size = max_queue_size
        self.worker_timeout = worker_timeout
        
        # Async components
        self._task_queue = AsyncTaskQueue(max_queue_size)
        self._workers: List[asyncio.Task] = []
        self._running = False
        self._loop: Optional[asyncio.AbstractEventLoop] = None
        self._thread: Optional[threading.Thread] = None
        
        # Thread pool for CPU-bound tasks
        self._thread_pool = ThreadPoolExecutor(max_workers=max_workers)
        
        # Task registry and stats
        self._active_tasks: Dict[str, AsyncTask] = {}
        self._completed_tasks: Dict[str, AsyncTask] = {}
        self._task_handlers: Dict[str, Callable] = {}
        
        # Metrics
        self._stats = {
            "tasks_processed": 0,
            "tasks_failed": 0,
            "tasks_cancelled": 0,
            "total_processing_time": 0.0,
            "avg_processing_time": 0.0,
            "queue_high_water_mark": 0
        }
        
        # Thread safety
        self._lock = threading.RLock()
        
        logger.info("AsyncProcessor initialized",
                   max_workers=max_workers,
                   max_queue_size=max_queue_size,
                   worker_timeout=worker_timeout)
    
    def register_handler(self, task_type: str, handler: Callable):
        """
        Register a handler function for a task type.
        
        Args:
            task_type: Type of task to handle
            handler: Async or sync function to handle the task
        """
        with self._lock:
            self._task_handlers[task_type] = handler
            logger.debug("Registered task handler",
                        task_type=task_type,
                        handler_name=handler.__name__)
    
    def start(self):
        """Start the async processor."""
        if self._running:
            return
        
        self._running = True
        
        # Start async event loop in separate thread
        self._thread = threading.Thread(
            target=self._run_event_loop,
            name="AsyncProcessor",
            daemon=True
        )
        self._thread.start()
        
        logger.info("AsyncProcessor started")
    
    def stop(self):
        """Stop the async processor gracefully."""
        if not self._running:
            return
        
        self._running = False
        
        # Stop event loop
        if self._loop and not self._loop.is_closed():
            self._loop.call_soon_threadsafe(self._loop.stop)
        
        # Wait for thread to finish
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=10.0)
        
        # Shutdown thread pool
        self._thread_pool.shutdown(wait=True)
        
        logger.info("AsyncProcessor stopped")
    
    def _run_event_loop(self):
        """Run the async event loop."""
        try:
            # Create new event loop for this thread
            self._loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self._loop)
            
            # Start worker tasks
            for i in range(self.max_workers):
                worker = self._loop.create_task(self._worker(f"worker-{i}"))
                self._workers.append(worker)
            
            # Start stats collector
            stats_task = self._loop.create_task(self._stats_collector())
            
            # Run until stopped
            self._loop.run_forever()
            
            # Cancel all tasks when stopping
            for worker in self._workers:
                worker.cancel()
            
            stats_task.cancel()
            
            # Wait for tasks to complete
            pending = [task for task in self._workers + [stats_task] if not task.done()]
            if pending:
                self._loop.run_until_complete(
                    asyncio.gather(*pending, return_exceptions=True)
                )
            
        except Exception as e:
            logger.error("Error in async event loop", error=str(e), exc_info=True)
        finally:
            if self._loop and not self._loop.is_closed():
                self._loop.close()
    
    async def _worker(self, worker_name: str):
        """Async worker that processes tasks from the queue."""
        logger.debug("Async worker started", worker_name=worker_name)
        
        try:
            while self._running:
                try:
                    # Get next task from queue
                    task = await asyncio.wait_for(
                        self._task_queue.get(),
                        timeout=1.0
                    )
                    
                    if task is None:
                        continue
                    
                    # Process the task
                    await self._process_task(task, worker_name)
                    
                except asyncio.TimeoutError:
                    # No task available, continue
                    continue
                except Exception as e:
                    logger.error("Worker error",
                                worker_name=worker_name,
                                error=str(e),
                                exc_info=True)
        
        except asyncio.CancelledError:
            logger.debug("Async worker cancelled", worker_name=worker_name)
        except Exception as e:
            logger.error("Async worker failed",
                        worker_name=worker_name,
                        error=str(e),
                        exc_info=True)
        finally:
            logger.debug("Async worker stopped", worker_name=worker_name)
    
    async def _process_task(self, task: AsyncTask, worker_name: str):
        """Process a single task."""
        task.status = TaskStatus.RUNNING
        task.started_at = time.time()
        
        # Add to active tasks
        with self._lock:
            self._active_tasks[task.task_id] = task
        
        # Record telemetry
        telemetry = get_telemetry_manager()
        
        try:
            logger.debug("Processing task",
                        task_id=task.task_id,
                        task_type=task.task_type,
                        worker_name=worker_name,
                        wait_time=task.wait_time)
            
            # Get handler for task type
            handler = self._task_handlers.get(task.task_type)
            if not handler:
                raise ValueError(f"No handler registered for task type: {task.task_type}")
            
            # Execute handler with timeout
            if asyncio.iscoroutinefunction(handler):
                # Async handler
                result = await asyncio.wait_for(
                    handler(task),
                    timeout=self.worker_timeout
                )
            else:
                # Sync handler - run in thread pool
                result = await self._loop.run_in_executor(
                    self._thread_pool,
                    handler,
                    task
                )
            
            # Task completed successfully
            task.status = TaskStatus.COMPLETED
            task.completed_at = time.time()
            
            # Call callback if provided
            if task.callback:
                try:
                    if asyncio.iscoroutinefunction(task.callback):
                        await task.callback(task, result)
                    else:
                        await self._loop.run_in_executor(
                            self._thread_pool,
                            task.callback,
                            task,
                            result
                        )
                except Exception as e:
                    logger.warning("Task callback failed",
                                  task_id=task.task_id,
                                  error=str(e))
            
            # Update stats
            with self._lock:
                self._stats["tasks_processed"] += 1
                if task.duration:
                    self._stats["total_processing_time"] += task.duration
                    self._stats["avg_processing_time"] = (
                        self._stats["total_processing_time"] / self._stats["tasks_processed"]
                    )
            
            # Record successful telemetry
            if telemetry:
                telemetry.action_executions_counter.add(1, {
                    "task_type": task.task_type,
                    "status": "success",
                    "worker": worker_name
                })
                
                if task.duration:
                    telemetry.action_duration_histogram.record(task.duration, {
                        "task_type": task.task_type,
                        "status": "success"
                    })
            
            logger.debug("Task completed successfully",
                        task_id=task.task_id,
                        duration=task.duration,
                        worker_name=worker_name)
        
        except asyncio.TimeoutError:
            task.status = TaskStatus.FAILED
            task.error = Exception(f"Task timeout after {self.worker_timeout}s")
            task.completed_at = time.time()
            
            with self._lock:
                self._stats["tasks_failed"] += 1
            
            logger.error("Task timed out",
                        task_id=task.task_id,
                        timeout=self.worker_timeout,
                        worker_name=worker_name)
            
            # Handle error through recovery system
            handle_error("async_processor", "task_execution", task.error, {
                "task_id": task.task_id,
                "task_type": task.task_type,
                "worker": worker_name
            })
        
        except Exception as e:
            task.status = TaskStatus.FAILED
            task.error = e
            task.completed_at = time.time()
            
            with self._lock:
                self._stats["tasks_failed"] += 1
            
            # Record failed telemetry
            if telemetry:
                telemetry.action_failures_counter.add(1, {
                    "task_type": task.task_type,
                    "error_type": type(e).__name__,
                    "worker": worker_name
                })
                
                if task.duration:
                    telemetry.action_duration_histogram.record(task.duration, {
                        "task_type": task.task_type,
                        "status": "error"
                    })
            
            logger.error("Task failed",
                        task_id=task.task_id,
                        task_type=task.task_type,
                        error=str(e),
                        worker_name=worker_name,
                        exc_info=True)
            
            # Handle error through recovery system
            handle_error("async_processor", "task_execution", e, {
                "task_id": task.task_id,
                "task_type": task.task_type,
                "worker": worker_name
            })
        
        finally:
            # Move from active to completed
            with self._lock:
                if task.task_id in self._active_tasks:
                    del self._active_tasks[task.task_id]
                self._completed_tasks[task.task_id] = task
                
                # Limit completed tasks history
                if len(self._completed_tasks) > 1000:
                    oldest_task_id = min(self._completed_tasks.keys(), 
                                       key=lambda k: self._completed_tasks[k].created_at)
                    del self._completed_tasks[oldest_task_id]
    
    async def _stats_collector(self):
        """Collect and update processor statistics."""
        try:
            while self._running:
                await asyncio.sleep(10.0)  # Update every 10 seconds
                
                # Update queue metrics
                queue_size = self._task_queue.qsize()
                with self._lock:
                    if queue_size > self._stats["queue_high_water_mark"]:
                        self._stats["queue_high_water_mark"] = queue_size
                
                # Update telemetry
                telemetry = get_telemetry_manager()
                if telemetry:
                    telemetry.queue_size_gauge.set(queue_size)
                    telemetry.active_workers_gauge.set(len(self._active_tasks))
                
                logger.debug("Processor stats updated",
                           queue_size=queue_size,
                           active_tasks=len(self._active_tasks),
                           completed_tasks=len(self._completed_tasks))
        
        except asyncio.CancelledError:
            pass
        except Exception as e:
            logger.error("Stats collector error", error=str(e))
    
    def submit_task(self,
                   task_type: str,
                   payload: Dict[str, Any],
                   priority: TaskPriority = TaskPriority.NORMAL,
                   callback: Optional[Callable] = None,
                   task_id: Optional[str] = None) -> str:
        """
        Submit a task for async processing.
        
        Args:
            task_type: Type of task
            payload: Task payload data
            priority: Task priority
            callback: Optional callback function
            task_id: Optional custom task ID
            
        Returns:
            Task ID for tracking
        """
        if task_id is None:
            task_id = f"{task_type}_{int(time.time() * 1000000)}"
        
        task = AsyncTask(
            task_id=task_id,
            task_type=task_type,
            priority=priority,
            payload=payload,
            callback=callback
        )
        
        # Submit to event loop
        if self._loop and self._running:
            future = asyncio.run_coroutine_threadsafe(
                self._task_queue.put(task),
                self._loop
            )
            
            try:
                success = future.result(timeout=1.0)
                if success:
                    logger.debug("Task submitted",
                               task_id=task_id,
                               task_type=task_type,
                               priority=priority.value)
                    return task_id
                else:
                    raise Exception("Failed to queue task - queue full")
            except Exception as e:
                logger.error("Failed to submit task",
                           task_id=task_id,
                           error=str(e))
                raise
        else:
            raise RuntimeError("AsyncProcessor not running")
    
    def get_task_status(self, task_id: str) -> Optional[TaskStatus]:
        """Get status of a task."""
        with self._lock:
            if task_id in self._active_tasks:
                return self._active_tasks[task_id].status
            elif task_id in self._completed_tasks:
                return self._completed_tasks[task_id].status
        return None
    
    def get_stats(self) -> Dict[str, Any]:
        """Get processor statistics."""
        with self._lock:
            return {
                **self._stats.copy(),
                "active_tasks": len(self._active_tasks),
                "completed_tasks": len(self._completed_tasks),
                "queue_size": self._task_queue.qsize(),
                "workers_count": len(self._workers),
                "running": self._running
            }


# Global async processor instance
_async_processor: Optional[AsyncProcessor] = None
_processor_lock = threading.RLock()


def get_async_processor() -> Optional[AsyncProcessor]:
    """Get the global async processor instance."""
    return _async_processor


def initialize_async_processor(max_workers: int = 10,
                             max_queue_size: int = 1000,
                             worker_timeout: float = 300.0) -> AsyncProcessor:
    """
    Initialize global async processor.
    
    Args:
        max_workers: Maximum number of async workers
        max_queue_size: Maximum task queue size
        worker_timeout: Timeout for worker tasks
        
    Returns:
        AsyncProcessor instance
    """
    global _async_processor
    
    with _processor_lock:
        if _async_processor is None:
            _async_processor = AsyncProcessor(
                max_workers=max_workers,
                max_queue_size=max_queue_size,
                worker_timeout=worker_timeout
            )
            _async_processor.start()
        
        return _async_processor


def shutdown_async_processor():
    """Shutdown the global async processor."""
    global _async_processor
    
    with _processor_lock:
        if _async_processor:
            _async_processor.stop()
            _async_processor = None
```
  ```
  --- END OF FILE scribe/core\async_processor.py ---
  --- START OF FILE scribe/core\atomic_write.py ---
  ```
  ```py
"""
Atomic file write utility for crash-safe file operations.

This module implements the write-temp -> fsync -> rename pattern to ensure
that file writes are atomic and crash-safe. If the process is interrupted
during a write operation, the original file remains unchanged.
"""

import os
import tempfile
from pathlib import Path
from typing import Union, BinaryIO, TextIO
import structlog
from .logging_config import get_scribe_logger
import time
import portalocker

logger = get_scribe_logger(__name__)


def atomic_write(filepath: Union[str, Path], data: Union[str, bytes], 
                 encoding: str = 'utf-8', mode: str = 'w') -> bool:
    """
    Write data to a file atomically using the write-temp -> fsync -> rename pattern.
    
    This function ensures that either the entire write operation succeeds, or the
    original file remains completely unchanged. It protects against corruption
    from power loss, process crashes, or other interruptions.
    
    Args:
        filepath: Path to the target file
        data: Data to write (string or bytes)
        encoding: Text encoding to use (ignored for binary mode)
        mode: Write mode ('w' for text, 'wb' for binary)
        
    Returns:
        bool: True if write succeeded, False otherwise
        
    Raises:
        ValueError: If mode is not supported
        OSError: If file operations fail
    """
    filepath = Path(filepath)
    
    # Validate mode
    if mode not in ('w', 'wb'):
        raise ValueError(f"Unsupported mode '{mode}'. Use 'w' for text or 'wb' for binary.")
    
    # Determine if we're writing text or binary
    is_binary = mode == 'wb'
    
    # Create temporary file in the same directory as target
    # This ensures the rename operation is atomic on POSIX systems
    temp_dir = filepath.parent
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    temp_fd = None
    temp_path = None
    
    try:
        # Create temporary file in same directory as target
        temp_fd, temp_path = tempfile.mkstemp(
            dir=temp_dir,
            prefix=f".{filepath.name}.tmp.",
            suffix=".atomic"
        )
        temp_path = Path(temp_path)
        
        logger.debug("atomic_write_started", 
                    target_file=str(filepath),
                    temp_file=str(temp_path),
                    data_size=len(data),
                    mode=mode)
        
        # Write data to temporary file with proper Windows file handle management
        try:
            if is_binary:
                if isinstance(data, str):
                    data = data.encode(encoding)
                # Use low-level file operations for binary mode
                os.write(temp_fd, data)
            else:
                if isinstance(data, bytes):
                    data = data.decode(encoding)
                # Use low-level file operations for text mode
                os.write(temp_fd, data.encode(encoding))
            
            # Force write to disk
            os.fsync(temp_fd)
            
        except Exception as write_error:
            logger.error("temp_file_write_failed",
                        temp_file=str(temp_path),
                        error=str(write_error))
            raise
        
        # Close the file descriptor before rename (Windows requirement)
        os.close(temp_fd)
        temp_fd = None
        
        # Atomic rename with simple retry logic for Windows
        max_retries = 5
        for retry in range(max_retries):
            try:
                # Perform atomic rename
                os.replace(str(temp_path), str(filepath))
                break
                
            except (OSError, PermissionError) as e:
                if retry == max_retries - 1:
                    logger.error("atomic_rename_failed_final",
                               temp_file=str(temp_path),
                               target_file=str(filepath),
                               retry_count=retry + 1,
                               error=str(e))
                    raise
                
                logger.debug("atomic_rename_retry",
                            temp_file=str(temp_path),
                            target_file=str(filepath),
                            retry_count=retry + 1,
                            error=str(e))
                time.sleep(min(1, 0.1 * (2 ** retry)))  # Exponential backoff

        logger.info("atomic_write_completed",
                   target_file=str(filepath),
                   data_size=len(data),
                   mode=mode)
        
        return True
        
    except Exception as e:
        logger.error("atomic_write_failed",
                    target_file=str(filepath),
                    temp_file=str(temp_path) if temp_path else None,
                    error=str(e),
                    error_type=type(e).__name__)
        
        # Clean up temporary file if it exists
        if temp_path and temp_path.exists():
            try:
                temp_path.unlink()
                logger.debug("temp_file_cleaned_up", temp_file=str(temp_path))
            except Exception as cleanup_error:
                logger.warning("temp_file_cleanup_failed",
                             temp_file=str(temp_path),
                             cleanup_error=str(cleanup_error))
        
        return False
        
    finally:
        # Ensure file descriptor is closed
        if temp_fd is not None:
            try:
                os.close(temp_fd)
            except Exception:
                pass  # Already closed or invalid


def atomic_write_json(filepath: Union[str, Path], data: dict, 
                      indent: int = 2, ensure_ascii: bool = False) -> bool:
    """
    Write JSON data to a file atomically.
    
    Args:
        filepath: Path to the target file
        data: Dictionary to write as JSON
        indent: JSON indentation level
        ensure_ascii: Whether to escape non-ASCII characters
        
    Returns:
        bool: True if write succeeded, False otherwise
    """
    import json
    
    try:
        json_data = json.dumps(data, indent=indent, ensure_ascii=ensure_ascii)
        return atomic_write(filepath, json_data, mode='w')
    except (TypeError, ValueError) as e:
        logger.error("json_serialization_failed",
                    target_file=str(filepath),
                    error=str(e))
        return False


def atomic_write_yaml(filepath: Union[str, Path], data: dict) -> bool:
    """
    Write YAML data to a file atomically.
    
    Args:
        filepath: Path to the target file
        data: Dictionary to write as YAML
        
    Returns:
        bool: True if write succeeded, False otherwise
    """
    try:
        import yaml
        yaml_data = yaml.dump(data, default_flow_style=False, sort_keys=False)
        return atomic_write(filepath, yaml_data, mode='w')
    except ImportError:
        logger.error("yaml_module_not_available", target_file=str(filepath))
        return False
    except yaml.YAMLError as e:
        logger.error("yaml_serialization_failed",
                    target_file=str(filepath),
                    error=str(e))
        return False


class AtomicWriteTestHelper:
    """Test helper class for simulating atomic write failures and interruptions"""
    
    def __init__(self):
        self.simulate_interruption = False
        self.simulate_fsync_failure = False
        self.simulate_rename_failure = False
        self.interruption_point = None
    
    def simulate_interruption_during_write(self, interruption_point='write'):
        """Simulate interruption at different points during write operation
        
        Args:
            interruption_point: Where to simulate failure ('write', 'fsync', 'rename')
        """
        self.simulate_interruption = True
        self.interruption_point = interruption_point
    
    def reset_simulation(self):
        """Reset all simulation flags"""
        self.simulate_interruption = False
        self.simulate_fsync_failure = False
        self.simulate_rename_failure = False
        self.interruption_point = None
    
    def should_fail_at_point(self, point):
        """Check if we should simulate failure at given point"""
        return self.simulate_interruption and self.interruption_point == point 
```
  ```
  --- END OF FILE scribe/core\atomic_write.py ---
  --- START OF FILE scribe/core\boundary_validator.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
HMA v2.1 Boundary Validation System

Validates data at all HMA boundary interfaces to ensure compliance
with JSON Schema requirements and event schema validation.
"""

import jsonschema
import json
import time
import uuid
from typing import Dict, Any, Optional, List
from pathlib import Path
from dataclasses import dataclass
from enum import Enum

from .logging_config import get_scribe_logger
from .hma_ports import ObservabilityPort

logger = get_scribe_logger(__name__)

class BoundaryType(Enum):
    """Types of HMA boundaries"""
    L1_INTERFACE = "l1_interface"
    L2_CORE = "l2_core"
    L3_PLUGIN = "l3_plugin"
    L4_INFRASTRUCTURE = "l4_infrastructure"

@dataclass
class ValidationResult:
    """Result of boundary validation"""
    valid: bool
    errors: List[str]
    boundary_type: BoundaryType
    component_id: str
    timestamp: float

class BoundaryValidator:
    """Validates data at HMA boundary interfaces"""
    
    def __init__(self, schema_registry: Dict[str, Dict], observability_port: Optional[ObservabilityPort] = None):
        self.schemas = schema_registry
        self.observability = observability_port
        self._load_schemas()
    
    def _load_schemas(self) -> None:
        """Load boundary validation schemas"""
        # Default schemas for HMA v2.1 boundaries
        self.default_schemas = {
            "l1_file_system_input": {
                "type": "object",
                "required": ["event_id", "type", "file_path", "timestamp"],
                "properties": {
                    "event_id": {"type": "string", "format": "uuid"},
                    "type": {"type": "string", "enum": ["created", "modified", "deleted", "moved"]},
                    "file_path": {"type": "string", "minLength": 1},
                    "old_path": {"type": "string"},
                    "timestamp": {"type": "number", "minimum": 0}
                }
            },
            "l2_plugin_execution_input": {
                "type": "object", 
                "required": ["plugin_id", "input_data", "request_id"],
                "properties": {
                    "plugin_id": {"type": "string", "minLength": 1},
                    "input_data": {"type": "object"},
                    "request_id": {"type": "string", "format": "uuid"},
                    "correlation_id": {"type": "string"}
                }
            },
            "event_schema": {
                "type": "object",
                "required": ["event_id", "event_type", "timestamp", "source", "data"],
                "properties": {
                    "event_id": {"type": "string", "format": "uuid"},
                    "event_type": {"type": "string", "minLength": 1},
                    "timestamp": {"type": "string", "format": "date-time"},
                    "source": {"type": "string", "minLength": 1},
                    "data": {"type": "object"},
                    "correlation_id": {"type": "string"},
                    "target": {"type": "string"}
                }
            }
        }
        
        # Merge with provided schemas
        self.schemas.update(self.default_schemas)
    
    def validate_l1_input(self, data: Dict[str, Any], interface: str) -> ValidationResult:
        """Validate L1 adapter input against schema"""
        schema_key = f"l1_{interface}_input"
        return self._validate_data(data, schema_key, BoundaryType.L1_INTERFACE, interface)
    
    def validate_l2_plugin_call(self, data: Dict[str, Any], component_id: str) -> ValidationResult:
        """Validate L2 plugin execution call"""
        return self._validate_data(data, "l2_plugin_execution_input", BoundaryType.L2_CORE, component_id)
    
    def validate_event_schema(self, event: Dict[str, Any], source_component: str) -> ValidationResult:
        """Validate event against HMA event schema"""
        return self._validate_data(event, "event_schema", BoundaryType.L3_PLUGIN, source_component)
    
    def validate_custom_boundary(self, data: Dict[str, Any], 
                                schema_key: str, 
                                boundary_type: BoundaryType,
                                component_id: str) -> ValidationResult:
        """Validate against custom boundary schema"""
        return self._validate_data(data, schema_key, boundary_type, component_id)
    
    def _validate_data(self, data: Dict[str, Any], 
                      schema_key: str, 
                      boundary_type: BoundaryType,
                      component_id: str) -> ValidationResult:
        """Internal validation logic"""
        timestamp = time.time()
        
        # Get schema
        schema = self.schemas.get(schema_key)
        if not schema:
            error_msg = f"No schema found for {schema_key}"
            logger.error("Schema not found", schema_key=schema_key, component=component_id)
            
            if self.observability:
                self.observability.emit_metric(
                    "hma_boundary_validation_errors_total", 
                    1.0,
                    {"boundary_type": boundary_type.value, "error_type": "schema_missing"}
                )
            
            return ValidationResult(False, [error_msg], boundary_type, component_id, timestamp)
        
        # Perform validation
        try:
            jsonschema.validate(data, schema)
            
            logger.debug("Boundary validation passed", 
                        boundary_type=boundary_type.value,
                        component=component_id,
                        schema_key=schema_key)
            
            if self.observability:
                self.observability.emit_metric(
                    "hma_boundary_validations_total",
                    1.0, 
                    {"boundary_type": boundary_type.value, "status": "success"}
                )
                
                self.observability.record_boundary_crossing(
                    "validator", component_id, "validate", 
                    (time.time() - timestamp) * 1000
                )
            
            return ValidationResult(True, [], boundary_type, component_id, timestamp)
            
        except jsonschema.ValidationError as e:
            error_msg = f"Validation failed: {e.message}"
            logger.error("Boundary validation failed",
                        boundary_type=boundary_type.value,
                        component=component_id,
                        error=error_msg,
                        error_path=list(e.absolute_path) if e.absolute_path else None)
            
            if self.observability:
                self.observability.emit_metric(
                    "hma_boundary_validation_errors_total",
                    1.0,
                    {"boundary_type": boundary_type.value, "error_type": "schema_violation"}
                )
            
            return ValidationResult(False, [error_msg], boundary_type, component_id, timestamp)
            
        except jsonschema.SchemaError as e:
            error_msg = f"Schema error: {e.message}"
            logger.error("Schema validation error",
                        boundary_type=boundary_type.value,
                        component=component_id,
                        schema_error=error_msg)
            
            return ValidationResult(False, [error_msg], boundary_type, component_id, timestamp)

class EventValidator:
    """Specialized validator for HMA events"""
    
    def __init__(self, boundary_validator: BoundaryValidator):
        self.boundary_validator = boundary_validator
    
    def create_hma_event(self, event_type: str, data: Dict[str, Any], 
                        source: str, target: Optional[str] = None) -> Dict[str, Any]:
        """Create HMA-compliant event"""
        event = {
            "event_id": str(uuid.uuid4()),
            "event_type": event_type,
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            "source": source,
            "data": data
        }
        
        if target:
            event["target"] = target
            
        return event
    
    def validate_and_publish_event(self, event: Dict[str, Any], source_component: str) -> bool:
        """Validate event and return whether it's safe to publish"""
        result = self.boundary_validator.validate_event_schema(event, source_component)
        return result.valid

def create_boundary_validator(schema_dir: Optional[Path] = None, 
                            observability_port: Optional[ObservabilityPort] = None) -> BoundaryValidator:
    """Factory function to create configured boundary validator"""
    
    # Load schemas from directory if provided
    schemas = {}
    if schema_dir and schema_dir.exists():
        for schema_file in schema_dir.glob("*.json"):
            try:
                with open(schema_file) as f:
                    schema_data = json.load(f)
                    schemas[schema_file.stem] = schema_data
            except Exception as e:
                logger.warning(f"Failed to load schema {schema_file}: {e}")
    
    return BoundaryValidator(schemas, observability_port)
```
  ```
  --- END OF FILE scribe/core\boundary_validator.py ---
  --- START OF FILE scribe/core\cache_manager.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Engine Cache Manager

Implements intelligent caching and memoization for performance optimization
with TTL, LRU eviction, memory management, and cache warming strategies.
"""

import time
import threading
import hashlib
import pickle
import weakref
from typing import Any, Dict, Optional, Callable, Union, TypeVar, Generic, List
from dataclasses import dataclass, field
from collections import OrderedDict
from functools import wraps, partial
import structlog

from .logging_config import get_scribe_logger
from .telemetry import get_telemetry_manager

logger = get_scribe_logger(__name__)

T = TypeVar('T')


@dataclass
class CacheEntry(Generic[T]):
    """Represents a cached value with metadata."""
    value: T
    created_at: float
    accessed_at: float
    access_count: int = 0
    ttl: Optional[float] = None
    size_bytes: int = 0
    
    @property
    def is_expired(self) -> bool:
        """Check if entry has expired based on TTL."""
        if self.ttl is None:
            return False
        return time.time() - self.created_at > self.ttl
    
    @property
    def age(self) -> float:
        """Get age of entry in seconds."""
        return time.time() - self.created_at
    
    def touch(self):
        """Update access time and count."""
        self.accessed_at = time.time()
        self.access_count += 1


class LRUCache:
    """
    LRU cache with TTL, size limits, and intelligent eviction.
    
    Features:
    - TTL-based expiration
    - Size-based eviction (LRU)
    - Memory usage tracking
    - Thread-safe operations
    - Cache statistics
    """
    
    def __init__(self,
                 max_size: int = 1000,
                 max_memory_mb: float = 100.0,
                 default_ttl: Optional[float] = 3600.0,
                 cleanup_interval: float = 300.0):
        """
        Initialize LRU cache.
        
        Args:
            max_size: Maximum number of entries
            max_memory_mb: Maximum memory usage in MB
            default_ttl: Default TTL in seconds (None = no expiration)
            cleanup_interval: Cleanup interval in seconds
        """
        self.max_size = max_size
        self.max_memory_bytes = int(max_memory_mb * 1024 * 1024)
        self.default_ttl = default_ttl
        self.cleanup_interval = cleanup_interval
        
        # Cache storage (key -> CacheEntry)
        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self._lock = threading.RLock()
        
        # Statistics
        self._stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "expirations": 0,
            "memory_bytes": 0,
            "cleanup_runs": 0
        }
        
        # Cleanup thread
        self._cleanup_thread: Optional[threading.Thread] = None
        self._running = False
        
        self._start_cleanup_thread()
        
        logger.debug("LRU cache initialized",
                    max_size=max_size,
                    max_memory_mb=max_memory_mb,
                    default_ttl=default_ttl)
    
    def _start_cleanup_thread(self):
        """Start background cleanup thread."""
        self._running = True
        self._cleanup_thread = threading.Thread(
            target=self._cleanup_worker,
            name="CacheCleanup",
            daemon=True
        )
        self._cleanup_thread.start()
    
    def _cleanup_worker(self):
        """Background worker for cache cleanup."""
        try:
            while self._running:
                time.sleep(self.cleanup_interval)
                if self._running:
                    self._cleanup_expired()
                    
        except Exception as e:
            logger.error("Cache cleanup worker error", error=str(e))
    
    def _cleanup_expired(self):
        """Remove expired entries."""
        with self._lock:
            expired_keys = []
            for key, entry in self._cache.items():
                if entry.is_expired:
                    expired_keys.append(key)
            
            for key in expired_keys:
                entry = self._cache.pop(key, None)
                if entry:
                    self._stats["expirations"] += 1
                    self._stats["memory_bytes"] -= entry.size_bytes
            
            if expired_keys:
                self._stats["cleanup_runs"] += 1
                logger.debug("Cache cleanup completed",
                           expired_entries=len(expired_keys),
                           remaining_entries=len(self._cache))
    
    def _estimate_size(self, value: Any) -> int:
        """Estimate memory size of value in bytes."""
        try:
            # Use pickle to estimate serialized size
            return len(pickle.dumps(value, protocol=pickle.HIGHEST_PROTOCOL))
        except Exception:
            # Fallback estimation
            if isinstance(value, str):
                return len(value.encode('utf-8'))
            elif isinstance(value, (int, float)):
                return 8
            elif isinstance(value, (list, tuple)):
                return sum(self._estimate_size(item) for item in value)
            elif isinstance(value, dict):
                return sum(self._estimate_size(k) + self._estimate_size(v) 
                          for k, v in value.items())
            else:
                return 64  # Default estimate
    
    def _evict_lru(self):
        """Evict least recently used entries to make space."""
        with self._lock:
            while (len(self._cache) >= self.max_size or 
                   self._stats["memory_bytes"] >= self.max_memory_bytes):
                
                if not self._cache:
                    break
                
                # Remove least recently used (first in OrderedDict)
                key, entry = self._cache.popitem(last=False)
                self._stats["evictions"] += 1
                self._stats["memory_bytes"] -= entry.size_bytes
                
                logger.debug("Cache entry evicted",
                           key=key,
                           age=entry.age,
                           access_count=entry.access_count)
    
    def get(self, key: str) -> Optional[Any]:
        """
        Get value from cache.
        
        Args:
            key: Cache key
            
        Returns:
            Cached value or None if not found/expired
        """
        with self._lock:
            entry = self._cache.get(key)
            
            if entry is None:
                self._stats["misses"] += 1
                return None
            
            if entry.is_expired:
                # Remove expired entry
                del self._cache[key]
                self._stats["expirations"] += 1
                self._stats["memory_bytes"] -= entry.size_bytes
                self._stats["misses"] += 1
                return None
            
            # Move to end (most recently used)
            entry.touch()
            self._cache.move_to_end(key)
            self._stats["hits"] += 1
            
            return entry.value
    
    def put(self, key: str, value: Any, ttl: Optional[float] = None) -> bool:
        """
        Put value in cache.
        
        Args:
            key: Cache key
            value: Value to cache
            ttl: Time to live in seconds (None uses default)
            
        Returns:
            True if cached successfully
        """
        if ttl is None:
            ttl = self.default_ttl
        
        # Estimate size
        size_bytes = self._estimate_size(value)
        
        # Check if value is too large
        if size_bytes > self.max_memory_bytes:
            logger.warning("Value too large for cache",
                          key=key,
                          size_mb=size_bytes / 1024 / 1024,
                          max_mb=self.max_memory_bytes / 1024 / 1024)
            return False
        
        with self._lock:
            # Remove existing entry if present
            if key in self._cache:
                old_entry = self._cache[key]
                self._stats["memory_bytes"] -= old_entry.size_bytes
            
            # Create new entry
            entry = CacheEntry(
                value=value,
                created_at=time.time(),
                accessed_at=time.time(),
                ttl=ttl,
                size_bytes=size_bytes
            )
            
            # Add to cache
            self._cache[key] = entry
            self._stats["memory_bytes"] += size_bytes
            
            # Evict if necessary
            self._evict_lru()
            
            logger.debug("Cache entry stored",
                        key=key,
                        size_bytes=size_bytes,
                        ttl=ttl,
                        cache_size=len(self._cache))
            
            return True
    
    def delete(self, key: str) -> bool:
        """
        Delete entry from cache.
        
        Args:
            key: Cache key
            
        Returns:
            True if entry was deleted
        """
        with self._lock:
            entry = self._cache.pop(key, None)
            if entry:
                self._stats["memory_bytes"] -= entry.size_bytes
                logger.debug("Cache entry deleted", key=key)
                return True
            return False
    
    def clear(self):
        """Clear all cache entries."""
        with self._lock:
            self._cache.clear()
            self._stats["memory_bytes"] = 0
            logger.info("Cache cleared")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        with self._lock:
            total_requests = self._stats["hits"] + self._stats["misses"]
            hit_rate = self._stats["hits"] / total_requests if total_requests > 0 else 0.0
            
            return {
                "size": len(self._cache),
                "max_size": self.max_size,
                "memory_bytes": self._stats["memory_bytes"],
                "memory_mb": self._stats["memory_bytes"] / 1024 / 1024,
                "max_memory_mb": self.max_memory_bytes / 1024 / 1024,
                "hit_rate": hit_rate,
                **self._stats
            }
    
    def shutdown(self):
        """Shutdown cache and cleanup thread."""
        self._running = False
        if self._cleanup_thread and self._cleanup_thread.is_alive():
            self._cleanup_thread.join(timeout=5.0)
        logger.debug("Cache shutdown completed")


class CacheManager:
    """
    Central cache manager with multiple cache instances and strategies.
    """
    
    def __init__(self):
        """Initialize cache manager."""
        self._caches: Dict[str, LRUCache] = {}
        self._lock = threading.RLock()
        
        # Default caches
        self._create_default_caches()
        
        logger.info("CacheManager initialized")
    
    def _create_default_caches(self):
        """Create default cache instances."""
        # File content cache
        self.create_cache(
            name="file_content",
            max_size=500,
            max_memory_mb=50.0,
            default_ttl=1800.0  # 30 minutes
        )
        
        # File metadata cache
        self.create_cache(
            name="file_metadata",
            max_size=2000,
            max_memory_mb=10.0,
            default_ttl=600.0  # 10 minutes
        )
        
        # Action results cache
        self.create_cache(
            name="action_results",
            max_size=1000,
            max_memory_mb=25.0,
            default_ttl=3600.0  # 1 hour
        )
        
        # Configuration cache
        self.create_cache(
            name="config",
            max_size=100,
            max_memory_mb=5.0,
            default_ttl=None  # No expiration
        )
    
    def create_cache(self,
                    name: str,
                    max_size: int = 1000,
                    max_memory_mb: float = 100.0,
                    default_ttl: Optional[float] = 3600.0) -> LRUCache:
        """
        Create a named cache instance.
        
        Args:
            name: Cache name
            max_size: Maximum number of entries
            max_memory_mb: Maximum memory usage in MB
            default_ttl: Default TTL in seconds
            
        Returns:
            LRU cache instance
        """
        with self._lock:
            if name in self._caches:
                raise ValueError(f"Cache '{name}' already exists")
            
            cache = LRUCache(
                max_size=max_size,
                max_memory_mb=max_memory_mb,
                default_ttl=default_ttl
            )
            
            self._caches[name] = cache
            logger.debug("Cache created", name=name)
            return cache
    
    def get_cache(self, name: str) -> Optional[LRUCache]:
        """Get cache instance by name."""
        with self._lock:
            return self._caches.get(name)
    
    def delete_cache(self, name: str) -> bool:
        """Delete a cache instance."""
        with self._lock:
            cache = self._caches.pop(name, None)
            if cache:
                cache.shutdown()
                logger.debug("Cache deleted", name=name)
                return True
            return False
    
    def get_stats(self) -> Dict[str, Dict[str, Any]]:
        """Get statistics for all caches."""
        with self._lock:
            return {name: cache.get_stats() 
                   for name, cache in self._caches.items()}
    
    def clear_all(self):
        """Clear all caches."""
        with self._lock:
            for cache in self._caches.values():
                cache.clear()
            logger.info("All caches cleared")
    
    def shutdown(self):
        """Shutdown all caches."""
        with self._lock:
            for cache in self._caches.values():
                cache.shutdown()
            logger.info("Cache manager shutdown completed")


def memoize(cache_name: str = "default",
           ttl: Optional[float] = None,
           key_func: Optional[Callable] = None):
    """
    Decorator for memoizing function results.
    
    Args:
        cache_name: Name of cache to use
        ttl: Time to live for cached results
        key_func: Function to generate cache key from args/kwargs
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Get cache instance
            cache_manager = get_cache_manager()
            cache = cache_manager.get_cache(cache_name)
            
            if cache is None:
                # No cache available, execute function directly
                return func(*args, **kwargs)
            
            # Generate cache key
            if key_func:
                cache_key = key_func(*args, **kwargs)
            else:
                # Default key generation
                key_parts = [func.__name__]
                key_parts.extend(str(arg) for arg in args)
                key_parts.extend(f"{k}={v}" for k, v in sorted(kwargs.items()))
                cache_key = hashlib.md5("|".join(key_parts).encode()).hexdigest()
            
            # Try to get from cache
            result = cache.get(cache_key)
            if result is not None:
                logger.debug("Cache hit for memoized function",
                           function=func.__name__,
                           cache_key=cache_key)
                return result
            
            # Execute function and cache result
            logger.debug("Cache miss for memoized function",
                        function=func.__name__,
                        cache_key=cache_key)
            
            result = func(*args, **kwargs)
            cache.put(cache_key, result, ttl=ttl)
            
            return result
        
        # Add cache control methods to wrapper
        wrapper._cache_name = cache_name
        wrapper._original_func = func
        
        def invalidate_cache():
            """Clear all cached results for this function."""
            cache_manager = get_cache_manager()
            cache = cache_manager.get_cache(cache_name)
            if cache:
                # For simplicity, clear entire cache
                # In production, you might want more granular invalidation
                cache.clear()
        
        wrapper.invalidate_cache = invalidate_cache
        
        return wrapper
    
    return decorator


def cache_warm_up(cache_name: str, items: List[tuple]):
    """
    Warm up cache with pre-computed values.
    
    Args:
        cache_name: Name of cache to warm up
        items: List of (key, value, ttl) tuples
    """
    cache_manager = get_cache_manager()
    cache = cache_manager.get_cache(cache_name)
    
    if cache is None:
        logger.warning("Cache not found for warm-up", cache_name=cache_name)
        return
    
    warmed_count = 0
    for key, value, ttl in items:
        if cache.put(key, value, ttl=ttl):
            warmed_count += 1
    
    logger.info("Cache warm-up completed",
               cache_name=cache_name,
               items_warmed=warmed_count,
               total_items=len(items))


# Global cache manager instance
_cache_manager: Optional[CacheManager] = None
_cache_lock = threading.RLock()


def get_cache_manager() -> CacheManager:
    """Get or create global cache manager."""
    global _cache_manager
    
    with _cache_lock:
        if _cache_manager is None:
            _cache_manager = CacheManager()
        
        return _cache_manager


def shutdown_cache_manager():
    """Shutdown the global cache manager."""
    global _cache_manager
    
    with _cache_lock:
        if _cache_manager:
            _cache_manager.shutdown()
            _cache_manager = None
```
  ```
  --- END OF FILE scribe/core\cache_manager.py ---
  --- START OF FILE scribe/core\circuit_breaker.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Circuit Breaker

Implements circuit breaker pattern for rule execution to prevent cascading failures.
Tracks failures, manages state transitions, and provides recovery mechanisms.
"""

import time
import threading
from enum import Enum
from typing import Dict, Any, Optional, Callable
import structlog

from .logging_config import get_scribe_logger

logger = get_scribe_logger(__name__)


class CircuitState(Enum):
    """Circuit breaker states."""
    CLOSED = "closed"      # Normal operation
    OPEN = "open"          # Failing, blocking requests
    HALF_OPEN = "half_open"  # Testing recovery


class CircuitBreakerError(Exception):
    """Exception raised when circuit breaker is open."""
    
    def __init__(self, rule_id: str, failure_count: int, last_failure_time: float):
        """
        Initialize the exception.
        
        Args:
            rule_id: ID of the rule with open circuit
            failure_count: Number of failures that triggered the circuit
            last_failure_time: Timestamp of the last failure
        """
        self.rule_id = rule_id
        self.failure_count = failure_count
        self.last_failure_time = last_failure_time
        
        super().__init__(f"Circuit breaker is OPEN for rule '{rule_id}' (failures: {failure_count})")


class CircuitBreaker:
    """
    Circuit breaker implementation for rule execution.
    
    Tracks failures for individual rules and prevents execution when
    failure threshold is exceeded. Provides automatic recovery after timeout.
    """
    
    def __init__(self, 
                 rule_id: str,
                 failure_threshold: int = 5,
                 recovery_timeout_seconds: int = 60,
                 success_threshold: int = 3):
        """
        Initialize circuit breaker for a specific rule.
        
        Args:
            rule_id: ID of the rule this circuit breaker protects
            failure_threshold: Number of failures before opening circuit
            recovery_timeout_seconds: Seconds to wait before attempting recovery
            success_threshold: Number of successes needed to close circuit from half-open
        """
        self.rule_id = rule_id
        self.failure_threshold = failure_threshold
        self.recovery_timeout_seconds = recovery_timeout_seconds
        self.success_threshold = success_threshold
        
        # State management
        self._state = CircuitState.CLOSED
        self._failure_count = 0
        self._success_count = 0
        self._last_failure_time = 0.0
        self._last_success_time = 0.0
        self._state_change_time = time.time()
        
        # Thread safety
        self._lock = threading.RLock()
        
        # Statistics
        self._total_calls = 0
        self._total_failures = 0
        self._total_successes = 0
        self._state_changes = 0
        
        logger.info("CircuitBreaker initialized",
                   rule_id=rule_id,
                   failure_threshold=failure_threshold,
                   recovery_timeout_seconds=recovery_timeout_seconds,
                   success_threshold=success_threshold)
    
    @property
    def state(self) -> CircuitState:
        """Get current circuit state."""
        with self._lock:
            return self._state
    
    @property
    def failure_count(self) -> int:
        """Get current failure count."""
        with self._lock:
            return self._failure_count
    
    @property
    def is_open(self) -> bool:
        """Check if circuit is open (blocking requests)."""
        return self.state == CircuitState.OPEN
    
    @property
    def is_closed(self) -> bool:
        """Check if circuit is closed (normal operation)."""
        return self.state == CircuitState.CLOSED
    
    @property
    def is_half_open(self) -> bool:
        """Check if circuit is half-open (testing recovery)."""
        return self.state == CircuitState.HALF_OPEN
    
    def _change_state(self, new_state: CircuitState, reason: str = "") -> None:
        """
        Change circuit state with logging.
        
        Args:
            new_state: New state to transition to
            reason: Reason for state change
        """
        old_state = self._state
        self._state = new_state
        self._state_change_time = time.time()
        self._state_changes += 1
        
        logger.info("Circuit breaker state changed",
                   rule_id=self.rule_id,
                   old_state=old_state.value,
                   new_state=new_state.value,
                   reason=reason,
                   failure_count=self._failure_count,
                   success_count=self._success_count)
    
    def _should_attempt_reset(self) -> bool:
        """
        Check if enough time has passed to attempt recovery.
        
        Returns:
            True if recovery should be attempted
        """
        if self._state != CircuitState.OPEN:
            return False
        
        time_since_failure = time.time() - self._last_failure_time
        return time_since_failure >= self.recovery_timeout_seconds
    
    def can_execute(self) -> bool:
        """
        Check if execution is allowed based on circuit state.
        
        Returns:
            True if execution is allowed
        """
        with self._lock:
            if self._state == CircuitState.CLOSED:
                return True
            
            elif self._state == CircuitState.OPEN:
                if self._should_attempt_reset():
                    self._change_state(CircuitState.HALF_OPEN, "Recovery timeout elapsed")
                    return True
                return False
            
            elif self._state == CircuitState.HALF_OPEN:
                return True
            
            return False
    
    def record_success(self) -> None:
        """Record a successful execution."""
        with self._lock:
            self._total_calls += 1
            self._total_successes += 1
            self._last_success_time = time.time()
            
            if self._state == CircuitState.CLOSED:
                # Reset failure count on success in closed state
                if self._failure_count > 0:
                    logger.debug("Resetting failure count after success",
                               rule_id=self.rule_id,
                               previous_failures=self._failure_count)
                    self._failure_count = 0
            
            elif self._state == CircuitState.HALF_OPEN:
                self._success_count += 1
                logger.debug("Success in half-open state",
                           rule_id=self.rule_id,
                           success_count=self._success_count,
                           success_threshold=self.success_threshold)
                
                if self._success_count >= self.success_threshold:
                    self._failure_count = 0
                    self._success_count = 0
                    self._change_state(CircuitState.CLOSED, "Success threshold reached")
            
            logger.debug("Success recorded",
                        rule_id=self.rule_id,
                        state=self._state.value,
                        total_successes=self._total_successes)
    
    def record_failure(self, error: Optional[Exception] = None) -> None:
        """
        Record a failed execution.
        
        Args:
            error: The exception that caused the failure
        """
        with self._lock:
            self._total_calls += 1
            self._total_failures += 1
            self._failure_count += 1
            self._last_failure_time = time.time()
            
            error_type = type(error).__name__ if error else "Unknown"
            error_message = str(error) if error else "No error details"
            
            logger.warning("Failure recorded",
                          rule_id=self.rule_id,
                          state=self._state.value,
                          failure_count=self._failure_count,
                          failure_threshold=self.failure_threshold,
                          error_type=error_type,
                          error_message=error_message)
            
            if self._state == CircuitState.CLOSED:
                if self._failure_count >= self.failure_threshold:
                    self._change_state(CircuitState.OPEN, f"Failure threshold exceeded ({self._failure_count}/{self.failure_threshold})")
            
            elif self._state == CircuitState.HALF_OPEN:
                # Any failure in half-open state goes back to open
                self._success_count = 0
                self._change_state(CircuitState.OPEN, "Failure during recovery test")
    
    def execute(self, func: Callable[[], Any], fallback_callback: Optional[Callable[[Exception], Any]] = None) -> Any:
        """
        Execute a function with circuit breaker protection.
        
        Args:
            func: Function to execute
            
        Returns:
            Result of function execution
            
        Raises:
            CircuitBreakerError: If circuit is open
            Exception: Any exception raised by the function
        """
        if not self.can_execute():
            error = CircuitBreakerError(self.rule_id, self._failure_count, self._last_failure_time)
            if fallback_callback:
                return fallback_callback(error)
            raise error
        
        try:
            result = func()
            self.record_success()
            return result
        except Exception as e:
            self.record_failure(e)
            if fallback_callback:
                return fallback_callback(e)
            raise
    
    def force_open(self, reason: str = "Manually opened") -> None:
        """
        Force circuit to open state.
        
        Args:
            reason: Reason for forcing open
        """
        with self._lock:
            self._change_state(CircuitState.OPEN, reason)
    
    def force_close(self, reason: str = "Manually closed") -> None:
        """
        Force circuit to closed state and reset counters.
        
        Args:
            reason: Reason for forcing closed
        """
        with self._lock:
            self._failure_count = 0
            self._success_count = 0
            self._change_state(CircuitState.CLOSED, reason)
    
    def reset(self) -> None:
        """Reset circuit breaker to initial state."""
        with self._lock:
            self._failure_count = 0
            self._success_count = 0
            self._last_failure_time = 0.0
            self._last_success_time = 0.0
            self._change_state(CircuitState.CLOSED, "Circuit breaker reset")
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get circuit breaker statistics.
        
        Returns:
            Dictionary with statistics
        """
        with self._lock:
            current_time = time.time()
            
            return {
                'rule_id': self.rule_id,
                'state': self._state.value,
                'failure_count': self._failure_count,
                'success_count': self._success_count,
                'failure_threshold': self.failure_threshold,
                'success_threshold': self.success_threshold,
                'recovery_timeout_seconds': self.recovery_timeout_seconds,
                'total_calls': self._total_calls,
                'total_failures': self._total_failures,
                'total_successes': self._total_successes,
                'state_changes': self._state_changes,
                'last_failure_time': self._last_failure_time,
                'last_success_time': self._last_success_time,
                'state_change_time': self._state_change_time,
                'time_since_last_failure': current_time - self._last_failure_time if self._last_failure_time > 0 else 0,
                'time_since_state_change': current_time - self._state_change_time,
                'can_execute': self.can_execute()
            }
    
    def __str__(self) -> str:
        """String representation of circuit breaker."""
        return f"CircuitBreaker(rule_id='{self.rule_id}', state={self._state.value}, failures={self._failure_count})"
    
    def __repr__(self) -> str:
        """Detailed string representation of circuit breaker."""
        return f"CircuitBreaker(rule_id='{self.rule_id}', state={self._state.value}, failure_count={self._failure_count}, failure_threshold={self.failure_threshold})"


class CircuitBreakerManager:
    """
    Manages circuit breakers for multiple rules.
    
    Provides centralized management of circuit breakers with automatic
    creation and configuration based on rule settings.
    """
    
    def __init__(self):
        """Initialize circuit breaker manager."""
        self._breakers: Dict[str, CircuitBreaker] = {}
        self._lock = threading.RLock()
        
        logger.info("CircuitBreakerManager initialized")
    
    def get_breaker(self, 
                   rule_id: str,
                   failure_threshold: int = 5,
                   recovery_timeout_seconds: int = 60,
                   success_threshold: int = 3) -> CircuitBreaker:
        """
        Get or create a circuit breaker for a rule.
        
        Args:
            rule_id: ID of the rule
            failure_threshold: Number of failures before opening circuit
            recovery_timeout_seconds: Seconds to wait before attempting recovery
            success_threshold: Number of successes needed to close circuit
            
        Returns:
            CircuitBreaker instance for the rule
        """
        with self._lock:
            if rule_id not in self._breakers:
                self._breakers[rule_id] = CircuitBreaker(
                    rule_id=rule_id,
                    failure_threshold=failure_threshold,
                    recovery_timeout_seconds=recovery_timeout_seconds,
                    success_threshold=success_threshold
                )
                logger.debug("Created new circuit breaker",
                           rule_id=rule_id,
                           failure_threshold=failure_threshold,
                           recovery_timeout_seconds=recovery_timeout_seconds)
            
            return self._breakers[rule_id]
    
    def remove_breaker(self, rule_id: str) -> bool:
        """
        Remove a circuit breaker for a rule.
        
        Args:
            rule_id: ID of the rule
            
        Returns:
            True if breaker was removed, False if not found
        """
        with self._lock:
            if rule_id in self._breakers:
                del self._breakers[rule_id]
                logger.debug("Removed circuit breaker", rule_id=rule_id)
                return True
            return False
    
    def get_all_breakers(self) -> Dict[str, CircuitBreaker]:
        """
        Get all circuit breakers.
        
        Returns:
            Dictionary mapping rule IDs to circuit breakers
        """
        with self._lock:
            return self._breakers.copy()
    
    def get_open_breakers(self) -> Dict[str, CircuitBreaker]:
        """
        Get all open circuit breakers.
        
        Returns:
            Dictionary mapping rule IDs to open circuit breakers
        """
        with self._lock:
            return {rule_id: breaker for rule_id, breaker in self._breakers.items() 
                   if breaker.is_open}
    
    def reset_all_breakers(self) -> None:
        """Reset all circuit breakers to closed state."""
        with self._lock:
            for breaker in self._breakers.values():
                breaker.reset()
            logger.info("All circuit breakers reset")
    
    def get_manager_stats(self) -> Dict[str, Any]:
        """
        Get manager statistics.
        
        Returns:
            Dictionary with manager statistics
        """
        with self._lock:
            breaker_stats = {}
            open_count = 0
            half_open_count = 0
            closed_count = 0
            
            for rule_id, breaker in self._breakers.items():
                stats = breaker.get_stats()
                breaker_stats[rule_id] = stats
                
                if breaker.is_open:
                    open_count += 1
                elif breaker.is_half_open:
                    half_open_count += 1
                else:
                    closed_count += 1
            
            return {
                'total_breakers': len(self._breakers),
                'open_breakers': open_count,
                'half_open_breakers': half_open_count,
                'closed_breakers': closed_count,
                'breaker_stats': breaker_stats,
                'breakers_by_state': {
                    'open': open_count,
                    'half_open': half_open_count,
                    'closed': closed_count
                }
            } 
```
  ```
  --- END OF FILE scribe/core\circuit_breaker.py ---
  --- START OF FILE scribe/core\config_manager.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Config Manager

Handles loading, validation, and hot-reloading of configuration files.
Implements atomic configuration swapping with JSON Schema validation.
"""

import json
import threading
import time
from pathlib import Path
from typing import Dict, Any, Optional, Callable, List
import structlog
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import jsonschema

from .logging_config import get_scribe_logger

logger = get_scribe_logger(__name__)


class ConfigChangeHandler(FileSystemEventHandler):
    """File system event handler for configuration file changes."""
    
    def __init__(self, config_manager: 'ConfigManager'):
        self.config_manager = config_manager
        super().__init__()
    
    def on_modified(self, event):
        """Handle file modification events."""
        if event.is_directory:
            return
        
        # Check if the modified file is our config file
        if Path(event.src_path).name == self.config_manager.config_filename:
            logger.info("Configuration file change detected", 
                       file_path=event.src_path)
            self.config_manager._reload_config()


class ConfigManager:
    """
    Configuration manager with hot-reloading and validation.
    
    Provides thread-safe access to configuration with automatic reloading
    when the configuration file changes.
    """
    
    DEFAULT_CONFIG_PATH = "scribe-config.json"
    
    def __init__(self, 
                 config_path: str = "tools/scribe/config/config.json",
                 schema_path: str = "tools/scribe/schemas/scribe_config.schema.json",
                 auto_reload: bool = True):
        """
        Initialize the configuration manager.
        
        Args:
            config_path: Path to the configuration file
            schema_path: Path to the JSON schema file
            auto_reload: Whether to enable automatic reloading on file changes
        """
        self.config_path = Path(config_path)
        self.schema_path = Path(schema_path)
        self.auto_reload = auto_reload
        self.config_filename = self.config_path.name
        self.repo_root = self._find_repo_root()

        # Thread safety
        self._config_lock = threading.RLock()
        self._config: Optional[Dict[str, Any]] = None
        self._schema: Optional[Dict[str, Any]] = None
        
        # Hot-reloading components
        self._observer: Optional[Observer] = None
        self._change_handler: Optional[ConfigChangeHandler] = None
        
        # Change callbacks
        self._change_callbacks: list[Callable[[Dict[str, Any]], None]] = []
        
        # Load initial configuration
        self._load_schema()
        self._load_and_validate_config()
        
        # Start hot-reloading if enabled
        if self.auto_reload:
            self._start_hot_reload()
        
        logger.info("ConfigManager initialized",
                   config_path=str(self.config_path),
                   schema_path=str(self.schema_path),
                   auto_reload=auto_reload)
    
    def _find_repo_root(self) -> str:
        """Find the repository root from the current path."""
        # A simple way is to look for a known marker, like .git directory
        current_path = Path.cwd()
        for parent in [current_path] + list(current_path.parents):
            if (parent / ".git").exists():
                return str(parent)
        return str(current_path) # Fallback to current dir

    def get_repo_root(self) -> str:
        """Get the repository root path."""
        return self.repo_root

    def _load_schema(self) -> None:
        """Load and parse the JSON schema."""
        try:
            if not self.schema_path.exists():
                raise FileNotFoundError(f"Schema file not found: {self.schema_path}")
            
            with open(self.schema_path, 'r', encoding='utf-8') as f:
                self._schema = json.load(f)
            
            logger.debug("JSON schema loaded successfully",
                        schema_path=str(self.schema_path))
            
        except Exception as e:
            logger.error("Failed to load JSON schema",
                        schema_path=str(self.schema_path),
                        error=str(e),
                        exc_info=True)
            raise
    
    def _load_and_validate_config(self) -> None:
        """Load configuration file and validate against schema."""
        try:
            # Load configuration file
            if not self.config_path.exists():
                raise FileNotFoundError(f"Configuration file not found: {self.config_path}")
            
            with open(self.config_path, 'r', encoding='utf-8') as f:
                temp_config = json.load(f)
            
            # Validate against schema
            if self._schema:
                try:
                    jsonschema.validate(temp_config, self._schema)
                    logger.debug("Configuration validation successful")
                except jsonschema.ValidationError as e:
                    logger.error("Configuration validation failed",
                                validation_error=str(e),
                                error_path=list(e.absolute_path) if e.absolute_path else None)
                    raise
                except jsonschema.SchemaError as e:
                    logger.error("Schema validation error",
                                schema_error=str(e))
                    raise
            
            # Atomic swap of configuration
            with self._config_lock:
                old_config = self._config
                self._config = temp_config
            
            logger.info("Configuration loaded and validated successfully",
                       config_version=temp_config.get('config_version', 'unknown'),
                       rules_count=len(temp_config.get('rules', [])))
            
            # Notify change callbacks
            self._notify_change_callbacks(temp_config)
            
        except Exception as e:
            logger.error("Failed to load configuration",
                        config_path=str(self.config_path),
                        error=str(e),
                        exc_info=True)
            
            # If this is initial load, re-raise the exception
            if self._config is None:
                raise
            
            # If this is a reload, keep the old configuration
            logger.warning("Keeping previous configuration due to load failure")
    
    def _reload_config(self) -> None:
        """Reload configuration file (called by file watcher)."""
        logger.info("Reloading configuration file")
        
        # Add a small delay to handle rapid file changes
        time.sleep(0.1)
        
        try:
            self._load_and_validate_config()
            logger.info("Configuration reloaded successfully")
        except Exception as e:
            logger.error("Configuration reload failed", error=str(e))
    
    def _start_hot_reload(self) -> None:
        """Start the file system watcher for hot-reloading."""
        try:
            self._change_handler = ConfigChangeHandler(self)
            self._observer = Observer()
            
            # Watch the directory containing the config file
            watch_dir = self.config_path.parent
            self._observer.schedule(self._change_handler, str(watch_dir), recursive=False)
            self._observer.start()
            
            logger.info("Hot-reload watcher started",
                       watch_directory=str(watch_dir))
            
        except Exception as e:
            logger.error("Failed to start hot-reload watcher",
                        error=str(e),
                        exc_info=True)
            # Don't raise - hot-reload is optional
    
    def stop(self) -> None:
        """Stop the configuration manager and cleanup resources."""
        if self._observer:
            try:
                self._observer.stop()
                self._observer.join(timeout=5.0)
                logger.info("Hot-reload watcher stopped")
            except Exception as e:
                logger.error("Error stopping hot-reload watcher", error=str(e))
        
        logger.info("ConfigManager stopped")
    
    def get_config(self) -> Dict[str, Any]:
        """
        Get the current configuration.
        
        Returns:
            A copy of the current configuration dictionary
        """
        with self._config_lock:
            if self._config is None:
                raise RuntimeError("Configuration not loaded")
            return self._config.copy()
    
    def get(self, key: str, default: Any = None) -> Any:
        """
        Get a configuration value by key.
        
        Args:
            key: Configuration key (supports dot notation)
            default: Default value if key not found
            
        Returns:
            Configuration value or default
        """
        config = self.get_config()
        
        # Support dot notation for nested keys
        keys = key.split('.')
        value = config
        
        for k in keys:
            if isinstance(value, dict) and k in value:
                value = value[k]
            else:
                return default
                
        return value
    
    def get_engine_settings(self) -> Dict[str, Any]:
        """Get engine settings from configuration."""
        config = self.get_config()
        return config.get('engine_settings', {})
    
    def get_security_settings(self) -> Dict[str, Any]:
        """Get security settings from configuration."""
        config = self.get_config()
        return config.get('security', {})
    
    def get_rules(self) -> list[Dict[str, Any]]:
        """Get list of rules from configuration."""
        config = self.get_config()
        return config.get('rules', [])
    
    def get_enabled_rules(self) -> list[Dict[str, Any]]:
        """Get list of enabled rules from configuration."""
        return [rule for rule in self.get_rules() if rule.get('enabled', False)]
    
    def get_plugin_settings(self) -> Dict[str, Any]:
        """Get plugin settings from configuration."""
        config = self.get_config()
        return config.get('plugins', {
            'directories': ['actions'],
            'auto_reload': False,
            'load_order': ['actions']
        })
    
    def get_plugin_directories(self) -> List[str]:
        """Get list of plugin directories from configuration."""
        plugin_settings = self.get_plugin_settings()
        return plugin_settings.get('directories', ['actions'])
    
    def get_plugin_auto_reload(self) -> bool:
        """Get plugin auto-reload setting from configuration."""
        plugin_settings = self.get_plugin_settings()
        return plugin_settings.get('auto_reload', False)
    
    def get_plugin_load_order(self) -> List[str]:
        """Get plugin load order from configuration."""
        plugin_settings = self.get_plugin_settings()
        return plugin_settings.get('load_order', [])
    
    def get_rule_by_id(self, rule_id: str) -> Optional[Dict[str, Any]]:
        """
        Get a specific rule by its ID.
        
        Args:
            rule_id: The rule ID to search for
            
        Returns:
            The rule dictionary if found, None otherwise
        """
        for rule in self.get_rules():
            if rule.get('id') == rule_id:
                return rule
        return None
    
    def add_change_callback(self, callback: Callable[[Dict[str, Any]], None]) -> None:
        """
        Add a callback to be called when configuration changes.
        
        Args:
            callback: Function to call with the new configuration
        """
        self._change_callbacks.append(callback)
        logger.debug("Configuration change callback added")
    
    def remove_change_callback(self, callback: Callable[[Dict[str, Any]], None]) -> None:
        """
        Remove a configuration change callback.
        
        Args:
            callback: The callback function to remove
        """
        if callback in self._change_callbacks:
            self._change_callbacks.remove(callback)
            logger.debug("Configuration change callback removed")
    
    def _notify_change_callbacks(self, new_config: Dict[str, Any]) -> None:
        """Notify all registered callbacks of configuration changes."""
        for callback in self._change_callbacks:
            try:
                callback(new_config)
            except Exception as e:
                logger.error("Error in configuration change callback",
                           callback=str(callback),
                           error=str(e),
                           exc_info=True)
    
    def validate_config_dict(self, config_dict: Dict[str, Any]) -> bool:
        """
        Validate a configuration dictionary against the schema.
        
        Args:
            config_dict: Configuration dictionary to validate
            
        Returns:
            True if valid, False otherwise
        """
        if not self._schema:
            logger.warning("No schema loaded, cannot validate configuration")
            return False
        
        try:
            jsonschema.validate(config_dict, self._schema)
            return True
        except (jsonschema.ValidationError, jsonschema.SchemaError) as e:
            logger.error("Configuration validation failed",
                        validation_error=str(e))
            return False
    
    def __enter__(self):
        """Context manager entry."""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        self.stop() 
```
  ```
  --- END OF FILE scribe/core\config_manager.py ---
  --- START OF FILE scribe/core\engine_factory.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
HMA v2.2 Compliant Engine Factory

This factory creates and configures all engine components to maintain
the minimalist core principle. The engine should only handle routing
and plugin lifecycle management, not component instantiation.
"""

from typing import Dict, Any, Optional
from pathlib import Path

from .config_manager import ConfigManager
from .security_manager import SecurityManager
from .plugin_loader import PluginLoader
from .async_processor import AsyncProcessor
from .hma_telemetry import create_hma_telemetry
from .port_adapters import (
    ScribePluginExecutionAdapter, ScribeConfigurationAdapter, ScribeHealthCheckAdapter,
    ScribeCommandExecutionAdapter, ScribeFileSystemAdapter, ScribeLoggingAdapter
)
from .adapters.nats_adapter import NatsEventBusAdapter
from .hma_ports import PortRegistry
from .logging_config import get_scribe_logger

logger = get_scribe_logger(__name__)


class EngineComponents:
    """Container for all engine components created by the factory."""
    
    def __init__(self):
        self.config_manager: Optional[ConfigManager] = None
        self.security_manager: Optional[SecurityManager] = None
        self.plugin_loader: Optional[PluginLoader] = None
        self.async_processor: Optional[AsyncProcessor] = None
        self.telemetry = None
        self.port_registry: Optional[PortRegistry] = None
        
        # Port adapters
        self.plugin_execution_adapter = None
        self.event_bus_adapter = None
        self.configuration_adapter = None
        self.health_check_adapter = None
        self.command_execution_adapter = None
        self.file_system_adapter = None
        self.logging_adapter = None


def create_engine_components(
    config_path: str = "tools/scribe/config/config.json",
    telemetry_config: Optional[Dict[str, Any]] = None,
    health_port: int = 9090
) -> EngineComponents:
    """
    Factory function to create all engine components following HMA v2.2 minimalist core principle.
    
    This function is responsible for instantiating ConfigManager, SecurityManager, 
    PluginLoader, AsyncProcessor, and all port adapters.
    
    Args:
        config_path: Path to configuration file
        telemetry_config: Telemetry configuration
        health_port: Port for health server
        
    Returns:
        EngineComponents: Container with all instantiated components
    """
    logger.info("Creating HMA v2.2 compliant engine components")
    
    components = EngineComponents()
    
    try:
        # Initialize configuration manager
        components.config_manager = ConfigManager(config_path=config_path)
        logger.debug("ConfigManager created")
        
        # Initialize telemetry
        telemetry_config = telemetry_config or {}
        components.telemetry = create_hma_telemetry(
            component_type="L2-Core",
            component_id="scribe-engine",
            layer="L2",
            **telemetry_config
        )
        logger.debug("HMA Telemetry created")
        
        # Initialize security manager
        components.security_manager = SecurityManager(
            config_manager=components.config_manager
        )
        logger.debug("SecurityManager created")
        
        # Initialize plugin loader
        plugin_directories = components.config_manager.get_plugin_directories()
        components.plugin_loader = PluginLoader(
            plugin_directories=plugin_directories
        )
        logger.debug("PluginLoader created")
        
        # Initialize async processor
        components.async_processor = AsyncProcessor()
        logger.debug("AsyncProcessor created")
        
        # Initialize port registry
        components.port_registry = PortRegistry()
        logger.debug("PortRegistry created")
        
        # Create and register port adapters
        _create_port_adapters(components)
        
        # Load all plugins
        components.plugin_loader.load_all_plugins()
        
        # Enable hot-reloading if configured
        if components.config_manager.get_plugin_auto_reload():
            components.plugin_loader.enable_hot_reload()
        
        logger.info("Engine components created successfully")
        return components
        
    except Exception as e:
        logger.error("Failed to create engine components", error=str(e), exc_info=True)
        raise


def _create_port_adapters(components: EngineComponents) -> None:
    """Create and register all port adapters."""
    
    # Plugin execution port adapter
    components.plugin_execution_adapter = ScribePluginExecutionAdapter(
        components.plugin_loader,
        components.security_manager, 
        components.telemetry,
        components.config_manager,
        components.port_registry
    )
    components.port_registry.register_port(
        "plugin_execution", 
        components.plugin_execution_adapter
    )
    logger.debug("Plugin execution adapter registered")
    
    # NATS-based event bus port adapter (HMA v2.2 Tier 2 recommended)
    nats_config = components.config_manager.get('nats', {})
    nats_url = nats_config.get('url', 'nats://localhost:4222')
    
    components.event_bus_adapter = NatsEventBusAdapter(
        components.telemetry,
        nats_url=nats_url,
        max_reconnect_attempts=nats_config.get('max_reconnect_attempts', -1),
        reconnect_time_wait=nats_config.get('reconnect_time_wait', 2)
    )
    
    # Start NATS adapter asynchronously (will be handled by the async loop)
    components.port_registry.register_port(
        "event_bus", 
        components.event_bus_adapter
    )
    logger.debug("NATS event bus adapter registered")
    
    # Configuration port adapter
    components.configuration_adapter = ScribeConfigurationAdapter(
        components.config_manager, 
        components.telemetry
    )
    components.port_registry.register_port(
        "configuration", 
        components.configuration_adapter
    )
    logger.debug("Configuration adapter registered")
    
    # Health check port adapter
    components.health_check_adapter = ScribeHealthCheckAdapter(components.telemetry)
    components.port_registry.register_port(
        "health_check", 
        components.health_check_adapter
    )
    logger.debug("Health check adapter registered")
    
    # Observability port (telemetry itself implements this)
    components.port_registry.register_port("observability", components.telemetry)
    logger.debug("Observability port registered")
    
    # Command execution port adapter
    components.command_execution_adapter = ScribeCommandExecutionAdapter(
        components.security_manager,
        components.telemetry
    )
    components.port_registry.register_port(
        "command_execution",
        components.command_execution_adapter
    )
    logger.debug("Command execution adapter registered")
    
    # File system port adapter
    components.file_system_adapter = ScribeFileSystemAdapter(
        components.security_manager,
        components.telemetry
    )
    components.port_registry.register_port(
        "file_system",
        components.file_system_adapter
    )
    logger.debug("File system adapter registered")
    
    # Logging port adapter
    components.logging_adapter = ScribeLoggingAdapter(components.telemetry)
    components.port_registry.register_port(
        "logging",
        components.logging_adapter
    )
    logger.debug("Logging adapter registered")
```
  ```
  --- END OF FILE scribe/core\engine_factory.py ---
  --- START OF FILE scribe/core\error_recovery.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Engine Error Recovery System

Implements comprehensive error handling, recovery strategies, and self-healing
mechanisms for robust operation in production environments.
"""

import time
import threading
import traceback
from enum import Enum
from typing import Dict, Any, Optional, List, Callable, Union
from dataclasses import dataclass, field
from collections import deque
import structlog

from .logging_config import get_scribe_logger
from .telemetry import get_telemetry_manager

logger = get_scribe_logger(__name__)


class ErrorSeverity(Enum):
    """Error severity levels for classification and handling."""
    LOW = "low"              # Minor issues, continue operation
    MEDIUM = "medium"        # Recoverable issues, retry with backoff
    HIGH = "high"           # Serious issues, invoke recovery procedures
    CRITICAL = "critical"   # System-threatening, emergency procedures


class RecoveryStrategy(Enum):
    """Recovery strategies for different error types."""
    IGNORE = "ignore"                    # Log and continue
    RETRY = "retry"                     # Retry with exponential backoff
    RESTART_COMPONENT = "restart_component"  # Restart failed component
    DEGRADE_GRACEFULLY = "degrade_gracefully"  # Reduce functionality
    FAILOVER = "failover"               # Switch to backup/alternative
    EMERGENCY_STOP = "emergency_stop"   # Shut down to prevent damage


@dataclass
class ErrorContext:
    """Context information for error events."""
    error_id: str
    timestamp: float
    component: str
    operation: str
    error_type: str
    error_message: str
    severity: ErrorSeverity
    recovery_strategy: RecoveryStrategy
    stack_trace: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    recovery_attempts: int = 0
    max_retries: int = 3
    resolved: bool = False


class ErrorPattern:
    """Pattern for matching and handling specific error types."""
    
    def __init__(self,
                 pattern_name: str,
                 error_types: List[str],
                 component_patterns: List[str],
                 severity: ErrorSeverity,
                 recovery_strategy: RecoveryStrategy,
                 max_retries: int = 3,
                 backoff_multiplier: float = 2.0):
        """
        Initialize error pattern.
        
        Args:
            pattern_name: Name of the pattern
            error_types: List of error type names to match
            component_patterns: List of component name patterns
            severity: Error severity level
            recovery_strategy: Recovery strategy to apply
            max_retries: Maximum retry attempts
            backoff_multiplier: Exponential backoff multiplier
        """
        self.pattern_name = pattern_name
        self.error_types = error_types
        self.component_patterns = component_patterns
        self.severity = severity
        self.recovery_strategy = recovery_strategy
        self.max_retries = max_retries
        self.backoff_multiplier = backoff_multiplier
    
    def matches(self, error_context: ErrorContext) -> bool:
        """Check if error context matches this pattern."""
        # Check error type
        error_type_match = any(
            error_type in error_context.error_type
            for error_type in self.error_types
        )
        
        # Check component pattern
        component_match = any(
            pattern in error_context.component
            for pattern in self.component_patterns
        )
        
        return error_type_match and component_match


class RecoveryAction:
    """Represents a recovery action that can be executed."""
    
    def __init__(self,
                 action_name: str,
                 action_func: Callable[[ErrorContext], bool],
                 timeout_seconds: float = 30.0,
                 prerequisites: Optional[List[str]] = None):
        """
        Initialize recovery action.
        
        Args:
            action_name: Name of the action
            action_func: Function to execute (returns success status)
            timeout_seconds: Timeout for action execution
            prerequisites: List of prerequisite actions
        """
        self.action_name = action_name
        self.action_func = action_func
        self.timeout_seconds = timeout_seconds
        self.prerequisites = prerequisites or []
    
    def execute(self, error_context: ErrorContext) -> bool:
        """
        Execute the recovery action.
        
        Args:
            error_context: Error context
            
        Returns:
            True if action succeeded
        """
        try:
            logger.info("Executing recovery action",
                       action_name=self.action_name,
                       error_id=error_context.error_id,
                       component=error_context.component)
            
            result = self.action_func(error_context)
            
            if result:
                logger.info("Recovery action succeeded",
                           action_name=self.action_name,
                           error_id=error_context.error_id)
            else:
                logger.warning("Recovery action failed",
                              action_name=self.action_name,
                              error_id=error_context.error_id)
            
            return result
            
        except Exception as e:
            logger.error("Recovery action raised exception",
                        action_name=self.action_name,
                        error_id=error_context.error_id,
                        exception=str(e),
                        exc_info=True)
            return False


class ErrorRecoveryManager:
    """
    Manages error detection, classification, and recovery for Scribe Engine.
    
    Provides centralized error handling with configurable recovery strategies,
    exponential backoff, circuit breaking, and self-healing capabilities.
    """
    
    def __init__(self):
        """Initialize error recovery manager."""
        self._error_patterns: List[ErrorPattern] = []
        self._recovery_actions: Dict[RecoveryStrategy, List[RecoveryAction]] = {}
        self._error_history: deque = deque(maxlen=1000)
        self._active_errors: Dict[str, ErrorContext] = {}
        self._component_health: Dict[str, bool] = {}
        
        self._lock = threading.RLock()
        self._recovery_thread: Optional[threading.Thread] = None
        self._running = False
        
        # Initialize default error patterns
        self._initialize_default_patterns()
        
        # Initialize default recovery actions
        self._initialize_default_actions()
        
        logger.info("ErrorRecoveryManager initialized")
    
    def _initialize_default_patterns(self):
        """Initialize default error patterns for common scenarios."""
        patterns = [
            ErrorPattern(
                pattern_name="file_system_errors",
                error_types=["FileNotFoundError", "PermissionError", "OSError"],
                component_patterns=["watcher", "atomic_write", "config"],
                severity=ErrorSeverity.MEDIUM,
                recovery_strategy=RecoveryStrategy.RETRY,
                max_retries=3
            ),
            ErrorPattern(
                pattern_name="network_timeouts",
                error_types=["TimeoutError", "ConnectionError", "RequestException"],
                component_patterns=["http", "api", "webhook"],
                severity=ErrorSeverity.MEDIUM,
                recovery_strategy=RecoveryStrategy.RETRY,
                max_retries=5,
                backoff_multiplier=1.5
            ),
            ErrorPattern(
                pattern_name="plugin_failures",
                error_types=["PluginLoadError", "ActionExecutionError", "ImportError"],
                component_patterns=["plugin", "action"],
                severity=ErrorSeverity.HIGH,
                recovery_strategy=RecoveryStrategy.RESTART_COMPONENT,
                max_retries=2
            ),
            ErrorPattern(
                pattern_name="memory_errors",
                error_types=["MemoryError", "OutOfMemoryError"],
                component_patterns=["worker", "processor"],
                severity=ErrorSeverity.CRITICAL,
                recovery_strategy=RecoveryStrategy.DEGRADE_GRACEFULLY,
                max_retries=1
            ),
            ErrorPattern(
                pattern_name="configuration_errors",
                error_types=["ValidationError", "ConfigurationError", "JSONDecodeError"],
                component_patterns=["config", "schema"],
                severity=ErrorSeverity.HIGH,
                recovery_strategy=RecoveryStrategy.FAILOVER,
                max_retries=1
            )
        ]
        
        for pattern in patterns:
            self.add_error_pattern(pattern)
    
    def _initialize_default_actions(self):
        """Initialize default recovery actions."""
        # Retry action
        self.add_recovery_action(
            RecoveryStrategy.RETRY,
            RecoveryAction(
                action_name="exponential_backoff_retry",
                action_func=self._retry_with_backoff,
                timeout_seconds=60.0
            )
        )
        
        # Restart component action
        self.add_recovery_action(
            RecoveryStrategy.RESTART_COMPONENT,
            RecoveryAction(
                action_name="restart_component",
                action_func=self._restart_component,
                timeout_seconds=30.0
            )
        )
        
        # Graceful degradation action
        self.add_recovery_action(
            RecoveryStrategy.DEGRADE_GRACEFULLY,
            RecoveryAction(
                action_name="degrade_gracefully",
                action_func=self._degrade_gracefully,
                timeout_seconds=15.0
            )
        )
        
        # Failover action
        self.add_recovery_action(
            RecoveryStrategy.FAILOVER,
            RecoveryAction(
                action_name="failover_to_backup",
                action_func=self._failover_to_backup,
                timeout_seconds=45.0
            )
        )
    
    def add_error_pattern(self, pattern: ErrorPattern):
        """Add an error pattern for matching and handling."""
        with self._lock:
            self._error_patterns.append(pattern)
            logger.debug("Added error pattern", pattern_name=pattern.pattern_name)
    
    def add_recovery_action(self, strategy: RecoveryStrategy, action: RecoveryAction):
        """Add a recovery action for a strategy."""
        with self._lock:
            if strategy not in self._recovery_actions:
                self._recovery_actions[strategy] = []
            self._recovery_actions[strategy].append(action)
            logger.debug("Added recovery action",
                        strategy=strategy.value,
                        action_name=action.action_name)
    
    def handle_error(self,
                    component: str,
                    operation: str,
                    error: Exception,
                    metadata: Optional[Dict[str, Any]] = None) -> str:
        """
        Handle an error with automatic classification and recovery.
        
        Args:
            component: Component where error occurred
            operation: Operation being performed
            error: The exception that occurred
            metadata: Additional context metadata
            
        Returns:
            Error ID for tracking
        """
        # Create error context
        error_id = f"{component}_{int(time.time() * 1000)}"
        error_context = ErrorContext(
            error_id=error_id,
            timestamp=time.time(),
            component=component,
            operation=operation,
            error_type=type(error).__name__,
            error_message=str(error),
            severity=ErrorSeverity.MEDIUM,  # Default, will be updated by pattern matching
            recovery_strategy=RecoveryStrategy.IGNORE,  # Default
            stack_trace=traceback.format_exc(),
            metadata=metadata or {}
        )
        
        # Find matching pattern and update context
        with self._lock:
            for pattern in self._error_patterns:
                if pattern.matches(error_context):
                    error_context.severity = pattern.severity
                    error_context.recovery_strategy = pattern.recovery_strategy
                    error_context.max_retries = pattern.max_retries
                    
                    logger.debug("Matched error pattern",
                               error_id=error_id,
                               pattern_name=pattern.pattern_name,
                               severity=pattern.severity.value,
                               strategy=pattern.recovery_strategy.value)
                    break
            
            # Store error context
            self._error_history.append(error_context)
            self._active_errors[error_id] = error_context
            
            # Update component health
            self._component_health[component] = False
        
        # Log error with appropriate level
        log_level = {
            ErrorSeverity.LOW: "debug",
            ErrorSeverity.MEDIUM: "warning", 
            ErrorSeverity.HIGH: "error",
            ErrorSeverity.CRITICAL: "critical"
        }.get(error_context.severity, "error")
        
        getattr(logger, log_level)(
            "Error detected and classified",
            error_id=error_id,
            component=component,
            operation=operation,
            error_type=error_context.error_type,
            severity=error_context.severity.value,
            recovery_strategy=error_context.recovery_strategy.value
        )
        
        # Record telemetry
        telemetry = get_telemetry_manager()
        if telemetry:
            telemetry.action_failures_counter.add(1, {
                "component": component,
                "error_type": error_context.error_type,
                "severity": error_context.severity.value
            })
        
        # Trigger recovery if strategy is not IGNORE
        if error_context.recovery_strategy != RecoveryStrategy.IGNORE:
            self._trigger_recovery(error_context)
        
        return error_id
    
    def _trigger_recovery(self, error_context: ErrorContext):
        """Trigger recovery process for an error."""
        try:
            recovery_actions = self._recovery_actions.get(error_context.recovery_strategy, [])
            
            if not recovery_actions:
                logger.warning("No recovery actions defined for strategy",
                             strategy=error_context.recovery_strategy.value,
                             error_id=error_context.error_id)
                return
            
            # Execute recovery actions
            for action in recovery_actions:
                try:
                    success = action.execute(error_context)
                    if success:
                        error_context.resolved = True
                        self._component_health[error_context.component] = True
                        
                        logger.info("Error recovery successful",
                                   error_id=error_context.error_id,
                                   action_name=action.action_name)
                        
                        # Record successful recovery
                        telemetry = get_telemetry_manager()
                        if telemetry:
                            telemetry.boundary_calls_counter.add(1, {
                                "interface_type": "internal",
                                "protocol": "recovery",
                                "operation": "success",
                                "strategy": error_context.recovery_strategy.value
                            })
                        break
                        
                except Exception as e:
                    logger.error("Recovery action failed",
                               error_id=error_context.error_id,
                               action_name=action.action_name,
                               error=str(e))
            
            if not error_context.resolved:
                logger.error("All recovery actions failed",
                           error_id=error_context.error_id,
                           strategy=error_context.recovery_strategy.value)
        
        except Exception as e:
            logger.error("Error during recovery process",
                        error_id=error_context.error_id,
                        error=str(e),
                        exc_info=True)
    
    def _retry_with_backoff(self, error_context: ErrorContext) -> bool:
        """Implement exponential backoff retry."""
        if error_context.recovery_attempts >= error_context.max_retries:
            logger.warning("Max retries exceeded",
                          error_id=error_context.error_id,
                          attempts=error_context.recovery_attempts)
            return False
        
        # Calculate backoff delay
        backoff_delay = (2 ** error_context.recovery_attempts) * 0.1
        
        logger.info("Retrying with backoff",
                   error_id=error_context.error_id,
                   attempt=error_context.recovery_attempts + 1,
                   delay_seconds=backoff_delay)
        
        time.sleep(backoff_delay)
        error_context.recovery_attempts += 1
        
        # Return True to indicate retry should be attempted
        # Actual retry logic would be handled by the calling component
        return True
    
    def _restart_component(self, error_context: ErrorContext) -> bool:
        """Restart a failed component."""
        logger.info("Attempting component restart",
                   error_id=error_context.error_id,
                   component=error_context.component)
        
        # Component restart logic would be implemented here
        # This is a placeholder for demonstration
        return True
    
    def _degrade_gracefully(self, error_context: ErrorContext) -> bool:
        """Implement graceful degradation."""
        logger.info("Implementing graceful degradation",
                   error_id=error_context.error_id,
                   component=error_context.component)
        
        # Graceful degradation logic would be implemented here
        # This could involve disabling non-essential features
        return True
    
    def _failover_to_backup(self, error_context: ErrorContext) -> bool:
        """Failover to backup system or configuration."""
        logger.info("Attempting failover to backup",
                   error_id=error_context.error_id,
                   component=error_context.component)
        
        # Failover logic would be implemented here
        # This could involve switching to backup config or services
        return True
    
    def get_error_stats(self) -> Dict[str, Any]:
        """Get error and recovery statistics."""
        with self._lock:
            error_count_by_severity = {}
            error_count_by_component = {}
            recovery_success_rate = {}
            
            for error_context in list(self._error_history):
                # Count by severity
                severity = error_context.severity.value
                error_count_by_severity[severity] = error_count_by_severity.get(severity, 0) + 1
                
                # Count by component
                component = error_context.component
                error_count_by_component[component] = error_count_by_component.get(component, 0) + 1
                
                # Track recovery success
                strategy = error_context.recovery_strategy.value
                if strategy not in recovery_success_rate:
                    recovery_success_rate[strategy] = {"total": 0, "successful": 0}
                
                recovery_success_rate[strategy]["total"] += 1
                if error_context.resolved:
                    recovery_success_rate[strategy]["successful"] += 1
            
            # Calculate success rates
            for strategy_stats in recovery_success_rate.values():
                if strategy_stats["total"] > 0:
                    strategy_stats["rate"] = strategy_stats["successful"] / strategy_stats["total"]
                else:
                    strategy_stats["rate"] = 0.0
            
            return {
                "total_errors": len(self._error_history),
                "active_errors": len(self._active_errors),
                "error_count_by_severity": error_count_by_severity,
                "error_count_by_component": error_count_by_component,
                "recovery_success_rate": recovery_success_rate,
                "component_health": self._component_health.copy(),
                "error_patterns_count": len(self._error_patterns)
            }
    
    def resolve_error(self, error_id: str):
        """Mark an error as resolved."""
        with self._lock:
            if error_id in self._active_errors:
                error_context = self._active_errors[error_id]
                error_context.resolved = True
                del self._active_errors[error_id]
                
                # Update component health
                self._component_health[error_context.component] = True
                
                logger.info("Error marked as resolved",
                           error_id=error_id,
                           component=error_context.component)
    
    def get_component_health(self, component: str) -> bool:
        """Get health status of a component."""
        with self._lock:
            return self._component_health.get(component, True)
    
    def clear_error_history(self):
        """Clear error history (for testing/maintenance)."""
        with self._lock:
            self._error_history.clear()
            self._active_errors.clear()
            logger.info("Error history cleared")


# Global error recovery manager instance
_error_recovery_manager: Optional[ErrorRecoveryManager] = None
_recovery_lock = threading.RLock()


def get_error_recovery_manager() -> ErrorRecoveryManager:
    """Get or create global error recovery manager."""
    global _error_recovery_manager
    
    with _recovery_lock:
        if _error_recovery_manager is None:
            _error_recovery_manager = ErrorRecoveryManager()
        
        return _error_recovery_manager


def handle_error(component: str,
                operation: str,
                error: Exception,
                metadata: Optional[Dict[str, Any]] = None) -> str:
    """
    Convenience function to handle errors using global recovery manager.
    
    Args:
        component: Component where error occurred
        operation: Operation being performed
        error: The exception that occurred
        metadata: Additional context metadata
        
    Returns:
        Error ID for tracking
    """
    manager = get_error_recovery_manager()
    return manager.handle_error(component, operation, error, metadata)
```
  ```
  --- END OF FILE scribe/core\error_recovery.py ---
  --- START OF FILE scribe/core\file_optimizer.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Engine File System Optimizer

Implements high-performance file system operations with batching, streaming,
memory mapping, and intelligent I/O scheduling for optimal performance.
"""

import os
import mmap
import threading
import time
from pathlib import Path
from typing import Dict, List, Optional, Union, Iterator, BinaryIO, TextIO, Any
from dataclasses import dataclass
from collections import deque
import structlog

from .logging_config import get_scribe_logger
from cache_manager import get_cache_manager, memoize
from .telemetry import get_telemetry_manager

logger = get_scribe_logger(__name__)


@dataclass
class FileOperation:
    """Represents a file operation for batching."""
    operation_type: str  # 'read', 'write', 'stat', 'delete'
    file_path: Path
    data: Optional[Union[str, bytes]] = None
    encoding: str = 'utf-8'
    callback: Optional[callable] = None
    created_at: float = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = time.time()


class FileStreamReader:
    """Memory-efficient streaming file reader."""
    
    def __init__(self, file_path: Union[str, Path], chunk_size: int = 8192):
        """
        Initialize streaming reader.
        
        Args:
            file_path: Path to file
            chunk_size: Size of chunks to read
        """
        self.file_path = Path(file_path)
        self.chunk_size = chunk_size
        self._file: Optional[BinaryIO] = None
        self._position = 0
        
    def __enter__(self):
        self._file = open(self.file_path, 'rb')
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self._file:
            self._file.close()
    
    def __iter__(self) -> Iterator[bytes]:
        """Iterate over file chunks."""
        if not self._file:
            raise RuntimeError("FileStreamReader not opened")
        
        while True:
            chunk = self._file.read(self.chunk_size)
            if not chunk:
                break
            self._position += len(chunk)
            yield chunk
    
    def read_lines(self, encoding: str = 'utf-8') -> Iterator[str]:
        """Iterate over file lines."""
        buffer = b''
        
        for chunk in self:
            buffer += chunk
            while b'\n' in buffer:
                line, buffer = buffer.split(b'\n', 1)
                yield line.decode(encoding, errors='replace') + '\n'
        
        # Yield remaining buffer if any
        if buffer:
            yield buffer.decode(encoding, errors='replace')
    
    def seek(self, position: int) -> int:
        """Seek to position in file."""
        if self._file:
            self._file.seek(position)
            self._position = position
            return position
        return 0
    
    def tell(self) -> int:
        """Get current position."""
        return self._position


class MemoryMappedFile:
    """Memory-mapped file for efficient large file access."""
    
    def __init__(self, file_path: Union[str, Path], mode: str = 'r'):
        """
        Initialize memory-mapped file.
        
        Args:
            file_path: Path to file
            mode: Access mode ('r', 'r+', 'w+')
        """
        self.file_path = Path(file_path)
        self.mode = mode
        self._file: Optional[BinaryIO] = None
        self._mmap: Optional[mmap.mmap] = None
        
    def __enter__(self):
        if self.mode == 'r':
            self._file = open(self.file_path, 'rb')
            self._mmap = mmap.mmap(self._file.fileno(), 0, access=mmap.ACCESS_READ)
        elif self.mode in ('r+', 'w+'):
            self._file = open(self.file_path, 'r+b')
            self._mmap = mmap.mmap(self._file.fileno(), 0)
        else:
            raise ValueError(f"Unsupported mode: {mode}")
        
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self._mmap:
            self._mmap.close()
        if self._file:
            self._file.close()
    
    def read(self, size: int = -1) -> bytes:
        """Read from memory-mapped file."""
        if not self._mmap:
            raise RuntimeError("MemoryMappedFile not opened")
        
        if size == -1:
            return self._mmap[:]
        else:
            current_pos = self._mmap.tell()
            return self._mmap[current_pos:current_pos + size]
    
    def readline(self) -> bytes:
        """Read a line from memory-mapped file."""
        if not self._mmap:
            raise RuntimeError("MemoryMappedFile not opened")
        
        return self._mmap.readline()
    
    def write(self, data: bytes) -> int:
        """Write to memory-mapped file."""
        if not self._mmap:
            raise RuntimeError("MemoryMappedFile not opened")
        
        return self._mmap.write(data)
    
    def seek(self, position: int) -> int:
        """Seek to position."""
        if self._mmap:
            self._mmap.seek(position)
            return position
        return 0
    
    def tell(self) -> int:
        """Get current position."""
        if self._mmap:
            return self._mmap.tell()
        return 0
    
    def size(self) -> int:
        """Get file size."""
        if self._mmap:
            return self._mmap.size()
        return 0


class BatchFileProcessor:
    """Batches file operations for improved I/O performance."""
    
    def __init__(self,
                 batch_size: int = 50,
                 batch_timeout: float = 1.0,
                 max_concurrent: int = 5):
        """
        Initialize batch processor.
        
        Args:
            batch_size: Maximum operations per batch
            batch_timeout: Maximum time to wait for batch
            max_concurrent: Maximum concurrent batches
        """
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.max_concurrent = max_concurrent
        
        self._operations: deque = deque()
        self._lock = threading.RLock()
        self._batch_thread: Optional[threading.Thread] = None
        self._running = False
        
        # Statistics
        self._stats = {
            "operations_batched": 0,
            "batches_processed": 0,
            "total_processing_time": 0.0,
            "avg_batch_size": 0.0
        }
        
        self._start_batch_processor()
        
        logger.debug("BatchFileProcessor initialized",
                    batch_size=batch_size,
                    batch_timeout=batch_timeout)
    
    def _start_batch_processor(self):
        """Start batch processing thread."""
        self._running = True
        self._batch_thread = threading.Thread(
            target=self._batch_worker,
            name="BatchFileProcessor",
            daemon=True
        )
        self._batch_thread.start()
    
    def _batch_worker(self):
        """Process batches of file operations."""
        try:
            while self._running:
                batch = self._collect_batch()
                if batch:
                    self._process_batch(batch)
                else:
                    time.sleep(0.1)  # No operations, brief pause
                    
        except Exception as e:
            logger.error("Batch processor error", error=str(e), exc_info=True)
    
    def _collect_batch(self) -> List[FileOperation]:
        """Collect operations for a batch."""
        batch = []
        start_time = time.time()
        
        with self._lock:
            # Collect operations until batch size or timeout
            while (len(batch) < self.batch_size and 
                   time.time() - start_time < self.batch_timeout):
                
                if self._operations:
                    batch.append(self._operations.popleft())
                else:
                    # No more operations, wait briefly
                    time.sleep(0.01)
                
                # Check if we should continue waiting
                if not self._operations and batch:
                    break
        
        return batch
    
    def _process_batch(self, batch: List[FileOperation]):
        """Process a batch of file operations."""
        if not batch:
            return
        
        start_time = time.time()
        
        try:
            # Group operations by type for efficiency
            read_ops = []
            write_ops = []
            stat_ops = []
            delete_ops = []
            
            for op in batch:
                if op.operation_type == 'read':
                    read_ops.append(op)
                elif op.operation_type == 'write':
                    write_ops.append(op)
                elif op.operation_type == 'stat':
                    stat_ops.append(op)
                elif op.operation_type == 'delete':
                    delete_ops.append(op)
            
            # Process each type in batch
            self._process_read_batch(read_ops)
            self._process_write_batch(write_ops)
            self._process_stat_batch(stat_ops)
            self._process_delete_batch(delete_ops)
            
            # Update statistics
            processing_time = time.time() - start_time
            with self._lock:
                self._stats["operations_batched"] += len(batch)
                self._stats["batches_processed"] += 1
                self._stats["total_processing_time"] += processing_time
                self._stats["avg_batch_size"] = (
                    self._stats["operations_batched"] / self._stats["batches_processed"]
                )
            
            logger.debug("Batch processed",
                        batch_size=len(batch),
                        processing_time=processing_time,
                        read_ops=len(read_ops),
                        write_ops=len(write_ops),
                        stat_ops=len(stat_ops),
                        delete_ops=len(delete_ops))
        
        except Exception as e:
            logger.error("Error processing batch",
                        batch_size=len(batch),
                        error=str(e),
                        exc_info=True)
    
    def _process_read_batch(self, operations: List[FileOperation]):
        """Process batch of read operations."""
        for op in operations:
            try:
                if op.file_path.exists():
                    with open(op.file_path, 'r', encoding=op.encoding) as f:
                        content = f.read()
                    
                    if op.callback:
                        op.callback(content, None)
                else:
                    error = FileNotFoundError(f"File not found: {op.file_path}")
                    if op.callback:
                        op.callback(None, error)
                        
            except Exception as e:
                logger.error("Read operation failed",
                           file_path=str(op.file_path),
                           error=str(e))
                if op.callback:
                    op.callback(None, e)
    
    def _process_write_batch(self, operations: List[FileOperation]):
        """Process batch of write operations."""
        for op in operations:
            try:
                # Ensure parent directory exists
                op.file_path.parent.mkdir(parents=True, exist_ok=True)
                
                with open(op.file_path, 'w', encoding=op.encoding) as f:
                    f.write(op.data)
                
                if op.callback:
                    op.callback(True, None)
                    
            except Exception as e:
                logger.error("Write operation failed",
                           file_path=str(op.file_path),
                           error=str(e))
                if op.callback:
                    op.callback(False, e)
    
    def _process_stat_batch(self, operations: List[FileOperation]):
        """Process batch of stat operations."""
        for op in operations:
            try:
                stat_result = op.file_path.stat() if op.file_path.exists() else None
                
                if op.callback:
                    op.callback(stat_result, None)
                    
            except Exception as e:
                logger.error("Stat operation failed",
                           file_path=str(op.file_path),
                           error=str(e))
                if op.callback:
                    op.callback(None, e)
    
    def _process_delete_batch(self, operations: List[FileOperation]):
        """Process batch of delete operations."""
        for op in operations:
            try:
                success = False
                if op.file_path.exists():
                    if op.file_path.is_file():
                        op.file_path.unlink()
                        success = True
                    elif op.file_path.is_dir():
                        op.file_path.rmdir()
                        success = True
                
                if op.callback:
                    op.callback(success, None)
                    
            except Exception as e:
                logger.error("Delete operation failed",
                           file_path=str(op.file_path),
                           error=str(e))
                if op.callback:
                    op.callback(False, e)
    
    def submit_operation(self, operation: FileOperation) -> bool:
        """Submit file operation for batch processing."""
        with self._lock:
            if len(self._operations) >= self.batch_size * 10:  # Prevent unbounded growth
                logger.warning("Operation queue full, dropping operation",
                             file_path=str(operation.file_path),
                             operation_type=operation.operation_type)
                return False
            
            self._operations.append(operation)
            return True
    
    def get_stats(self) -> Dict[str, Any]:
        """Get batch processor statistics."""
        with self._lock:
            return {
                **self._stats.copy(),
                "pending_operations": len(self._operations),
                "running": self._running
            }
    
    def shutdown(self):
        """Shutdown batch processor."""
        self._running = False
        if self._batch_thread and self._batch_thread.is_alive():
            self._batch_thread.join(timeout=5.0)
        logger.debug("Batch processor shutdown completed")


class FileOptimizer:
    """
    Main file system optimizer with caching, batching, and streaming.
    """
    
    def __init__(self):
        """Initialize file optimizer."""
        self._batch_processor = BatchFileProcessor()
        self._cache_manager = get_cache_manager()
        self._telemetry = get_telemetry_manager()
        
        # File handle pool for reuse
        self._file_handles: Dict[str, Any] = {}
        self._handle_lock = threading.RLock()
        
        logger.info("FileOptimizer initialized")
    
    @memoize(cache_name="file_metadata", ttl=300.0)
    def get_file_info(self, file_path: Union[str, Path]) -> Optional[Dict[str, Any]]:
        """
        Get file information with caching.
        
        Args:
            file_path: Path to file
            
        Returns:
            File information dictionary
        """
        path = Path(file_path)
        
        try:
            if not path.exists():
                return None
            
            stat = path.stat()
            
            return {
                "size": stat.st_size,
                "modified_time": stat.st_mtime,
                "created_time": stat.st_ctime,
                "is_file": path.is_file(),
                "is_dir": path.is_dir(),
                "permissions": oct(stat.st_mode)[-3:],
                "path": str(path.absolute())
            }
            
        except Exception as e:
            logger.error("Error getting file info",
                        file_path=str(file_path),
                        error=str(e))
            return None
    
    def read_file_optimized(self,
                          file_path: Union[str, Path],
                          encoding: str = 'utf-8',
                          use_cache: bool = True,
                          stream: bool = False) -> Union[str, FileStreamReader, None]:
        """
        Read file with optimization strategies.
        
        Args:
            file_path: Path to file
            encoding: Text encoding
            use_cache: Whether to use cache
            stream: Whether to return stream reader
            
        Returns:
            File content, stream reader, or None
        """
        path = Path(file_path)
        path_str = str(path)
        
        # Check cache first if enabled
        if use_cache and not stream:
            cache = self._cache_manager.get_cache("file_content")
            if cache:
                cached_content = cache.get(path_str)
                if cached_content is not None:
                    logger.debug("File read from cache", file_path=path_str)
                    return cached_content
        
        try:
            # Record telemetry
            with self._telemetry.trace_boundary_call(
                "outbound", "file_system", str(path), "read"
            ) if self._telemetry else nullcontext():
                
                if stream:
                    # Return streaming reader for large files
                    return FileStreamReader(path)
                
                # Check file size for optimization strategy
                file_info = self.get_file_info(path)
                if not file_info:
                    return None
                
                file_size = file_info["size"]
                
                if file_size > 10 * 1024 * 1024:  # 10MB threshold
                    # Use memory mapping for large files
                    with MemoryMappedFile(path, 'r') as mmf:
                        content = mmf.read().decode(encoding, errors='replace')
                else:
                    # Regular read for small files
                    with open(path, 'r', encoding=encoding) as f:
                        content = f.read()
                
                # Cache the content if not too large
                if use_cache and file_size < 1024 * 1024:  # 1MB cache limit
                    cache = self._cache_manager.get_cache("file_content")
                    if cache:
                        # TTL based on file size (larger files cached shorter)
                        ttl = max(300, 3600 - (file_size // 1024))  # 5min to 1hour
                        cache.put(path_str, content, ttl=ttl)
                
                logger.debug("File read completed",
                           file_path=path_str,
                           size=file_size,
                           cached=use_cache)
                
                return content
        
        except Exception as e:
            logger.error("Error reading file",
                        file_path=path_str,
                        error=str(e))
            return None
    
    def write_file_optimized(self,
                           file_path: Union[str, Path],
                           content: str,
                           encoding: str = 'utf-8',
                           atomic: bool = True,
                           batch: bool = False) -> bool:
        """
        Write file with optimization strategies.
        
        Args:
            file_path: Path to file
            content: Content to write
            encoding: Text encoding
            atomic: Whether to use atomic write
            batch: Whether to batch the operation
            
        Returns:
            True if successful
        """
        path = Path(file_path)
        
        if batch:
            # Submit to batch processor
            operation = FileOperation(
                operation_type='write',
                file_path=path,
                data=content,
                encoding=encoding
            )
            return self._batch_processor.submit_operation(operation)
        
        try:
            # Record telemetry
            with self._telemetry.trace_boundary_call(
                "outbound", "file_system", str(path), "write"
            ) if self._telemetry else nullcontext():
                
                if atomic:
                    # Use atomic write for safety
                    from atomic_write import atomic_write
                    success = atomic_write(path, content, encoding=encoding)
                else:
                    # Direct write for performance
                    path.parent.mkdir(parents=True, exist_ok=True)
                    with open(path, 'w', encoding=encoding) as f:
                        f.write(content)
                    success = True
                
                if success:
                    # Invalidate cache
                    cache = self._cache_manager.get_cache("file_content")
                    if cache:
                        cache.delete(str(path))
                    
                    logger.debug("File write completed",
                               file_path=str(path),
                               size=len(content),
                               atomic=atomic)
                
                return success
        
        except Exception as e:
            logger.error("Error writing file",
                        file_path=str(path),
                        error=str(e))
            return False
    
    def copy_file_optimized(self,
                          src_path: Union[str, Path],
                          dst_path: Union[str, Path],
                          chunk_size: int = 64 * 1024) -> bool:
        """
        Copy file with streaming for large files.
        
        Args:
            src_path: Source file path
            dst_path: Destination file path
            chunk_size: Size of chunks for streaming copy
            
        Returns:
            True if successful
        """
        src = Path(src_path)
        dst = Path(dst_path)
        
        try:
            # Ensure destination directory exists
            dst.parent.mkdir(parents=True, exist_ok=True)
            
            # Get source file size
            src_size = src.stat().st_size
            
            # Record telemetry
            with self._telemetry.trace_boundary_call(
                "outbound", "file_system", f"{src}->{dst}", "copy"
            ) if self._telemetry else nullcontext():
                
                if src_size > 100 * 1024 * 1024:  # 100MB threshold
                    # Streaming copy for large files
                    with open(src, 'rb') as src_file, open(dst, 'wb') as dst_file:
                        while True:
                            chunk = src_file.read(chunk_size)
                            if not chunk:
                                break
                            dst_file.write(chunk)
                else:
                    # Memory copy for small files
                    with open(src, 'rb') as src_file:
                        content = src_file.read()
                    with open(dst, 'wb') as dst_file:
                        dst_file.write(content)
                
                logger.debug("File copy completed",
                           src_path=str(src),
                           dst_path=str(dst),
                           size=src_size)
                
                return True
        
        except Exception as e:
            logger.error("Error copying file",
                        src_path=str(src),
                        dst_path=str(dst),
                        error=str(e))
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """Get optimizer statistics."""
        return {
            "batch_processor": self._batch_processor.get_stats(),
            "cache_manager": self._cache_manager.get_stats()
        }
    
    def shutdown(self):
        """Shutdown file optimizer."""
        self._batch_processor.shutdown()
        logger.info("File optimizer shutdown completed")


# Context manager for null context (Python 3.7+ has nullcontext)
class nullcontext:
    def __enter__(self):
        return self
    def __exit__(self, *args):
        pass


# Global file optimizer instance
_file_optimizer: Optional[FileOptimizer] = None
_optimizer_lock = threading.RLock()


def get_file_optimizer() -> FileOptimizer:
    """Get or create global file optimizer."""
    global _file_optimizer
    
    with _optimizer_lock:
        if _file_optimizer is None:
            _file_optimizer = FileOptimizer()
        
        return _file_optimizer


def shutdown_file_optimizer():
    """Shutdown the global file optimizer."""
    global _file_optimizer
    
    with _optimizer_lock:
        if _file_optimizer:
            _file_optimizer.shutdown()
            _file_optimizer = None
```
  ```
  --- END OF FILE scribe/core\file_optimizer.py ---
  --- START OF FILE scribe/core\health_monitor.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Engine Health Monitor

Implements comprehensive health monitoring, metrics collection, and alerting
for production deployment with proactive issue detection and self-healing.
"""

import time
import threading
import psutil
import socket
from typing import Dict, Any, List, Optional, Callable, Union
from dataclasses import dataclass, field
from enum import Enum
from http.server import HTTPServer, BaseHTTPRequestHandler
import json
import structlog

from .logging_config import get_scribe_logger
from .telemetry import get_telemetry_manager
from .error_recovery import get_error_recovery_manager

logger = get_scribe_logger(__name__)


class HealthStatus(Enum):
    """Health status levels."""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    CRITICAL = "critical"


class AlertSeverity(Enum):
    """Alert severity levels."""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class HealthCheck:
    """Represents a health check configuration."""
    name: str
    check_func: Callable[[], Dict[str, Any]]
    interval_seconds: float = 30.0
    timeout_seconds: float = 10.0
    enabled: bool = True
    alert_on_failure: bool = True
    failure_threshold: int = 3
    recovery_threshold: int = 2
    
    # Runtime state
    last_check_time: float = 0.0
    last_status: HealthStatus = HealthStatus.HEALTHY
    consecutive_failures: int = 0
    consecutive_successes: int = 0
    total_checks: int = 0
    total_failures: int = 0
    avg_response_time: float = 0.0
    last_error: Optional[str] = None


@dataclass 
class Alert:
    """Represents a health alert."""
    alert_id: str
    severity: AlertSeverity
    component: str
    message: str
    timestamp: float
    resolved: bool = False
    resolved_at: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def duration(self) -> Optional[float]:
        """Get alert duration if resolved."""
        if self.resolved and self.resolved_at:
            return self.resolved_at - self.timestamp
        return None


class HealthMetrics:
    """Collects and manages health metrics."""
    
    def __init__(self):
        """Initialize health metrics collector."""
        self._metrics: Dict[str, List[float]] = {}
        self._lock = threading.RLock()
        self.max_history = 100  # Keep last 100 data points
        
    def record_metric(self, name: str, value: float):
        """Record a metric value."""
        with self._lock:
            if name not in self._metrics:
                self._metrics[name] = []
            
            self._metrics[name].append(value)
            
            # Trim history
            if len(self._metrics[name]) > self.max_history:
                self._metrics[name] = self._metrics[name][-self.max_history:]
    
    def get_metric_stats(self, name: str) -> Optional[Dict[str, float]]:
        """Get statistics for a metric."""
        with self._lock:
            values = self._metrics.get(name, [])
            if not values:
                return None
            
            return {
                "current": values[-1],
                "average": sum(values) / len(values),
                "min": min(values),
                "max": max(values),
                "count": len(values)
            }
    
    def get_all_metrics(self) -> Dict[str, Dict[str, float]]:
        """Get statistics for all metrics."""
        with self._lock:
            return {name: self.get_metric_stats(name) 
                   for name in self._metrics.keys()}


class HealthHTTPHandler(BaseHTTPRequestHandler):
    """HTTP handler for health check endpoints."""
    
    def __init__(self, health_monitor, *args, **kwargs):
        self.health_monitor = health_monitor
        super().__init__(*args, **kwargs)
    
    def do_GET(self):
        """Handle GET requests."""
        try:
            if self.path == "/health":
                self._handle_health_check()
            elif self.path == "/health/detailed":
                self._handle_detailed_health()
            elif self.path == "/metrics":
                self._handle_metrics()
            elif self.path == "/alerts":
                self._handle_alerts()
            else:
                self._send_response(404, {"error": "Not found"})
                
        except Exception as e:
            logger.error("Health endpoint error", error=str(e))
            self._send_response(500, {"error": "Internal server error"})
    
    def _handle_health_check(self):
        """Handle basic health check."""
        overall_status = self.health_monitor.get_overall_status()
        
        status_code = 200
        if overall_status == HealthStatus.DEGRADED:
            status_code = 200  # Still operational
        elif overall_status == HealthStatus.UNHEALTHY:
            status_code = 503  # Service unavailable
        elif overall_status == HealthStatus.CRITICAL:
            status_code = 503  # Service unavailable
        
        self._send_response(status_code, {
            "status": overall_status.value,
            "timestamp": time.time()
        })
    
    def _handle_detailed_health(self):
        """Handle detailed health check."""
        health_report = self.health_monitor.get_health_report()
        
        status_code = 200
        if health_report["overall_status"] in ["unhealthy", "critical"]:
            status_code = 503
        
        self._send_response(status_code, health_report)
    
    def _handle_metrics(self):
        """Handle metrics endpoint."""
        metrics = self.health_monitor.get_metrics()
        self._send_response(200, metrics)
    
    def _handle_alerts(self):
        """Handle alerts endpoint."""
        alerts = self.health_monitor.get_active_alerts()
        self._send_response(200, {"alerts": alerts})
    
    def _send_response(self, status_code: int, data: Dict[str, Any]):
        """Send JSON response."""
        self.send_response(status_code)
        self.send_header('Content-Type', 'application/json')
        self.send_header('Access-Control-Allow-Origin', '*')
        self.end_headers()
        
        response = json.dumps(data, indent=2, default=str)
        self.wfile.write(response.encode('utf-8'))
    
    def log_message(self, format, *args):
        """Override to use our logger."""
        logger.debug("Health endpoint request", 
                    client=self.client_address[0],
                    path=self.path,
                    method=self.command)


class HealthMonitor:
    """
    Comprehensive health monitoring system for Scribe Engine.
    
    Features:
    - Configurable health checks
    - System resource monitoring
    - Alert management
    - HTTP health endpoints
    - Self-healing integration
    """
    
    def __init__(self, port: int = 9469):
        """
        Initialize health monitor.
        
        Args:
            port: Port for health check HTTP server
        """
        self.port = port
        
        # Health checks registry
        self._health_checks: Dict[str, HealthCheck] = {}
        self._lock = threading.RLock()
        
        # Monitoring components
        self._metrics = HealthMetrics()
        self._alerts: Dict[str, Alert] = {}
        self._alert_handlers: List[Callable[[Alert], None]] = []
        
        # System monitoring
        self._system_stats = {}
        self._monitor_thread: Optional[threading.Thread] = None
        self._http_server: Optional[HTTPServer] = None
        self._running = False
        
        # Register default health checks
        self._register_default_checks()
        
        logger.info("HealthMonitor initialized", port=port)
    
    def _register_default_checks(self):
        """Register default system health checks."""
        # System resource checks
        self.register_health_check(
            name="system_memory",
            check_func=self._check_system_memory,
            interval_seconds=30.0
        )
        
        self.register_health_check(
            name="system_cpu",
            check_func=self._check_system_cpu,
            interval_seconds=30.0
        )
        
        self.register_health_check(
            name="system_disk",
            check_func=self._check_system_disk,
            interval_seconds=60.0
        )
        
        # Application checks
        self.register_health_check(
            name="error_rate",
            check_func=self._check_error_rate,
            interval_seconds=30.0
        )
        
        self.register_health_check(
            name="component_health",
            check_func=self._check_component_health,
            interval_seconds=45.0
        )
    
    def _check_system_memory(self) -> Dict[str, Any]:
        """Check system memory usage."""
        try:
            memory = psutil.virtual_memory()
            swap = psutil.swap_memory()
            
            self._metrics.record_metric("memory_percent", memory.percent)
            self._metrics.record_metric("swap_percent", swap.percent)
            
            status = HealthStatus.HEALTHY
            if memory.percent > 90 or swap.percent > 80:
                status = HealthStatus.CRITICAL
            elif memory.percent > 80 or swap.percent > 60:
                status = HealthStatus.UNHEALTHY
            elif memory.percent > 70 or swap.percent > 40:
                status = HealthStatus.DEGRADED
            
            return {
                "status": status.value,
                "memory_percent": memory.percent,
                "memory_available_gb": memory.available / 1024**3,
                "swap_percent": swap.percent,
                "details": "Memory usage within acceptable limits"
            }
            
        except Exception as e:
            return {
                "status": HealthStatus.UNHEALTHY.value,
                "error": str(e),
                "details": "Failed to check memory usage"
            }
    
    def _check_system_cpu(self) -> Dict[str, Any]:
        """Check system CPU usage."""
        try:
            # Get CPU usage over 1 second interval
            cpu_percent = psutil.cpu_percent(interval=1.0)
            load_avg = psutil.getloadavg() if hasattr(psutil, 'getloadavg') else (0, 0, 0)
            
            self._metrics.record_metric("cpu_percent", cpu_percent)
            
            status = HealthStatus.HEALTHY
            if cpu_percent > 95:
                status = HealthStatus.CRITICAL
            elif cpu_percent > 85:
                status = HealthStatus.UNHEALTHY
            elif cpu_percent > 75:
                status = HealthStatus.DEGRADED
            
            return {
                "status": status.value,
                "cpu_percent": cpu_percent,
                "load_avg": load_avg,
                "cpu_count": psutil.cpu_count(),
                "details": "CPU usage within acceptable limits"
            }
            
        except Exception as e:
            return {
                "status": HealthStatus.UNHEALTHY.value,
                "error": str(e),
                "details": "Failed to check CPU usage"
            }
    
    def _check_system_disk(self) -> Dict[str, Any]:
        """Check system disk usage."""
        try:
            disk_usage = psutil.disk_usage('/')
            disk_percent = (disk_usage.used / disk_usage.total) * 100
            
            self._metrics.record_metric("disk_percent", disk_percent)
            
            status = HealthStatus.HEALTHY
            if disk_percent > 95:
                status = HealthStatus.CRITICAL
            elif disk_percent > 90:
                status = HealthStatus.UNHEALTHY
            elif disk_percent > 80:
                status = HealthStatus.DEGRADED
            
            return {
                "status": status.value,
                "disk_percent": disk_percent,
                "disk_free_gb": disk_usage.free / 1024**3,
                "disk_total_gb": disk_usage.total / 1024**3,
                "details": "Disk usage within acceptable limits"
            }
            
        except Exception as e:
            return {
                "status": HealthStatus.UNHEALTHY.value,
                "error": str(e),
                "details": "Failed to check disk usage"
            }
    
    def _check_error_rate(self) -> Dict[str, Any]:
        """Check application error rate."""
        try:
            error_manager = get_error_recovery_manager()
            error_stats = error_manager.get_error_stats()
            
            total_errors = error_stats.get("total_errors", 0)
            recent_errors = sum(1 for error in error_manager._error_history 
                              if time.time() - error.timestamp < 300)  # Last 5 minutes
            
            error_rate = recent_errors / 5.0  # Errors per minute
            self._metrics.record_metric("error_rate", error_rate)
            
            status = HealthStatus.HEALTHY
            if error_rate > 10:
                status = HealthStatus.CRITICAL
            elif error_rate > 5:
                status = HealthStatus.UNHEALTHY
            elif error_rate > 2:
                status = HealthStatus.DEGRADED
            
            return {
                "status": status.value,
                "error_rate_per_minute": error_rate,
                "recent_errors": recent_errors,
                "total_errors": total_errors,
                "details": "Error rate within acceptable limits"
            }
            
        except Exception as e:
            return {
                "status": HealthStatus.UNHEALTHY.value,
                "error": str(e),
                "details": "Failed to check error rate"
            }
    
    def _check_component_health(self) -> Dict[str, Any]:
        """Check health of application components."""
        try:
            error_manager = get_error_recovery_manager()
            component_health = error_manager._component_health
            
            unhealthy_components = [comp for comp, healthy in component_health.items() 
                                  if not healthy]
            
            status = HealthStatus.HEALTHY
            if len(unhealthy_components) > 3:
                status = HealthStatus.CRITICAL
            elif len(unhealthy_components) > 1:
                status = HealthStatus.UNHEALTHY
            elif len(unhealthy_components) > 0:
                status = HealthStatus.DEGRADED
            
            return {
                "status": status.value,
                "total_components": len(component_health),
                "healthy_components": len(component_health) - len(unhealthy_components),
                "unhealthy_components": unhealthy_components,
                "details": "Component health status acceptable"
            }
            
        except Exception as e:
            return {
                "status": HealthStatus.UNHEALTHY.value,
                "error": str(e),
                "details": "Failed to check component health"
            }
    
    def register_health_check(self,
                            name: str,
                            check_func: Callable[[], Dict[str, Any]],
                            interval_seconds: float = 30.0,
                            timeout_seconds: float = 10.0,
                            failure_threshold: int = 3) -> bool:
        """
        Register a custom health check.
        
        Args:
            name: Unique name for the health check
            check_func: Function that returns health status
            interval_seconds: Check interval
            timeout_seconds: Check timeout
            failure_threshold: Consecutive failures before alerting
            
        Returns:
            True if registered successfully
        """
        with self._lock:
            if name in self._health_checks:
                logger.warning("Health check already exists", name=name)
                return False
            
            health_check = HealthCheck(
                name=name,
                check_func=check_func,
                interval_seconds=interval_seconds,
                timeout_seconds=timeout_seconds,
                failure_threshold=failure_threshold
            )
            
            self._health_checks[name] = health_check
            logger.debug("Health check registered", name=name)
            return True
    
    def start(self):
        """Start health monitoring."""
        if self._running:
            return
        
        self._running = True
        
        # Start monitoring thread
        self._monitor_thread = threading.Thread(
            target=self._monitor_worker,
            name="HealthMonitor",
            daemon=True
        )
        self._monitor_thread.start()
        
        # Start HTTP server
        self._start_http_server()
        
        logger.info("Health monitoring started", port=self.port)
    
    def _start_http_server(self):
        """Start HTTP server for health endpoints."""
        try:
            def handler_factory(*args, **kwargs):
                return HealthHTTPHandler(self, *args, **kwargs)
            
            self._http_server = HTTPServer(('0.0.0.0', self.port), handler_factory)
            
            server_thread = threading.Thread(
                target=self._http_server.serve_forever,
                name="HealthHTTPServer",
                daemon=True
            )
            server_thread.start()
            
            logger.info("Health HTTP server started", port=self.port)
            
        except Exception as e:
            logger.error("Failed to start health HTTP server",
                        port=self.port,
                        error=str(e))
    
    def _monitor_worker(self):
        """Main monitoring worker thread."""
        try:
            while self._running:
                current_time = time.time()
                
                with self._lock:
                    for health_check in self._health_checks.values():
                        if not health_check.enabled:
                            continue
                        
                        # Check if it's time to run this check
                        if (current_time - health_check.last_check_time >= 
                            health_check.interval_seconds):
                            
                            self._run_health_check(health_check, current_time)
                
                # Sleep before next iteration
                time.sleep(1.0)
                
        except Exception as e:
            logger.error("Health monitor worker error", error=str(e), exc_info=True)
    
    def _run_health_check(self, health_check: HealthCheck, current_time: float):
        """Run a single health check."""
        try:
            start_time = time.time()
            
            # Execute health check with timeout
            result = health_check.check_func()
            
            execution_time = time.time() - start_time
            
            # Update statistics
            health_check.last_check_time = current_time
            health_check.total_checks += 1
            
            # Update average response time
            if health_check.avg_response_time == 0:
                health_check.avg_response_time = execution_time
            else:
                health_check.avg_response_time = (
                    (health_check.avg_response_time * 0.9) + (execution_time * 0.1)
                )
            
            # Determine status
            status_str = result.get("status", "unknown")
            if status_str == "healthy":
                status = HealthStatus.HEALTHY
            elif status_str == "degraded":
                status = HealthStatus.DEGRADED
            elif status_str == "unhealthy":
                status = HealthStatus.UNHEALTHY
            elif status_str == "critical":
                status = HealthStatus.CRITICAL
            else:
                status = HealthStatus.UNHEALTHY
            
            # Update status tracking
            if status == HealthStatus.HEALTHY:
                health_check.consecutive_failures = 0
                health_check.consecutive_successes += 1
                health_check.last_error = None
            else:
                health_check.consecutive_successes = 0
                health_check.consecutive_failures += 1
                health_check.total_failures += 1
                health_check.last_error = result.get("error") or result.get("details")
            
            health_check.last_status = status
            
            # Check for alerts
            if (health_check.alert_on_failure and 
                health_check.consecutive_failures >= health_check.failure_threshold):
                
                self._trigger_alert(health_check, result)
            
            # Record telemetry
            telemetry = get_telemetry_manager()
            if telemetry:
                telemetry.boundary_calls_counter.add(1, {
                    "interface_type": "internal",
                    "protocol": "health_check",
                    "operation": health_check.name,
                    "status": status.value
                })
                
                telemetry.boundary_call_duration_histogram.record(execution_time, {
                    "interface_type": "internal",
                    "protocol": "health_check",
                    "operation": health_check.name
                })
            
            logger.debug("Health check completed",
                        name=health_check.name,
                        status=status.value,
                        execution_time=execution_time)
        
        except Exception as e:
            health_check.consecutive_failures += 1
            health_check.total_failures += 1
            health_check.last_error = str(e)
            health_check.last_status = HealthStatus.CRITICAL
            
            logger.error("Health check failed",
                        name=health_check.name,
                        error=str(e))
            
            # Trigger alert for check failure
            if (health_check.alert_on_failure and 
                health_check.consecutive_failures >= health_check.failure_threshold):
                
                self._trigger_alert(health_check, {"error": str(e)})
    
    def _trigger_alert(self, health_check: HealthCheck, result: Dict[str, Any]):
        """Trigger an alert for a failed health check."""
        alert_id = f"{health_check.name}_{int(time.time())}"
        
        # Determine severity based on status
        severity = AlertSeverity.WARNING
        if health_check.last_status == HealthStatus.CRITICAL:
            severity = AlertSeverity.CRITICAL
        elif health_check.last_status == HealthStatus.UNHEALTHY:
            severity = AlertSeverity.ERROR
        
        alert = Alert(
            alert_id=alert_id,
            severity=severity,
            component=health_check.name,
            message=f"Health check '{health_check.name}' failed {health_check.consecutive_failures} times",
            timestamp=time.time(),
            metadata={
                "consecutive_failures": health_check.consecutive_failures,
                "failure_threshold": health_check.failure_threshold,
                "last_error": health_check.last_error,
                "check_result": result
            }
        )
        
        with self._lock:
            self._alerts[alert_id] = alert
        
        # Notify alert handlers
        for handler in self._alert_handlers:
            try:
                handler(alert)
            except Exception as e:
                logger.error("Alert handler failed",
                           handler=handler.__name__,
                           alert_id=alert_id,
                           error=str(e))
        
        logger.warning("Health alert triggered",
                      alert_id=alert_id,
                      component=health_check.name,
                      severity=severity.value)
    
    def add_alert_handler(self, handler: Callable[[Alert], None]):
        """Add an alert handler function."""
        self._alert_handlers.append(handler)
        logger.debug("Alert handler added", handler=handler.__name__)
    
    def get_overall_status(self) -> HealthStatus:
        """Get overall system health status."""
        with self._lock:
            if not self._health_checks:
                return HealthStatus.HEALTHY
            
            statuses = [check.last_status for check in self._health_checks.values() 
                       if check.enabled]
            
            if HealthStatus.CRITICAL in statuses:
                return HealthStatus.CRITICAL
            elif HealthStatus.UNHEALTHY in statuses:
                return HealthStatus.UNHEALTHY
            elif HealthStatus.DEGRADED in statuses:
                return HealthStatus.DEGRADED
            else:
                return HealthStatus.HEALTHY
    
    def get_health_report(self) -> Dict[str, Any]:
        """Get detailed health report."""
        with self._lock:
            overall_status = self.get_overall_status()
            
            checks_summary = {}
            for name, check in self._health_checks.items():
                checks_summary[name] = {
                    "status": check.last_status.value,
                    "last_check": check.last_check_time,
                    "consecutive_failures": check.consecutive_failures,
                    "total_checks": check.total_checks,
                    "total_failures": check.total_failures,
                    "avg_response_time": check.avg_response_time,
                    "last_error": check.last_error
                }
            
            return {
                "overall_status": overall_status.value,
                "timestamp": time.time(),
                "checks": checks_summary,
                "active_alerts": len([a for a in self._alerts.values() if not a.resolved]),
                "system_info": {
                    "uptime": time.time() - (self._monitor_thread.ident if self._monitor_thread else time.time()),
                    "process_id": os.getpid() if 'os' in globals() else 0
                }
            }
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get health metrics."""
        return self._metrics.get_all_metrics()
    
    def get_active_alerts(self) -> List[Dict[str, Any]]:
        """Get active (unresolved) alerts."""
        with self._lock:
            return [
                {
                    "alert_id": alert.alert_id,
                    "severity": alert.severity.value,
                    "component": alert.component,
                    "message": alert.message,
                    "timestamp": alert.timestamp,
                    "metadata": alert.metadata
                }
                for alert in self._alerts.values()
                if not alert.resolved
            ]
    
    def resolve_alert(self, alert_id: str) -> bool:
        """Mark an alert as resolved."""
        with self._lock:
            alert = self._alerts.get(alert_id)
            if alert and not alert.resolved:
                alert.resolved = True
                alert.resolved_at = time.time()
                
                logger.info("Alert resolved",
                           alert_id=alert_id,
                           duration=alert.duration)
                return True
            return False
    
    def stop(self):
        """Stop health monitoring."""
        if not self._running:
            return
        
        self._running = False
        
        # Stop HTTP server
        if self._http_server:
            self._http_server.shutdown()
            self._http_server = None
        
        # Wait for monitor thread
        if self._monitor_thread and self._monitor_thread.is_alive():
            self._monitor_thread.join(timeout=5.0)
        
        logger.info("Health monitoring stopped")


# Global health monitor instance
_health_monitor: Optional[HealthMonitor] = None
_monitor_lock = threading.RLock()


def get_health_monitor() -> Optional[HealthMonitor]:
    """Get the global health monitor instance."""
    return _health_monitor


def initialize_health_monitor(port: int = 9469) -> HealthMonitor:
    """
    Initialize global health monitor.
    
    Args:
        port: Port for health check HTTP server
        
    Returns:
        HealthMonitor instance
    """
    global _health_monitor
    
    with _monitor_lock:
        if _health_monitor is None:
            _health_monitor = HealthMonitor(port=port)
            _health_monitor.start()
        
        return _health_monitor


def shutdown_health_monitor():
    """Shutdown the global health monitor."""
    global _health_monitor
    
    with _monitor_lock:
        if _health_monitor:
            _health_monitor.stop()
            _health_monitor = None
```
  ```
  --- END OF FILE scribe/core\health_monitor.py ---
  --- START OF FILE scribe/core\hma_ports.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
HMA v2.1 Mandatory Port Definitions

This module defines all mandatory HMA ports required for v2.1 compliance.
These ports provide technology-agnostic interfaces for all boundary interactions.
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List, Callable
import asyncio
from dataclasses import dataclass
from enum import Enum

class PortStatus(Enum):
    """Port status enumeration"""
    AVAILABLE = "available"
    BUSY = "busy"
    ERROR = "error"
    OFFLINE = "offline"

@dataclass
class PluginExecutionContext:
    """Context for plugin execution"""
    plugin_id: str
    request_id: str
    correlation_id: Optional[str] = None
    metadata: Dict[str, Any] = None

class PluginExecutionPort(ABC):
    """HMA v2.1 mandatory port for plugin execution"""
    
    @abstractmethod
    async def execute_plugin(self, 
                           plugin_id: str, 
                           input_data: Dict[str, Any],
                           context: Optional[PluginExecutionContext] = None) -> Dict[str, Any]:
        """
        Execute plugin with validated input
        
        Args:
            plugin_id: Unique plugin identifier
            input_data: Validated input data
            context: Execution context
            
        Returns:
            Plugin execution result
        """
        pass
    
    @abstractmethod
    def get_plugin_status(self, plugin_id: str) -> PortStatus:
        """Get current plugin execution status"""
        pass
    
    @abstractmethod
    async def validate_plugin_input(self, plugin_id: str, input_data: Dict[str, Any]) -> bool:
        """Validate input data against plugin schema"""
        pass
    
    @abstractmethod
    def list_available_plugins(self) -> List[str]:
        """List all available plugins"""
        pass

class CredBrokerQueryPort(ABC):
    """HMA v2.1 mandatory port for credential access"""
    
    @abstractmethod
    async def get_credential(self, 
                           credential_id: str, 
                           requesting_plugin: str,
                           context: Optional[Dict[str, Any]] = None) -> Optional[str]:
        """
        Retrieve credential with access control
        
        Args:
            credential_id: Unique credential identifier
            requesting_plugin: Plugin requesting the credential
            context: Request context for audit
            
        Returns:
            Credential value if authorized, None otherwise
        """
        pass
    
    @abstractmethod
    async def validate_access(self, credential_id: str, plugin_id: str) -> bool:
        """Validate plugin access to credential"""
        pass
    
    @abstractmethod
    async def store_credential(self, 
                             credential_id: str, 
                             credential_value: str,
                             metadata: Optional[Dict[str, Any]] = None) -> bool:
        """Store credential securely"""
        pass
    
    @abstractmethod
    async def revoke_access(self, credential_id: str, plugin_id: str) -> bool:
        """Revoke plugin access to credential"""
        pass

class EventBusPort(ABC):
    """HMA v2.1 mandatory port for event communication"""
    
    @abstractmethod
    async def publish_event(self, 
                          event_type: str,
                          event_data: Dict[str, Any], 
                          target: Optional[str] = None,
                          correlation_id: Optional[str] = None) -> bool:
        """
        Publish event with optional targeting
        
        Args:
            event_type: Type of event being published
            event_data: Event payload
            target: Optional target plugin/component
            correlation_id: For distributed tracing
            
        Returns:
            True if event was published successfully
        """
        pass
    
    @abstractmethod
    async def subscribe_to_events(self, 
                                event_types: List[str], 
                                callback: Callable,
                                subscriber_id: str) -> bool:
        """Subscribe to specific event types"""
        pass
    
    @abstractmethod
    async def unsubscribe_from_events(self, 
                                    event_types: List[str],
                                    subscriber_id: str) -> bool:
        """Unsubscribe from event types"""
        pass
    
    @abstractmethod
    def get_event_statistics(self) -> Dict[str, Any]:
        """Get event bus statistics"""
        pass

class ObservabilityPort(ABC):
    """HMA v2.1 mandatory port for telemetry"""
    
    @abstractmethod
    def emit_metric(self, 
                   name: str, 
                   value: float, 
                   labels: Dict[str, str],
                   metric_type: str = "counter") -> None:
        """Emit metric with HMA labels"""
        pass
    
    @abstractmethod
    def start_span(self, 
                  operation: str, 
                  component_id: str,
                  parent_span: Optional[Any] = None) -> Any:
        """Start distributed trace span"""
        pass
    
    @abstractmethod
    def emit_log(self, 
                level: str, 
                message: str, 
                component_id: str,
                extra_data: Optional[Dict[str, Any]] = None) -> None:
        """Emit structured log with HMA context"""
        pass
    
    @abstractmethod
    def record_boundary_crossing(self, 
                               from_component: str,
                               to_component: str,
                               operation: str,
                               duration_ms: Optional[float] = None) -> None:
        """Record HMA boundary crossing for compliance"""
        pass

class ConfigurationPort(ABC):
    """HMA v2.1 port for configuration management"""
    
    @abstractmethod
    async def get_config_value(self, 
                             key: str, 
                             component_id: str,
                             default: Any = None) -> Any:
        """Get configuration value for component"""
        pass
    
    @abstractmethod
    async def set_config_value(self, 
                             key: str, 
                             value: Any,
                             component_id: str) -> bool:
        """Set configuration value"""
        pass
    
    @abstractmethod
    async def validate_config(self, 
                            config: Dict[str, Any],
                            schema_id: str) -> bool:
        """Validate configuration against schema"""
        pass
    
    @abstractmethod
    def subscribe_to_config_changes(self, 
                                  callback: Callable,
                                  component_id: str) -> bool:
        """Subscribe to configuration changes"""
        pass

class HealthCheckPort(ABC):
    """HMA v2.1 port for health monitoring"""
    
    @abstractmethod
    async def check_component_health(self, component_id: str) -> Dict[str, Any]:
        """Check health status of a component"""
        pass
    
    @abstractmethod
    async def register_health_check(self, 
                                  component_id: str,
                                  check_function: Callable) -> bool:
        """Register health check function"""
        pass
    
    @abstractmethod
    async def get_system_health(self) -> Dict[str, Any]:
        """Get overall system health status"""
        pass

# Additional ports for plugin operations
class CommandExecutionPort(ABC):
    """HMA v2.2 port for secure command execution"""
    
    @abstractmethod
    async def execute_command_safely(self, 
                                   command_list: List[str],
                                   cwd: Optional[str] = None,
                                   timeout: int = 30,
                                   allowed_env_vars: Optional[List[str]] = None) -> tuple[bool, str, str]:
        """Execute command with security controls"""
        pass
    
    @abstractmethod
    async def validate_command_security(self, command_list: List[str]) -> bool:
        """Validate command against security policies"""
        pass

class FileSystemPort(ABC):
    """HMA v2.2 port for file system operations"""
    
    @abstractmethod
    async def read_file_safely(self, file_path: str) -> Optional[str]:
        """Read file with security validation"""
        pass
    
    @abstractmethod
    async def write_file_safely(self, file_path: str, content: str) -> bool:
        """Write file with security validation"""
        pass
    
    @abstractmethod
    async def validate_file_access(self, file_path: str, operation: str) -> bool:
        """Validate file access permissions"""
        pass

class LoggingPort(ABC):
    """HMA v2.2 port for structured logging"""
    
    @abstractmethod
    def log_info(self, message: str, **context) -> None:
        """Log information with context"""
        pass
    
    @abstractmethod
    def log_warning(self, message: str, **context) -> None:
        """Log warning with context"""
        pass
    
    @abstractmethod
    def log_error(self, message: str, **context) -> None:
        """Log error with context"""
        pass
    
    @abstractmethod
    def log_debug(self, message: str, **context) -> None:
        """Log debug information with context"""
        pass

class PluginContextPort(ABC):
    """HMA v2.2 port providing plugin execution context"""
    
    @abstractmethod
    def get_plugin_id(self) -> str:
        """Get current plugin identifier"""
        pass
    
    @abstractmethod
    def get_execution_context(self) -> Dict[str, Any]:
        """Get plugin execution context"""
        pass
    
    @abstractmethod
    def get_port(self, port_type: str) -> Any:
        """Get access to other ports through context"""
        pass

# Port registry for dependency injection
class PortRegistry:
    """Registry for HMA ports to enable dependency injection"""
    
    def __init__(self):
        self._ports: Dict[str, Any] = {}
    
    def register_port(self, port_type: str, port_instance: Any) -> None:
        """Register a port implementation"""
        self._ports[port_type] = port_instance
    
    def get_port(self, port_type: str) -> Any:
        """Get a registered port implementation"""
        if port_type not in self._ports:
            raise ValueError(f"Port type '{port_type}' not registered")
        return self._ports[port_type]
    
    def list_registered_ports(self) -> List[str]:
        """List all registered port types"""
        return list(self._ports.keys())
```
  ```
  --- END OF FILE scribe/core\hma_ports.py ---
  --- START OF FILE scribe/core\hma_telemetry.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
HMA v2.1 Telemetry System

Implements mandatory OpenTelemetry boundary telemetry with HMA resource attributes
and compliance tracking for all boundary crossings.
"""

import time
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from contextlib import contextmanager

# Fallback context manager for when OpenTelemetry is not available
class FallbackSpan:
    """Fallback span implementation"""
    def __enter__(self):
        return self
    def __exit__(self, exc_type, exc_val, exc_tb):
        pass

@contextmanager
def fallback_span():
    """Fallback span context manager"""
    yield FallbackSpan()

# Optional OpenTelemetry imports with fallbacks
try:
    from opentelemetry import trace, metrics
    from opentelemetry.sdk.trace import TracerProvider
    from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter
    from opentelemetry.sdk.resources import Resource
    from opentelemetry.sdk.metrics import MeterProvider
    from opentelemetry.exporter.prometheus import PrometheusMetricReader
    from prometheus_client import start_http_server
    HAS_OPENTELEMETRY = True
except ImportError:
    # Fallback implementations for when OpenTelemetry is not available
    trace = None
    metrics = None
    TracerProvider = None
    BatchSpanProcessor = None
    ConsoleSpanExporter = None
    Resource = None
    MeterProvider = None
    PrometheusMetricReader = None
    start_http_server = None
    HAS_OPENTELEMETRY = False

from .logging_config import get_scribe_logger
from .hma_ports import ObservabilityPort

logger = get_scribe_logger(__name__)

@dataclass
class HMAComponent:
    """HMA Component identification"""
    component_type: str  # L2-Core | L2-Orchestrator | L3-Capability
    component_id: str
    layer: str  # L2 | L3
    version: str = "2.1.0"

class HMATelemetry(ObservabilityPort):
    """HMA v2.1 compliant OpenTelemetry implementation"""
    
    def __init__(self, component: HMAComponent, 
                 jaeger_endpoint: Optional[str] = None,
                 prometheus_port: int = 8000):
        self.component = component
        self.prometheus_port = prometheus_port
        
        if HAS_OPENTELEMETRY:
            # Initialize OpenTelemetry when available
            self._setup_tracing(jaeger_endpoint)
            self._setup_metrics()
            self._create_hma_metrics()
            logger.info("HMA Telemetry initialized with OpenTelemetry",
                       component_type=component.component_type,
                       component_id=component.component_id,
                       layer=component.layer)
        else:
            # Initialize fallback telemetry
            self._setup_fallback_telemetry()
            logger.warning("HMA Telemetry initialized with fallback (OpenTelemetry not available)",
                          component_type=component.component_type,
                          component_id=component.component_id,
                          layer=component.layer)
    
    def _setup_fallback_telemetry(self):
        """Setup fallback telemetry when OpenTelemetry is not available"""
        # Initialize fallback attributes
        self.tracer = None
        self.meter = None
        self.boundary_calls_counter = None
        self.plugin_execution_histogram = None
        self.active_plugins_gauge = None
        self.validation_counter = None
        self.error_counter = None
        
        logger.info("Fallback telemetry initialized - metrics will be logged only")
    
    def _setup_tracing(self, jaeger_endpoint: Optional[str]):
        """Setup distributed tracing with HMA resource attributes"""
        # Create HMA-compliant resource
        resource = Resource.create({
            # HMA v2.1 mandatory resource attributes
            "hma.component.type": self.component.component_type,
            "hma.component.id": self.component.component_id,
            "hma.layer": self.component.layer,
            "hma.version": "2.1",
            
            # Standard OTEL attributes
            "service.name": self.component.component_id,
            "service.version": self.component.version,
            "service.namespace": "scribe-engine"
        })
        
        # Setup tracer provider
        tracer_provider = TracerProvider(resource=resource)
        
        # Add console exporter for development
        tracer_provider.add_span_processor(
            BatchSpanProcessor(ConsoleSpanExporter())
        )
        
        # Add Jaeger exporter if endpoint provided
        if jaeger_endpoint:
            try:
                # Note: Would use JaegerExporter if available
                # from opentelemetry.exporter.jaeger.thrift import JaegerExporter
                # jaeger_exporter = JaegerExporter(collector_endpoint=jaeger_endpoint)
                # tracer_provider.add_span_processor(BatchSpanProcessor(jaeger_exporter))
                logger.info("Jaeger exporter configured", endpoint=jaeger_endpoint)
            except ImportError:
                logger.warning("Jaeger exporter not available, using console exporter only")
        
        trace.set_tracer_provider(tracer_provider)
        self.tracer = trace.get_tracer(__name__)
    
    def _setup_metrics(self):
        """Setup metrics with Prometheus"""
        try:
            # Create metric reader for Prometheus
            metric_reader = PrometheusMetricReader()
            
            # Create meter provider with HMA resource
            resource = Resource.create({
                "hma.component.type": self.component.component_type,
                "hma.component.id": self.component.component_id,
                "hma.layer": self.component.layer,
                "service.name": self.component.component_id,
            })
            
            meter_provider = MeterProvider(
                resource=resource,
                metric_readers=[metric_reader]
            )
            
            metrics.set_meter_provider(meter_provider)
            self.meter = metrics.get_meter(__name__)
            
            # Start Prometheus HTTP server
            start_http_server(self.prometheus_port)
            logger.info("Prometheus metrics server started", port=self.prometheus_port)
            
        except Exception as e:
            logger.error("Failed to setup metrics", error=str(e))
            # Create fallback meter
            self.meter = metrics.get_meter(__name__)
    
    def _create_hma_metrics(self):
        """Create HMA v2.1 mandatory metrics"""
        # Boundary call metrics
        self.boundary_calls_counter = self.meter.create_counter(
            "hma_boundary_calls_total",
            description="Total HMA boundary calls",
            unit="1"
        )
        
        # Plugin execution metrics
        self.plugin_execution_histogram = self.meter.create_histogram(
            "hma_plugin_execution_duration_seconds", 
            description="Plugin execution duration",
            unit="s"
        )
        
        # Active plugins gauge
        self.active_plugins_gauge = self.meter.create_up_down_counter(
            "hma_active_plugins",
            description="Number of active plugins",
            unit="1"
        )
        
        # Validation metrics
        self.validation_counter = self.meter.create_counter(
            "hma_boundary_validations_total",
            description="Total boundary validations",
            unit="1"
        )
        
        # Error metrics
        self.error_counter = self.meter.create_counter(
            "hma_errors_total",
            description="Total HMA errors",
            unit="1"
        )
    
    def start_span(self, operation: str, component_id: str, parent_span: Optional[Any] = None):
        """Start distributed trace span with HMA attributes"""
        span_name = f"hma.{self.component.layer.lower()}.{operation}"
        
        span = self.tracer.start_span(
            span_name,
            attributes={
                "hma.operation": operation,
                "hma.component.id": component_id,
                "hma.component.type": self.component.component_type,
                "hma.layer": self.component.layer
            }
        )
        
        return span
    
    def trace_boundary_operation(self, operation: str, boundary_type: str, 
                                source_component: str, target_component: str):
        """Trace HMA boundary operations with compliance tracking"""
        if HAS_OPENTELEMETRY and self.tracer:
            span_name = f"hma.boundary.{boundary_type}.{operation}"
            
            span = self.tracer.start_span(
                span_name,
                attributes={
                    "hma.boundary.type": boundary_type,
                    "hma.operation": operation,
                    "hma.source.component": source_component,
                    "hma.target.component": target_component,
                    "hma.compliance.version": "2.1"
                }
            )
        else:
            # Fallback span for when OpenTelemetry is not available
            span = FallbackSpan()
        
        # Record boundary call metric
        self.record_boundary_crossing(source_component, target_component, operation)
        
        return span
    
    def emit_metric(self, name: str, value: float, labels: Dict[str, str], 
                   metric_type: str = "counter") -> None:
        """Emit metric with HMA labels"""
        # Add HMA context to labels
        hma_labels = {
            "hma_component_type": self.component.component_type,
            "hma_component_id": self.component.component_id,
            "hma_layer": self.component.layer,
            **labels
        }
        
        if HAS_OPENTELEMETRY and self.meter:
            try:
                if metric_type == "counter":
                    counter = self.meter.create_counter(name, description=f"HMA metric: {name}")
                    counter.add(value, hma_labels)
                elif metric_type == "histogram":
                    histogram = self.meter.create_histogram(name, description=f"HMA histogram: {name}")
                    histogram.record(value, hma_labels)
                elif metric_type == "gauge":
                    gauge = self.meter.create_up_down_counter(name, description=f"HMA gauge: {name}")
                    gauge.add(value, hma_labels)
                    
            except Exception as e:
                logger.error("Failed to emit metric", name=name, error=str(e))
        else:
            # Log metric when OpenTelemetry is not available
            logger.debug(f"Metric {metric_type}: {name}", value=value, **hma_labels)
    
    def emit_log(self, level: str, message: str, component_id: str,
                extra_data: Optional[Dict[str, Any]] = None) -> None:
        """Emit structured log with HMA context"""
        log_data = {
            "level": level,
            "message": message,
            "hma_component_type": self.component.component_type,
            "hma_component_id": component_id,
            "hma_layer": self.component.layer,
            "timestamp": time.time()
        }
        
        if extra_data:
            log_data.update(extra_data)
        
        # Use appropriate log level
        if level.upper() == "ERROR":
            logger.error(message, **log_data)
        elif level.upper() == "WARNING":
            logger.warning(message, **log_data)
        elif level.upper() == "INFO":
            logger.info(message, **log_data)
        else:
            logger.debug(message, **log_data)
    
    def record_boundary_crossing(self, from_component: str, to_component: str,
                               operation: str, duration_ms: Optional[float] = None) -> None:
        """Record HMA boundary crossing for compliance"""
        if HAS_OPENTELEMETRY and self.boundary_calls_counter:
            # Record boundary call
            self.boundary_calls_counter.add(1, {
                "from_component": from_component,
                "to_component": to_component,
                "operation": operation,
                "hma_layer": self.component.layer
            })
            
            # Record duration if provided
            if duration_ms is not None and self.plugin_execution_histogram:
                self.plugin_execution_histogram.record(duration_ms / 1000.0, {
                    "operation": operation,
                    "from_component": from_component,
                    "to_component": to_component
                })
        
        logger.debug("HMA boundary crossing recorded",
                    from_component=from_component,
                    to_component=to_component,
                    operation=operation,
                    duration_ms=duration_ms)
    
    def record_plugin_activity(self, plugin_id: str, active: bool) -> None:
        """Record plugin activation/deactivation"""
        if HAS_OPENTELEMETRY and self.active_plugins_gauge:
            delta = 1 if active else -1
            self.active_plugins_gauge.add(delta, {"plugin_id": plugin_id})
        
        logger.debug("Plugin activity recorded", plugin_id=plugin_id, active=active)
    
    def record_validation_result(self, boundary_type: str, success: bool) -> None:
        """Record boundary validation result"""
        if HAS_OPENTELEMETRY and self.validation_counter:
            self.validation_counter.add(1, {
                "boundary_type": boundary_type,
                "status": "success" if success else "failure"
            })
        
        logger.debug("Validation result recorded", boundary_type=boundary_type, success=success)
    
    def record_error(self, error_type: str, component_id: str, details: Optional[str] = None) -> None:
        """Record HMA compliance error"""
        if HAS_OPENTELEMETRY and self.error_counter:
            self.error_counter.add(1, {
                "error_type": error_type,
                "component_id": component_id,
                "hma_layer": self.component.layer
            })
        
        if details:
            self.emit_log("ERROR", f"HMA compliance error: {details}", component_id,
                         {"error_type": error_type})

def create_hma_telemetry(component_type: str, component_id: str, layer: str,
                        jaeger_endpoint: Optional[str] = None,
                        prometheus_port: int = 8000) -> HMATelemetry:
    """Factory function to create HMA telemetry instance"""
    component = HMAComponent(component_type, component_id, layer)
    return HMATelemetry(component, jaeger_endpoint, prometheus_port)
```
  ```
  --- END OF FILE scribe/core\hma_telemetry.py ---
  --- START OF FILE scribe/core\logging_config.py ---
  ```
  ```py
"""
Scribe Engine - Structured Logging Configuration

This module configures structlog for machine-parsable JSON log output.
All logs are emitted as JSON lines to stdout for easy ingestion into
monitoring systems like Grafana Loki or ELK stack.
"""

import sys
import logging
import structlog
from typing import Dict, Any, Optional


def configure_structured_logging(
    log_level: str = "INFO",
    include_stdlib_logs: bool = True,
    add_caller_info: bool = False
) -> None:
    """
    Configure structlog for structured JSON logging.
    
    This sets up the complete processor chain to output machine-parsable
    JSON logs with timestamps, log levels, and contextual information.
    
    Args:
        log_level: Minimum log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        include_stdlib_logs: Whether to capture standard library logs
        add_caller_info: Whether to include caller file/line information
    """
    # Configure standard library logging to work with structlog
    if include_stdlib_logs:
        logging.basicConfig(
            format="%(message)s",
            stream=sys.stdout,
            level=getattr(logging, log_level.upper())
        )
    
    # Build processor chain
    processors = [
        # Filter by log level
        structlog.stdlib.filter_by_level,
        
        # Add logger name
        structlog.stdlib.add_logger_name,
        
        # Add log level
        structlog.stdlib.add_log_level,
        
        # Handle positional arguments
        structlog.stdlib.PositionalArgumentsFormatter(),
        
        # Add ISO timestamp
        structlog.processors.TimeStamper(fmt="iso"),
        
        # Add stack info for exceptions
        structlog.processors.StackInfoRenderer(),
        
        # Format exception info
        structlog.processors.format_exc_info,
        
        # Handle Unicode properly
        structlog.processors.UnicodeDecoder(),
    ]
    
    # Add caller info if requested (useful for debugging)
    if add_caller_info:
        processors.insert(-1, structlog.processors.CallsiteParameterAdder(
            parameters=[structlog.processors.CallsiteParameter.FILENAME,
                       structlog.processors.CallsiteParameter.LINENO,
                       structlog.processors.CallsiteParameter.FUNC_NAME]
        ))
    
    # Final JSON renderer
    processors.append(structlog.processors.JSONRenderer())
    
    # Configure structlog
    structlog.configure(
        processors=processors,
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )


def get_scribe_logger(name: str, **initial_context) -> structlog.stdlib.BoundLogger:
    """
    Get a configured logger with optional initial context.
    
    Args:
        name: Logger name (typically __name__)
        **initial_context: Initial context to bind to the logger
        
    Returns:
        Configured structlog logger with bound context
    """
    logger = structlog.get_logger(name)
    
    if initial_context:
        logger = logger.bind(**initial_context)
    
    return logger


def add_global_context(**context) -> None:
    """
    Add global context that will be included in all log messages.
    
    Args:
        **context: Key-value pairs to add to global context
    """
    structlog.configure(
        processors=structlog.get_config()["processors"],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
        initial_values=context
    )


def create_request_logger(request_id: str, **context) -> structlog.stdlib.BoundLogger:
    """
    Create a logger with request-specific context.
    
    Useful for tracking events through the processing pipeline.
    
    Args:
        request_id: Unique identifier for this request/event
        **context: Additional context to bind
        
    Returns:
        Logger with bound request context
    """
    return get_scribe_logger("scribe.request").bind(
        request_id=request_id,
        **context
    )


def log_performance_metrics(
    logger: structlog.stdlib.BoundLogger,
    operation: str,
    duration_ms: float,
    **metrics
) -> None:
    """
    Log performance metrics in a standardized format.
    
    Args:
        logger: Structlog logger instance
        operation: Name of the operation being measured
        duration_ms: Duration in milliseconds
        **metrics: Additional metrics to log
    """
    logger.info(
        "performance_metrics",
        operation=operation,
        duration_ms=round(duration_ms, 2),
        **metrics
    )


def log_event_processing(
    logger: structlog.stdlib.BoundLogger,
    event_type: str,
    file_path: str,
    status: str,
    duration_ms: Optional[float] = None,
    **context
) -> None:
    """
    Log event processing in a standardized format.
    
    Args:
        logger: Structlog logger instance
        event_type: Type of file system event
        file_path: Path to the file that triggered the event
        status: Processing status (started, completed, failed)
        duration_ms: Processing duration in milliseconds
        **context: Additional context
    """
    log_data = {
        "event_processing": True,
        "event_type": event_type,
        "file_path": file_path,
        "status": status,
        **context
    }
    
    if duration_ms is not None:
        log_data["duration_ms"] = round(duration_ms, 2)
    
    if status == "failed":
        logger.error("Event processing failed", **log_data)
    elif status == "completed":
        logger.info("Event processing completed", **log_data)
    else:
        logger.info("Event processing status", **log_data)


# JSON Schema for log validation (useful for testing)
LOG_SCHEMA = {
    "type": "object",
    "required": ["timestamp", "level", "event"],
    "properties": {
        "timestamp": {"type": "string"},
        "level": {"type": "string", "enum": ["debug", "info", "warning", "error", "critical"]},
        "event": {"type": "string"},
        "logger": {"type": "string"},
        "request_id": {"type": "string"},
        "event_type": {"type": "string"},
        "file_path": {"type": "string"},
        "duration_ms": {"type": "number"},
        "status": {"type": "string"}
    },
    "additionalProperties": True
} 
```
  ```
  --- END OF FILE scribe/core\logging_config.py ---
  --- START OF FILE scribe/core\metrics.py ---
  ```
  ```py
from prometheus_client import Counter, Gauge, start_http_server
import logging

logger = logging.getLogger(__name__)

events_processed_counter = Counter('scribe_events_processed', 'Total events processed')
events_failed_counter = Counter('scribe_events_failed', 'Total events failed')
queue_size_gauge = Gauge('scribe_queue_size', 'Current event queue size')

def start_metrics_server(port: int = 8000):
    try:
        start_http_server(port)
        logger.info(f"Prometheus metrics server started on port {port}")
    except Exception as e:
        logger.error(f"Failed to start metrics server: {e}") 
```
  ```
  --- END OF FILE scribe/core\metrics.py ---
  --- START OF FILE scribe/core\minimal_core.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
HMA v2.1 Minimal Core Implementation

Implements the L2 Microkernel Core with strict adherence to HMA principles:
- Routing and lifecycle management only
- No business logic
- Technology-agnostic port-based boundaries
- Comprehensive telemetry and validation
"""

import asyncio
import time
import uuid
from typing import Dict, Any, Optional, List, Callable
from dataclasses import dataclass
from enum import Enum

from .hma_ports import (
    PluginExecutionPort, CredBrokerQueryPort, EventBusPort, 
    ObservabilityPort, ConfigurationPort, HealthCheckPort,
    PortRegistry
)
from .boundary_validator import BoundaryValidator, BoundaryType
from .hma_telemetry import HMATelemetry
from .logging_config import get_scribe_logger

logger = get_scribe_logger(__name__)

class CoreState(Enum):
    """Core operational states"""
    STOPPED = "stopped"
    STARTING = "starting" 
    RUNNING = "running"
    STOPPING = "stopping"
    ERROR = "error"

@dataclass
class PluginRegistration:
    """Plugin registration information"""
    plugin_id: str
    plugin_type: str  # L2-Orchestrator | L3-Capability
    version: str
    manifest: Dict[str, Any]
    status: str
    registered_at: float
    
class PluginLifecycleManager:
    """Manages plugin lifecycle operations"""
    
    def __init__(self, observability: ObservabilityPort):
        self.plugins: Dict[str, PluginRegistration] = {}
        self.observability = observability
    
    async def register_plugin(self, plugin_manifest: Dict[str, Any]) -> bool:
        """Register a plugin with manifest validation"""
        try:
            plugin_id = plugin_manifest["plugin_metadata"]["id"]
            plugin_type = plugin_manifest["plugin_metadata"]["type"]
            version = plugin_manifest["plugin_metadata"]["version"]
            
            # Validate manifest schema
            if not self._validate_manifest(plugin_manifest):
                logger.error("Plugin manifest validation failed", plugin_id=plugin_id)
                return False
            
            # Create registration
            registration = PluginRegistration(
                plugin_id=plugin_id,
                plugin_type=plugin_type,
                version=version,
                manifest=plugin_manifest,
                status="registered",
                registered_at=time.time()
            )
            
            self.plugins[plugin_id] = registration
            
            # Record telemetry
            self.observability.record_plugin_activity(plugin_id, True)
            self.observability.emit_log("INFO", f"Plugin registered: {plugin_id}", "core")
            
            logger.info("Plugin registered successfully",
                       plugin_id=plugin_id,
                       plugin_type=plugin_type,
                       version=version)
            
            return True
            
        except Exception as e:
            logger.error("Plugin registration failed", error=str(e))
            return False
    
    async def unregister_plugin(self, plugin_id: str) -> bool:
        """Unregister a plugin"""
        if plugin_id not in self.plugins:
            logger.warning("Attempted to unregister unknown plugin", plugin_id=plugin_id)
            return False
        
        try:
            del self.plugins[plugin_id]
            self.observability.record_plugin_activity(plugin_id, False)
            self.observability.emit_log("INFO", f"Plugin unregistered: {plugin_id}", "core")
            
            logger.info("Plugin unregistered successfully", plugin_id=plugin_id)
            return True
            
        except Exception as e:
            logger.error("Plugin unregistration failed", plugin_id=plugin_id, error=str(e))
            return False
    
    def get_plugin_info(self, plugin_id: str) -> Optional[PluginRegistration]:
        """Get plugin registration information"""
        return self.plugins.get(plugin_id)
    
    def list_plugins(self, plugin_type: Optional[str] = None) -> List[PluginRegistration]:
        """List registered plugins, optionally filtered by type"""
        if plugin_type:
            return [p for p in self.plugins.values() if p.plugin_type == plugin_type]
        return list(self.plugins.values())
    
    def _validate_manifest(self, manifest: Dict[str, Any]) -> bool:
        """Validate plugin manifest schema"""
        required_fields = [
            "manifest_version",
            "plugin_metadata",
            "hma_compliance"
        ]
        
        try:
            for field in required_fields:
                if field not in manifest:
                    logger.error("Missing required manifest field", field=field)
                    return False
            
            # Check HMA version compatibility
            hma_version = manifest.get("hma_compliance", {}).get("hma_version")
            if hma_version != "2.1":
                logger.error("Incompatible HMA version", 
                           required="2.1", 
                           found=hma_version)
                return False
            
            return True
            
        except Exception as e:
            logger.error("Manifest validation error", error=str(e))
            return False

class HMAMinimalCore:
    """
    HMA v2.1 compliant minimal core
    
    Responsibilities:
    1. Request routing to appropriate plugins
    2. Plugin lifecycle management  
    3. Control plane service access
    
    Non-responsibilities (delegated to L2 Orchestrators):
    - Business logic processing
    - Multi-plugin workflow coordination
    - Domain-specific decision making
    """
    
    def __init__(self, port_registry: PortRegistry, config: Dict[str, Any]):
        self.config = config
        self.state = CoreState.STOPPED
        
        # Get ports from registry
        self.plugin_execution_port = port_registry.get_port("plugin_execution")
        self.event_bus_port = port_registry.get_port("event_bus")
        self.observability_port = port_registry.get_port("observability")
        self.config_port = port_registry.get_port("configuration")
        self.health_port = port_registry.get_port("health_check")
        
        # Initialize components
        self.lifecycle_manager = PluginLifecycleManager(self.observability_port)
        self.boundary_validator = BoundaryValidator({}, self.observability_port)
        
        # Routing table maps request types to plugins
        self.routing_table: Dict[str, str] = {}
        
        # Core telemetry
        from .hma_telemetry import HMAComponent
        component = HMAComponent(
            component_type="L2-Core",
            component_id="scribe-minimal-core", 
            layer="L2"
        )
        self.telemetry = HMATelemetry(component)
        
        logger.info("HMA Minimal Core initialized")
    
    async def start(self) -> bool:
        """Start the core system"""
        if self.state != CoreState.STOPPED:
            logger.warning("Core already running or starting")
            return False
        
        try:
            self.state = CoreState.STARTING
            
            with self.telemetry.trace_boundary_operation(
                "core_startup", "l2_core", "system", "core"
            ):
                # Load configuration
                await self._load_configuration()
                
                # Initialize routing table
                await self._initialize_routing()
                
                # Register health check
                await self.health_port.register_health_check(
                    "core", self._health_check
                )
                
                self.state = CoreState.RUNNING
                
                self.observability_port.emit_log(
                    "INFO", "HMA Minimal Core started successfully", "core"
                )
                
                logger.info("HMA Minimal Core started successfully")
                return True
                
        except Exception as e:
            self.state = CoreState.ERROR
            logger.error("Core startup failed", error=str(e))
            self.observability_port.record_error("core_startup_failed", "core", str(e))
            return False
    
    async def stop(self) -> bool:
        """Stop the core system gracefully"""
        if self.state not in [CoreState.RUNNING, CoreState.ERROR]:
            return True
        
        try:
            self.state = CoreState.STOPPING
            
            with self.telemetry.trace_boundary_operation(
                "core_shutdown", "l2_core", "core", "system"
            ):
                # Unregister all plugins
                for plugin_id in list(self.lifecycle_manager.plugins.keys()):
                    await self.lifecycle_manager.unregister_plugin(plugin_id)
                
                self.state = CoreState.STOPPED
                
                logger.info("HMA Minimal Core stopped successfully")
                return True
                
        except Exception as e:
            logger.error("Core shutdown failed", error=str(e))
            return False
    
    async def route_request(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """
        Core responsibility: Route requests to appropriate plugins
        
        This is pure routing logic with no business processing.
        """
        if self.state != CoreState.RUNNING:
            raise RuntimeError("Core not running")
        
        with self.telemetry.trace_boundary_operation(
            "route_request", "l2_core", "external", "core"
        ) as span:
            # Validate request at boundary
            validation_result = self.boundary_validator.validate_l2_plugin_call(
                request, "core"
            )
            
            if not validation_result.valid:
                error_msg = f"Request validation failed: {validation_result.errors}"
                self.observability_port.record_error(
                    "boundary_validation_failed", "core", error_msg
                )
                raise ValueError(error_msg)
            
            # Determine target plugin (pure routing logic)
            plugin_id = self._resolve_plugin(request)
            
            if not plugin_id:
                raise ValueError(f"No plugin found for request type: {request.get('type')}")
            
            # Route to plugin via port (no business logic here)
            try:
                result = await self.plugin_execution_port.execute_plugin(
                    plugin_id, request
                )
                
                # Record successful routing
                self.observability_port.record_boundary_crossing(
                    "core", plugin_id, "route_request"
                )
                
                return result
                
            except Exception as e:
                self.observability_port.record_error(
                    "plugin_execution_failed", plugin_id, str(e)
                )
                raise
    
    def _resolve_plugin(self, request: Dict[str, Any]) -> Optional[str]:
        """
        Simple routing logic - no business processing
        
        Routes based on request type to registered plugin.
        Complex routing decisions are delegated to L2 Orchestrators.
        """
        request_type = request.get("type")
        if not request_type:
            return None
        
        # Check direct mapping first
        if request_type in self.routing_table:
            return self.routing_table[request_type]
        
        # For file events, route to appropriate handler
        if request_type in ["file_created", "file_modified", "file_deleted"]:
            # Check for specific file-based routing
            file_path = request.get("file_path", "")
            
            # Simple pattern matching (no complex business logic)
            if file_path.endswith((".md", ".txt")):
                return self.routing_table.get("file_processor")
            elif file_path.endswith((".cmd", ".trigger")):
                return self.routing_table.get("command_processor")
        
        # Default fallback to orchestrator for complex decisions
        return self.routing_table.get("default_orchestrator")
    
    async def register_plugin(self, plugin_manifest: Dict[str, Any]) -> bool:
        """Core responsibility: Plugin lifecycle management"""
        with self.telemetry.trace_boundary_operation(
            "register_plugin", "l2_core", "external", "core"
        ):
            success = await self.lifecycle_manager.register_plugin(plugin_manifest)
            
            if success:
                # Update routing table based on plugin capabilities
                await self._update_routing(plugin_manifest)
            
            return success
    
    async def unregister_plugin(self, plugin_id: str) -> bool:
        """Unregister plugin and update routing"""
        with self.telemetry.trace_boundary_operation(
            "unregister_plugin", "l2_core", "external", "core"
        ):
            # Remove from routing table
            self._remove_from_routing(plugin_id)
            
            # Unregister plugin
            return await self.lifecycle_manager.unregister_plugin(plugin_id)
    
    def get_plugin_status(self, plugin_id: str) -> Optional[str]:
        """Get plugin status"""
        plugin_info = self.lifecycle_manager.get_plugin_info(plugin_id)
        return plugin_info.status if plugin_info else None
    
    def get_core_status(self) -> Dict[str, Any]:
        """Get core system status"""
        plugins = self.lifecycle_manager.list_plugins()
        
        return {
            "state": self.state.value,
            "plugin_count": len(plugins),
            "plugins_by_type": {
                "L2-Orchestrator": len([p for p in plugins if p.plugin_type == "L2-Orchestrator"]),
                "L3-Capability": len([p for p in plugins if p.plugin_type == "L3-Capability"])
            },
            "routing_rules": len(self.routing_table),
            "uptime_seconds": time.time() - getattr(self, 'start_time', time.time())
        }
    
    async def _load_configuration(self) -> None:
        """Load core configuration"""
        # Load routing configuration
        routing_config = await self.config_port.get_config_value(
            "routing", "core", {}
        )
        
        # Initialize default routing
        self.routing_table.update(routing_config)
    
    async def _initialize_routing(self) -> None:
        """Initialize routing table with defaults"""
        default_routes = {
            "file_processor": "enhanced_frontmatter_action",
            "command_processor": "run_command_action", 
            "default_orchestrator": "file_processing_orchestrator"
        }
        
        self.routing_table.update(default_routes)
    
    async def _update_routing(self, plugin_manifest: Dict[str, Any]) -> None:
        """Update routing table when plugin is registered"""
        try:
            plugin_id = plugin_manifest["plugin_metadata"]["id"]
            
            # Check if plugin defines event patterns for routing
            interface_contracts = plugin_manifest.get("interface_contracts", {})
            action_interface = interface_contracts.get("action_interface", {})
            event_patterns = action_interface.get("event_patterns", [])
            
            # Add routing rules based on event patterns
            for pattern in event_patterns:
                event_type = pattern.get("event_type")
                if event_type:
                    self.routing_table[event_type] = plugin_id
                    
        except Exception as e:
            logger.warning("Failed to update routing for plugin", error=str(e))
    
    def _remove_from_routing(self, plugin_id: str) -> None:
        """Remove plugin from routing table"""
        routes_to_remove = [
            route_key for route_key, route_plugin 
            in self.routing_table.items() 
            if route_plugin == plugin_id
        ]
        
        for route_key in routes_to_remove:
            del self.routing_table[route_key]
    
    async def _health_check(self) -> Dict[str, Any]:
        """Core health check"""
        return {
            "status": "healthy" if self.state == CoreState.RUNNING else "unhealthy",
            "state": self.state.value,
            "plugin_count": len(self.lifecycle_manager.plugins),
            "routing_rules": len(self.routing_table)
        }
```
  ```
  --- END OF FILE scribe/core\minimal_core.py ---
  --- START OF FILE scribe/core\mtls.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Engine mTLS Implementation

Implements mutual TLS authentication for secure network communications
as required by HMA v2.2 mandatory Tier 1 technologies.
"""

import ssl
import socket
import threading
import urllib3
from pathlib import Path
from typing import Optional, Dict, Any, Union, List
import structlog
import requests

from .logging_config import get_scribe_logger

logger = get_scribe_logger(__name__)


class MTLSConfig:
    """Configuration for mutual TLS authentication."""
    
    def __init__(self,
                 cert_file: Union[str, Path],
                 key_file: Union[str, Path],
                 ca_file: Union[str, Path],
                 verify_mode: int = ssl.CERT_REQUIRED,
                 check_hostname: bool = True):
        """
        Initialize mTLS configuration.
        
        Args:
            cert_file: Path to client certificate file
            key_file: Path to client private key file  
            ca_file: Path to CA certificate file
            verify_mode: SSL verification mode
            check_hostname: Whether to verify hostname
        """
        self.cert_file = Path(cert_file)
        self.key_file = Path(key_file)
        self.ca_file = Path(ca_file)
        self.verify_mode = verify_mode
        self.check_hostname = check_hostname
        
        # Validate certificate files exist
        self._validate_certificate_files()
        
        logger.info("mTLS configuration initialized",
                   cert_file=str(self.cert_file),
                   ca_file=str(self.ca_file),
                   verify_mode=verify_mode,
                   check_hostname=check_hostname)
    
    def _validate_certificate_files(self):
        """Validate that all certificate files exist and are readable."""
        for file_path, file_type in [
            (self.cert_file, "client certificate"),
            (self.key_file, "client private key"), 
            (self.ca_file, "CA certificate")
        ]:
            if not file_path.exists():
                raise FileNotFoundError(f"{file_type} file not found: {file_path}")
            
            if not file_path.is_file():
                raise ValueError(f"{file_type} path is not a file: {file_path}")
            
            try:
                with open(file_path, 'r') as f:
                    f.read(1)
            except PermissionError:
                raise PermissionError(f"Cannot read {file_type} file: {file_path}")
    
    def create_ssl_context(self) -> ssl.SSLContext:
        """
        Create SSL context with mTLS configuration.
        
        Returns:
            Configured SSL context
        """
        try:
            # Create SSL context with TLS 1.2+ support
            context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)
            
            # Load CA certificate for server verification
            context.load_verify_locations(str(self.ca_file))
            
            # Load client certificate and key for client authentication
            context.load_cert_chain(str(self.cert_file), str(self.key_file))
            
            # Configure verification settings
            context.verify_mode = self.verify_mode
            context.check_hostname = self.check_hostname
            
            # Ensure strong cipher suites only
            context.set_ciphers('HIGH:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!SRP:!CAMELLIA')
            
            # Disable weak protocols
            context.options |= ssl.OP_NO_SSLv2
            context.options |= ssl.OP_NO_SSLv3
            context.options |= ssl.OP_NO_TLSv1
            context.options |= ssl.OP_NO_TLSv1_1
            
            logger.debug("SSL context created successfully")
            return context
            
        except Exception as e:
            logger.error("Failed to create SSL context",
                        error=str(e),
                        cert_file=str(self.cert_file),
                        ca_file=str(self.ca_file))
            raise


class MTLSHTTPAdapter(requests.adapters.HTTPAdapter):
    """Custom HTTP adapter with mTLS support for requests library."""
    
    def __init__(self, mtls_config: MTLSConfig, *args, **kwargs):
        """
        Initialize mTLS HTTP adapter.
        
        Args:
            mtls_config: mTLS configuration
        """
        self.mtls_config = mtls_config
        super().__init__(*args, **kwargs)
        
        logger.debug("mTLS HTTP adapter initialized")
    
    def init_poolmanager(self, *args, **kwargs):
        """Initialize urllib3 pool manager with mTLS SSL context."""
        ssl_context = self.mtls_config.create_ssl_context()
        kwargs['ssl_context'] = ssl_context
        
        return super().init_poolmanager(*args, **kwargs)


class MTLSSession:
    """HTTP session with mTLS support."""
    
    def __init__(self, mtls_config: MTLSConfig):
        """
        Initialize mTLS session.
        
        Args:
            mtls_config: mTLS configuration
        """
        self.mtls_config = mtls_config
        self.session = requests.Session()
        
        # Mount mTLS adapter for HTTPS URLs
        adapter = MTLSHTTPAdapter(mtls_config)
        self.session.mount('https://', adapter)
        
        logger.debug("mTLS session initialized")
    
    def get(self, url: str, **kwargs) -> requests.Response:
        """Make GET request with mTLS authentication."""
        return self._request('GET', url, **kwargs)
    
    def post(self, url: str, **kwargs) -> requests.Response:
        """Make POST request with mTLS authentication."""
        return self._request('POST', url, **kwargs)
    
    def put(self, url: str, **kwargs) -> requests.Response:
        """Make PUT request with mTLS authentication."""
        return self._request('PUT', url, **kwargs)
    
    def delete(self, url: str, **kwargs) -> requests.Response:
        """Make DELETE request with mTLS authentication."""
        return self._request('DELETE', url, **kwargs)
    
    def _request(self, method: str, url: str, **kwargs) -> requests.Response:
        """
        Make HTTP request with mTLS authentication and error handling.
        
        Args:
            method: HTTP method
            url: Request URL
            **kwargs: Additional request parameters
            
        Returns:
            HTTP response
        """
        try:
            logger.debug("Making mTLS request",
                        method=method,
                        url=url)
            
            response = self.session.request(method, url, **kwargs)
            
            logger.debug("mTLS request completed",
                        method=method,
                        url=url,
                        status_code=response.status_code)
            
            return response
            
        except ssl.SSLError as e:
            logger.error("SSL error during mTLS request",
                        method=method,
                        url=url,
                        error=str(e))
            raise
        
        except requests.exceptions.RequestException as e:
            logger.error("Request error during mTLS request",
                        method=method,
                        url=url,
                        error=str(e))
            raise
        
        except Exception as e:
            logger.error("Unexpected error during mTLS request",
                        method=method,
                        url=url,
                        error=str(e),
                        error_type=type(e).__name__)
            raise
    
    def close(self):
        """Close the session and clean up resources."""
        self.session.close()
        logger.debug("mTLS session closed")


class MTLSServer:
    """Simple mTLS server for testing and local services."""
    
    def __init__(self,
                 host: str,
                 port: int,
                 mtls_config: MTLSConfig,
                 request_handler=None):
        """
        Initialize mTLS server.
        
        Args:
            host: Server host
            port: Server port
            mtls_config: mTLS configuration
            request_handler: Custom request handler function
        """
        self.host = host
        self.port = port
        self.mtls_config = mtls_config
        self.request_handler = request_handler or self._default_handler
        
        self._server_socket = None
        self._running = False
        self._server_thread = None
        
        logger.info("mTLS server initialized",
                   host=host,
                   port=port)
    
    def _default_handler(self, client_socket: ssl.SSLSocket, address: tuple):
        """Default request handler that sends a simple HTTP response."""
        try:
            # Read request
            request = client_socket.recv(4096).decode('utf-8')
            
            # Send simple HTTP response
            response = (
                "HTTP/1.1 200 OK\r\n"
                "Content-Type: text/plain\r\n"
                "Content-Length: 13\r\n"
                "\r\n"
                "mTLS server OK"
            )
            
            client_socket.send(response.encode('utf-8'))
            
            logger.debug("Handled mTLS request",
                        client_address=address,
                        request_preview=request.split('\n')[0] if request else "")
            
        except Exception as e:
            logger.error("Error handling mTLS request",
                        client_address=address,
                        error=str(e))
        finally:
            client_socket.close()
    
    def start(self):
        """Start the mTLS server."""
        if self._running:
            return
        
        try:
            # Create SSL context
            ssl_context = self.mtls_config.create_ssl_context()
            
            # Create server socket
            self._server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self._server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            self._server_socket.bind((self.host, self.port))
            self._server_socket.listen(5)
            
            # Wrap with SSL
            self._server_socket = ssl_context.wrap_socket(
                self._server_socket,
                server_side=True
            )
            
            self._running = True
            
            # Start server thread
            self._server_thread = threading.Thread(
                target=self._server_loop,
                name=f"MTLSServer-{self.host}:{self.port}",
                daemon=True
            )
            self._server_thread.start()
            
            logger.info("mTLS server started",
                       host=self.host,
                       port=self.port)
            
        except Exception as e:
            logger.error("Failed to start mTLS server",
                        host=self.host,
                        port=self.port,
                        error=str(e))
            self._running = False
            raise
    
    def _server_loop(self):
        """Main server loop."""
        try:
            while self._running:
                try:
                    # Accept client connection
                    client_socket, address = self._server_socket.accept()
                    
                    logger.debug("Accepted mTLS connection",
                               client_address=address)
                    
                    # Handle request in separate thread
                    client_thread = threading.Thread(
                        target=self.request_handler,
                        args=(client_socket, address),
                        daemon=True
                    )
                    client_thread.start()
                    
                except Exception as e:
                    if self._running:
                        logger.error("Error accepting mTLS connection",
                                    error=str(e))
                        
        except Exception as e:
            logger.error("mTLS server loop error",
                        error=str(e))
        finally:
            self._cleanup()
    
    def stop(self):
        """Stop the mTLS server."""
        if not self._running:
            return
        
        self._running = False
        
        try:
            if self._server_socket:
                self._server_socket.close()
            
            if self._server_thread and self._server_thread.is_alive():
                self._server_thread.join(timeout=5.0)
            
            logger.info("mTLS server stopped",
                       host=self.host,
                       port=self.port)
            
        except Exception as e:
            logger.error("Error stopping mTLS server",
                        error=str(e))
    
    def _cleanup(self):
        """Clean up server resources."""
        try:
            if self._server_socket:
                self._server_socket.close()
                self._server_socket = None
        except Exception:
            pass


class MTLSManager:
    """Manager for mTLS configurations and sessions."""
    
    def __init__(self):
        """Initialize mTLS manager."""
        self._configurations: Dict[str, MTLSConfig] = {}
        self._sessions: Dict[str, MTLSSession] = {}
        self._lock = threading.RLock()
        
        logger.info("mTLS manager initialized")
    
    def add_configuration(self, name: str, mtls_config: MTLSConfig):
        """
        Add a named mTLS configuration.
        
        Args:
            name: Configuration name
            mtls_config: mTLS configuration
        """
        with self._lock:
            self._configurations[name] = mtls_config
            logger.debug("Added mTLS configuration", name=name)
    
    def get_session(self, config_name: str) -> Optional[MTLSSession]:
        """
        Get or create mTLS session for configuration.
        
        Args:
            config_name: Name of the configuration
            
        Returns:
            mTLS session or None if configuration not found
        """
        with self._lock:
            if config_name not in self._configurations:
                logger.warning("mTLS configuration not found", config_name=config_name)
                return None
            
            if config_name not in self._sessions:
                config = self._configurations[config_name]
                self._sessions[config_name] = MTLSSession(config)
                logger.debug("Created new mTLS session", config_name=config_name)
            
            return self._sessions[config_name]
    
    def close_session(self, config_name: str):
        """
        Close mTLS session for configuration.
        
        Args:
            config_name: Name of the configuration
        """
        with self._lock:
            if config_name in self._sessions:
                self._sessions[config_name].close()
                del self._sessions[config_name]
                logger.debug("Closed mTLS session", config_name=config_name)
    
    def close_all_sessions(self):
        """Close all mTLS sessions."""
        with self._lock:
            for config_name in list(self._sessions.keys()):
                self.close_session(config_name)
            logger.info("Closed all mTLS sessions")
    
    def list_configurations(self) -> List[str]:
        """Get list of configuration names."""
        with self._lock:
            return list(self._configurations.keys())


# Global mTLS manager instance
_mtls_manager: Optional[MTLSManager] = None
_mtls_lock = threading.RLock()


def get_mtls_manager() -> MTLSManager:
    """Get or create global mTLS manager."""
    global _mtls_manager
    
    with _mtls_lock:
        if _mtls_manager is None:
            _mtls_manager = MTLSManager()
        
        return _mtls_manager


def create_mtls_session(cert_file: Union[str, Path],
                       key_file: Union[str, Path],
                       ca_file: Union[str, Path]) -> MTLSSession:
    """
    Create a new mTLS session with the provided certificates.
    
    Args:
        cert_file: Path to client certificate file
        key_file: Path to client private key file
        ca_file: Path to CA certificate file
        
    Returns:
        Configured mTLS session
    """
    config = MTLSConfig(cert_file, key_file, ca_file)
    return MTLSSession(config)
```
  ```
  --- END OF FILE scribe/core\mtls.py ---
  --- START OF FILE scribe/core\plugin_loader.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Plugin Loader

Handles dynamic discovery and loading of action plugins.
Implements secure plugin loading with error handling and validation.
"""

import importlib.util
import inspect
import sys
import json
from pathlib import Path
from typing import Dict, List, Type, Optional, Any
import structlog
import jsonschema

from .logging_config import get_scribe_logger
from tools.scribe.actions.base import BaseAction
from .config_manager import ConfigManager
from .security_manager import SecurityManager
from .port_adapters import ScribePluginContextAdapter


logger = get_scribe_logger(__name__)


class PluginLoadError(Exception):
    """Exception raised when a plugin fails to load."""
    
    def __init__(self, plugin_path: str, message: str, original_error: Optional[Exception] = None):
        """
        Initialize the exception.
        
        Args:
            plugin_path: Path to the plugin that failed to load
            message: Error message
            original_error: The original exception that caused this error
        """
        self.plugin_path = plugin_path
        self.original_error = original_error
        
        if original_error:
            super().__init__(f"Failed to load plugin '{plugin_path}': {message} (caused by: {original_error})")
        else:
            super().__init__(f"Failed to load plugin '{plugin_path}': {message}")


class PluginInfo:
    """Information about a loaded plugin with HMA v2.2 manifest support."""
    
    def __init__(self, 
                 action_class: Type[BaseAction], 
                 module_path: str, 
                 action_type: str,
                 manifest: Optional[Dict[str, Any]] = None):
        """
        Initialize plugin information.
        
        Args:
            action_class: The action class
            module_path: Path to the module file
            action_type: The action type identifier
            manifest: Plugin manifest data (HMA v2.2)
        """
        self.action_class = action_class
        self.module_path = module_path
        self.action_type = action_type
        self.class_name = action_class.__name__
        self.module_name = action_class.__module__
        self.manifest = manifest or {}
    
    def create_instance(self, params: Dict[str, Any],
                        port_registry,
                        execution_context: Optional[Dict[str, Any]] = None) -> BaseAction:
        """
        Create an instance of the action plugin with HMA v2.2 port-based access.
        
        Args:
            params: Action-specific parameters from the rule.
            port_registry: The port registry for accessing core functionality.
            execution_context: Plugin execution context.

        Returns:
            New instance of the action plugin.
        """
        # Create plugin context adapter
        plugin_context = ScribePluginContextAdapter(
            plugin_id=self.action_type,
            port_registry=port_registry,
            execution_context=execution_context or {}
        )
        
        return self.action_class(
            action_type=self.action_type,
            params=params,
            plugin_context=plugin_context
        )
    
    def __str__(self) -> str:
        return f"PluginInfo(type='{self.action_type}', class='{self.class_name}')"
    
    def __repr__(self) -> str:
        return f"PluginInfo(action_type='{self.action_type}', class_name='{self.class_name}', module_path='{self.module_path}')"


class PluginLoader:
    """
    Loads and manages action plugins dynamically.
    
    Discovers Python files in the actions directory, loads them as modules,
    and finds classes that inherit from BaseAction.
    """
    
    def __init__(self, plugin_directories: List[str] = None, load_order: List[str] = None):
        """
        Initialize the plugin loader.
        
        Args:
            plugin_directories: List of directories containing plugin files (relative to scribe root)
            load_order: Order in which to load plugin directories (optional)
        """
        # Determine the absolute path to the plugins directories
        scribe_root = Path(__file__).parent.parent
        
        if plugin_directories is None:
            plugin_directories = ["actions"]
        
        # Store both relative and absolute paths
        self.relative_plugin_directories = plugin_directories
        self.plugin_directories = []
        self.directory_map = {}  # Maps relative to absolute paths
        
        for plugin_dir in plugin_directories:
            abs_plugin_dir = scribe_root / plugin_dir
            self.plugin_directories.append(abs_plugin_dir)
            self.directory_map[plugin_dir] = abs_plugin_dir
        
        # Set load order
        self.load_order = load_order or plugin_directories
        
        # Plugin registry with directory tracking
        self._plugins: Dict[str, PluginInfo] = {}
        self._loaded_modules: Dict[str, Any] = {}
        self._plugin_directory_map: Dict[str, str] = {}  # Maps plugin name to directory
        
        # Hot-reload support (Phase 2)
        self._file_watchers = {}
        self._auto_reload = False
        
        logger.info("PluginLoader initialized",
                   plugin_directories=[str(d) for d in self.plugin_directories],
                   load_order=self.load_order)
    
    def discover_plugins(self) -> List[str]:
        """
        Discover all Python files in the plugins directory.
        
        Returns:
            List of plugin file paths
        """
        plugin_files = []
        
        for plugin_dir in self.plugin_directories:
            if not plugin_dir.exists():
                logger.warning("Plugins directory does not exist",
                              directory=str(plugin_dir))
                continue
            
            # Find all .py files except __init__.py and base.py
            for file_path in plugin_dir.glob("*.py"):
                if file_path.name in ["__init__.py", "base.py"]:
                    continue
                
                plugin_files.append(str(file_path))
                logger.debug("Discovered plugin file", file_path=str(file_path))
        
        logger.info("Plugin discovery complete",
                   total_files=len(plugin_files),
                   files=plugin_files)
        
        return plugin_files
    
    def load_plugin_module(self, file_path: str) -> Any:
        """
        Load a Python file as a module.
        
        Args:
            file_path: Path to the Python file
            
        Returns:
            The loaded module
            
        Raises:
            PluginLoadError: If the module fails to load
        """
        try:
            file_path_obj = Path(file_path)
            module_name_str = None
            package_name_str = None

            # Determine module and package name relative to a sys.path entry
            # This makes relative imports (..) work correctly.
            abs_file_path = file_path_obj.resolve()

            # Find the longest sys.path entry that is an ancestor of this file
            longest_ancestor_path = None
            for p_str in sys.path:
                p_path = Path(p_str).resolve()
                if abs_file_path.is_relative_to(p_path): # Requires Python 3.9+
                    if longest_ancestor_path is None or len(str(p_path)) > len(str(longest_ancestor_path)):
                        longest_ancestor_path = p_path
            
            if longest_ancestor_path:
                relative_module_path = abs_file_path.relative_to(longest_ancestor_path)
                # Construct module name, e.g., tools.scribe.actions.my_plugin
                module_name_str = ".".join(relative_module_path.parts[:-1] + (file_path_obj.stem,))
                # Construct package name, e.g., tools.scribe.actions
                package_name_str = ".".join(relative_module_path.parts[:-1])
            else:
                # Fallback if no ancestor found in sys.path (should be rare in controlled env)
                module_name_str = f"scribe_plugin_{file_path_obj.stem}"
                package_name_str = "" # Indicates top-level, relative imports will fail
                logger.warning(f"Plugin path {file_path} not found under any sys.path entry. Using fallback module name: {module_name_str}. Relative imports within plugin may fail.")

            # This was the line with the typo (module_name instead of module_name_str in the f-string).
            # It should be module_name_str as corrected in the previous diff application for this file.
            # The error output shows the log still has "module_name" if it's a fallback.
            # Let's ensure all references here use module_name_str for logging the name.
            logger.debug(f"Attempting to load plugin {file_path} as module '{module_name_str}' with package '{package_name_str}'")

            spec = importlib.util.spec_from_file_location(module_name_str, str(file_path))
            if spec is None:
                raise PluginLoadError(file_path, f"Could not create module spec for '{module_name_str}'")
            
            if spec.loader is None:
                raise PluginLoadError(file_path, f"Module spec for '{module_name_str}' has no loader")
            
            module = importlib.util.module_from_spec(spec)
            
            # Set __package__ to allow relative imports from within the plugin
            # This needs to happen BEFORE exec_module
            if package_name_str:
                 module.__package__ = package_name_str
            
            # Add to sys.modules BEFORE exec_module to handle circular dependencies or re-imports within plugin
            sys.modules[module_name_str] = module

            spec.loader.exec_module(module) # Execute the module to make its content available
            
            logger.debug("Plugin module loaded successfully",
                        file_path=file_path,
                        module_name=module_name_str) # Use module_name_str
            
            return module
            
        except Exception as e:
            raise PluginLoadError(file_path, "Failed to load module", e)
    
    def extract_action_classes(self, module: Any, file_path: str) -> List[Type[BaseAction]]:
        """
        Extract action classes from a loaded module.
        
        Args:
            module: The loaded module
            file_path: Path to the module file (for error reporting)
            
        Returns:
            List of action classes found in the module
        """
        action_classes = []
        
        try:
            # Get all members of the module
            for name, obj in inspect.getmembers(module):
                # Check if it's a class
                if not inspect.isclass(obj):
                    continue
                
                # Skip if it's the BaseAction class itself
                if obj is BaseAction:
                    continue
                
                # Check if it's a subclass of BaseAction
                if issubclass(obj, BaseAction) and obj != BaseAction:
                    action_classes.append(obj)
                    logger.debug("Found action class",
                                file_path=file_path,
                                class_name=name)
            
            logger.debug("Action class extraction complete",
                        file_path=file_path,
                        classes_found=len(action_classes))
            
        except Exception as e:
            logger.error("Error extracting action classes",
                        file_path=file_path,
                        error=str(e),
                        exc_info=True)
        
        return action_classes
    
    def determine_action_type(self, action_class: Type[BaseAction], file_path: str) -> str:
        """
        Determine the action type for a plugin class.
        
        This method tries several strategies to determine the action type:
        1. Check if the class has an ACTION_TYPE class attribute
        2. Use the class name converted to snake_case
        3. Use the filename without extension
        
        Args:
            action_class: The action class
            file_path: Path to the module file
            
        Returns:
            The action type string
        """
        # Strategy 1: Check for ACTION_TYPE class attribute
        if hasattr(action_class, 'ACTION_TYPE'):
            action_type = action_class.ACTION_TYPE
            if isinstance(action_type, str) and action_type.strip():
                logger.debug("Using ACTION_TYPE attribute",
                           class_name=action_class.__name__,
                           action_type=action_type)
                return action_type.strip()
        
        # Strategy 2: Convert class name to snake_case
        class_name = action_class.__name__
        if class_name.endswith('Action'):
            class_name = class_name[:-6]  # Remove 'Action' suffix
        
        # Convert CamelCase to snake_case
        import re
        snake_case = re.sub(r'(?<!^)(?=[A-Z])', '_', class_name).lower()
        
        if snake_case:
            logger.debug("Using snake_case class name",
                        class_name=action_class.__name__,
                        action_type=snake_case)
            return snake_case
        
        # Strategy 3: Use filename
        file_name = Path(file_path).stem
        logger.debug("Using filename as action type",
                    class_name=action_class.__name__,
                    action_type=file_name)
        return file_name
    
    def load_plugin_manifest(self, plugin_directory: Path) -> Optional[Dict[str, Any]]:
        """
        Load and validate plugin manifest file.
        
        Args:
            plugin_directory: Directory containing manifest.json
            
        Returns:
            Validated manifest data or None if not found/invalid
        """
        manifest_path = plugin_directory / "manifest.json"
        
        if not manifest_path.exists():
            logger.debug("No manifest found for plugin", plugin_directory=str(plugin_directory))
            return None
        
        try:
            with open(manifest_path, 'r', encoding='utf-8') as f:
                manifest_data = json.load(f)
            
            # HMA v2.2 Mandatory Validation
            manifest_version = manifest_data.get("manifest_version")
            hma_version = manifest_data.get("hma_compliance", {}).get("hma_version")
            
            # Enforce HMA v2.2 compliance
            if manifest_version != "2.2":
                raise jsonschema.ValidationError(
                    f"Plugin manifest version must be '2.2', found '{manifest_version}'"
                )
            
            if hma_version != "2.2":
                raise jsonschema.ValidationError(
                    f"HMA compliance version must be '2.2', found '{hma_version}'"
                )
            
            # Validate mandatory HMA v2.2 fields
            required_hma_fields = ["hma_version", "tier_classification", "boundary_interfaces"]
            hma_compliance = manifest_data.get("hma_compliance", {})
            
            for field in required_hma_fields:
                if field not in hma_compliance:
                    raise jsonschema.ValidationError(
                        f"Missing mandatory HMA v2.2 field: hma_compliance.{field}"
                    )
            
            # Load schema for validation
            schema_path = Path(__file__).parent.parent / "schemas" / "plugin_manifest.schema.json"
            if schema_path.exists():
                with open(schema_path, 'r', encoding='utf-8') as f:
                    schema = json.load(f)
                
                # Validate manifest against schema
                jsonschema.validate(manifest_data, schema)
                logger.debug("Plugin manifest HMA v2.2 validation passed", 
                           plugin_directory=str(plugin_directory),
                           manifest_version=manifest_version,
                           hma_version=hma_version)
            else:
                logger.warning("Plugin manifest schema not found, skipping schema validation",
                              schema_path=str(schema_path))
            
            return manifest_data
            
        except json.JSONDecodeError as e:
            logger.error("Invalid JSON in plugin manifest",
                        manifest_path=str(manifest_path),
                        error=str(e))
            return None
        except jsonschema.ValidationError as e:
            logger.error("Plugin manifest validation failed",
                        manifest_path=str(manifest_path),
                        error=str(e))
            return None
        except Exception as e:
            logger.error("Error loading plugin manifest",
                        manifest_path=str(manifest_path),
                        error=str(e))
            return None
    
    def load_plugin(self, file_path: str) -> List[PluginInfo]:
        """
        Load a single plugin file and extract action classes with manifest support.
        
        Args:
            file_path: Path to the plugin file
            
        Returns:
            List of PluginInfo objects for loaded actions
            
        Raises:
            PluginLoadError: If the plugin fails to load
        """
        plugin_infos = []
        
        try:
            # Security validation
            if not self.validate_plugin_security(file_path):
                raise PluginLoadError(file_path, "Plugin failed security validation")
            
            # Check for manifest in plugin directory
            plugin_file_path = Path(file_path)
            plugin_directory = plugin_file_path.parent / plugin_file_path.stem
            manifest = None
            
            if plugin_directory.exists() and plugin_directory.is_dir():
                manifest = self.load_plugin_manifest(plugin_directory)
                if manifest:
                    logger.info("HMA v2.2 manifest loaded for plugin",
                               plugin_path=file_path,
                               manifest_version=manifest.get('manifest_version'),
                               hma_version=manifest.get('hma_compliance', {}).get('hma_version'))
            
            # Determine which directory this plugin belongs to
            plugin_file_path = Path(file_path)
            plugin_directory = None
            
            for rel_dir, abs_dir in self.directory_map.items():
                if plugin_file_path.is_relative_to(abs_dir):
                    plugin_directory = rel_dir
                    break
            
            if plugin_directory is None:
                logger.warning("Plugin file not in any configured directory", 
                              file_path=file_path)
                plugin_directory = "unknown"
            
            # Load the module
            module = self.load_plugin_module(file_path)
            self._loaded_modules[file_path] = module
            
            # Extract action classes
            action_classes = self.extract_action_classes(module, file_path)
            
            if not action_classes:
                logger.warning("No action classes found in plugin", file_path=file_path)
                return plugin_infos
            
            # Create PluginInfo objects for each action class
            for action_class in action_classes:
                action_type = self.determine_action_type(action_class, file_path)
                
                # Check for name conflicts
                if action_type in self._plugins:
                    existing_plugin = self._plugins[action_type]
                    logger.warning("Plugin action type conflict",
                                  action_type=action_type,
                                  new_plugin=file_path,
                                  existing_plugin=existing_plugin.module_path)
                    # Skip this plugin to avoid conflicts
                    continue
                
                plugin_info = PluginInfo(action_class, file_path, action_type, manifest)
                plugin_infos.append(plugin_info)
                
                # Register the plugin
                self._plugins[action_type] = plugin_info
                self._plugin_directory_map[action_type] = plugin_directory
                
                # Log HMA compliance status
                hma_compliance = "HMA v2.2 compliant" if manifest else "Legacy (no manifest)"
                logger.info("Plugin action loaded successfully",
                           action_type=action_type,
                           action_class=action_class.__name__,
                           plugin_file=file_path,
                           plugin_directory=plugin_directory,
                           hma_compliance=hma_compliance)
            
            return plugin_infos
            
        except PluginLoadError:
            raise
        except Exception as e:
            raise PluginLoadError(file_path, f"Unexpected error during plugin loading", e)
    
    def load_all_plugins(self) -> Dict[str, PluginInfo]:
        """
        Discover and load all plugins with dependency resolution.
        
        Returns:
            Dictionary mapping action types to PluginInfo objects
        """
        logger.info("Starting plugin loading process")
        
        # Clear existing plugins
        self._plugins.clear()
        self._loaded_modules.clear()
        self._plugin_directory_map.clear()
        
        # Use dependency-resolved load order
        plugin_files = self.resolve_plugin_load_order()
        
        if not plugin_files:
            logger.warning("No plugin files discovered")
            return self._plugins
        
        loaded_count = 0
        failed_count = 0
        
        for plugin_file in plugin_files:
            try:
                plugin_infos = self.load_plugin(plugin_file)
                
                for plugin_info in plugin_infos:
                    self._plugins[plugin_info.action_type] = plugin_info
                    loaded_count += 1
                
            except PluginLoadError as e:
                logger.error("Plugin load error",
                            plugin_file=plugin_file,
                            error=str(e))
                failed_count += 1
            except Exception as e:
                logger.error("Unexpected error loading plugin",
                            plugin_file=plugin_file,
                            error=str(e),
                            exc_info=True)
                failed_count += 1
        
        logger.info("Plugin loading completed",
                   total_discovered=len(plugin_files),
                   successfully_loaded=loaded_count,
                   failed_to_load=failed_count,
                   action_types=list(self._plugins.keys()))
        
        return self._plugins
    
    def get_plugin(self, action_type: str) -> Optional[PluginInfo]:
        """
        Get plugin information for a specific action type.
        
        Args:
            action_type: The action type to look up
            
        Returns:
            PluginInfo if found, None otherwise
        """
        return self._plugins.get(action_type)
    
    def get_all_plugins(self) -> Dict[str, PluginInfo]:
        """
        Get all loaded plugins.
        
        Returns:
            Dictionary mapping action types to PluginInfo objects
        """
        return self._plugins.copy()
    
    def create_action_instance(self, action_type: str) -> Optional[BaseAction]:
        """
        Create an instance of an action plugin.
        
        Args:
            action_type: The action type to instantiate.
            params: Action-specific parameters from the rule.
            config_manager: The ConfigManager instance.
            security_manager: The SecurityManager instance.
            
        Returns:
            Action instance if found, None otherwise.
        """
        plugin_info = self.get_plugin(action_type)
        if plugin_info is None:
            logger.warning("Action type not found for instantiation", action_type=action_type)
            return None
        
        try:
            instance = plugin_info.create_instance(params, config_manager, security_manager)
            logger.debug("Action instance created with dependencies",
                        action_type=action_type,
                        class_name=plugin_info.class_name,
                        params=params)
            return instance
        except Exception as e:
            logger.error("Error creating action instance with dependencies",
                        action_type=action_type,
                        class_name=plugin_info.class_name,
                        error=str(e),
                        exc_info=True)
            return None
    
    def reload_plugins(self) -> Dict[str, PluginInfo]:
        """
        Reload all plugins from the plugins directory.
        
        Returns:
            Dictionary mapping action types to PluginInfo objects
        """
        logger.info("Reloading all plugins")
        
        # Clear module cache for reloaded modules
        for file_path in self._loaded_modules:
            module_name = f"scribe_plugin_{Path(file_path).stem}"
            if module_name in sys.modules:
                del sys.modules[module_name]
        
        return self.load_all_plugins()
    
    def get_plugin_stats(self) -> Dict[str, Any]:
        """Get plugin loader statistics."""
        return {
            'total_plugins': len(self._plugins),
            'loaded_modules': len(self._loaded_modules),
            'action_types': list(self._plugins.keys()),
            'plugin_files': len(self._loaded_modules),
            'plugins_directory': [str(d) for d in self.plugin_directories]
        }
    
    def enable_hot_reload(self) -> None:
        """Enable hot-reloading of plugin files."""
        try:
            from watchdog.observers import Observer
            from watchdog.events import FileSystemEventHandler
            
            class PluginFileHandler(FileSystemEventHandler):
                def __init__(self, plugin_loader):
                    self.plugin_loader = plugin_loader
                
                def on_modified(self, event):
                    if event.is_directory:
                        return
                    
                    if event.src_path.endswith('.py'):
                        logger.info("Plugin file changed, reloading", 
                                   file_path=event.src_path)
                        self.plugin_loader.reload_plugins()
            
            self._auto_reload = True
            observer = Observer()
            
            for plugin_dir in self.plugin_directories:
                if plugin_dir.exists():
                    handler = PluginFileHandler(self)
                    observer.schedule(handler, str(plugin_dir), recursive=False)
                    self._file_watchers[str(plugin_dir)] = (observer, handler)
                    logger.info("Hot-reload enabled for plugin directory", 
                               directory=str(plugin_dir))
            
            observer.start()
            logger.info("Plugin hot-reloading enabled")
            
        except ImportError:
            logger.warning("Watchdog not available, hot-reloading disabled")
        except Exception as e:
            logger.error("Failed to enable hot-reloading", error=str(e))
    
    def disable_hot_reload(self) -> None:
        """Disable hot-reloading of plugin files."""
        self._auto_reload = False
        for directory, (observer, handler) in self._file_watchers.items():
            try:
                observer.stop()
                observer.join(timeout=5.0)
                logger.info("Hot-reload disabled for plugin directory", 
                           directory=directory)
            except Exception as e:
                logger.error("Error stopping plugin file watcher", 
                            directory=directory, error=str(e))
        
        self._file_watchers.clear()
        logger.info("Plugin hot-reloading disabled")
    
    def validate_plugin_security(self, plugin_path: str) -> bool:
        """
        Validate plugin security before loading.
        
        Args:
            plugin_path: Path to the plugin file
            
        Returns:
            True if plugin passes security validation
        """
        try:
            # Read plugin file content
            with open(plugin_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Security checks
            dangerous_imports = [
                'subprocess', 'os.system', 'eval', 'exec', '__import__',
                'importlib.import_module', 'open', 'file'
            ]
            
            dangerous_functions = [
                'eval(', 'exec(', 'compile(', '__import__(',
                'getattr(', 'setattr(', 'delattr(', 'globals(', 'locals('
            ]
            
            # Check for dangerous imports
            for dangerous_import in dangerous_imports:
                if dangerous_import in content:
                    logger.warning("Plugin contains potentially dangerous import",
                                  plugin_path=plugin_path,
                                  dangerous_pattern=dangerous_import)
                    return False
            
            # Check for dangerous function calls
            for dangerous_func in dangerous_functions:
                if dangerous_func in content:
                    logger.warning("Plugin contains potentially dangerous function",
                                  plugin_path=plugin_path,
                                  dangerous_pattern=dangerous_func)
                    return False
            
            # Check file permissions (should not be world-writable)
            plugin_file = Path(plugin_path)
            if plugin_file.stat().st_mode & 0o002:  # World-writable
                logger.warning("Plugin file is world-writable",
                              plugin_path=plugin_path)
                return False
            
            logger.debug("Plugin security validation passed",
                        plugin_path=plugin_path)
            return True
            
        except Exception as e:
            logger.error("Plugin security validation failed",
                        plugin_path=plugin_path,
                        error=str(e))
            return False
    
    def add_plugin_directory(self, directory: str, position: int = -1) -> bool:
        """
        Add a new plugin directory at runtime.
        
        Args:
            directory: Directory path (relative to scribe root)
            position: Position in load order (-1 for end)
            
        Returns:
            True if directory was added successfully
        """
        try:
            scribe_root = Path(__file__).parent.parent
            abs_plugin_dir = scribe_root / directory
            
            if directory in self.relative_plugin_directories:
                logger.warning("Plugin directory already exists", directory=directory)
                return False
            
            if not abs_plugin_dir.exists():
                logger.error("Plugin directory does not exist", directory=str(abs_plugin_dir))
                return False
            
            # Add to lists
            if position == -1:
                self.relative_plugin_directories.append(directory)
                self.plugin_directories.append(abs_plugin_dir)
                self.load_order.append(directory)
            else:
                self.relative_plugin_directories.insert(position, directory)
                self.plugin_directories.insert(position, abs_plugin_dir)
                self.load_order.insert(position, directory)
            
            # Update directory map
            self.directory_map[directory] = abs_plugin_dir
            
            # Enable hot-reload for new directory if it's enabled
            if self._auto_reload and directory not in self._file_watchers:
                # Add hot-reload watcher for this directory
                pass  # Implementation would go here
            
            logger.info("Plugin directory added", 
                       directory=directory, 
                       absolute_path=str(abs_plugin_dir))
            return True
            
        except Exception as e:
            logger.error("Failed to add plugin directory",
                        directory=directory,
                        error=str(e))
            return False
    
    def remove_plugin_directory(self, directory: str) -> bool:
        """
        Remove a plugin directory at runtime.
        
        Args:
            directory: Directory path to remove
            
        Returns:
            True if directory was removed successfully
        """
        try:
            if directory not in self.relative_plugin_directories:
                logger.warning("Plugin directory not found", directory=directory)
                return False
            
            # Remove from all lists
            index = self.relative_plugin_directories.index(directory)
            self.relative_plugin_directories.remove(directory)
            self.plugin_directories.pop(index)
            
            if directory in self.load_order:
                self.load_order.remove(directory)
            
            # Remove from directory map
            if directory in self.directory_map:
                del self.directory_map[directory]
            
            # Remove plugins loaded from this directory
            plugins_to_remove = []
            for plugin_name, plugin_dir in self._plugin_directory_map.items():
                if plugin_dir == directory:
                    plugins_to_remove.append(plugin_name)
            
            for plugin_name in plugins_to_remove:
                if plugin_name in self._plugins:
                    del self._plugins[plugin_name]
                del self._plugin_directory_map[plugin_name]
            
            # Stop hot-reload watcher for this directory
            if directory in self._file_watchers:
                observer, handler = self._file_watchers[directory]
                observer.stop()
                del self._file_watchers[directory]
            
            logger.info("Plugin directory removed", 
                       directory=directory,
                       removed_plugins=plugins_to_remove)
            return True
            
        except Exception as e:
            logger.error("Failed to remove plugin directory",
                        directory=directory,
                        error=str(e))
            return False
    
    def get_plugin_dependencies(self, plugin_path: str) -> List[str]:
        """
        Extract plugin dependencies from file.
        
        Args:
            plugin_path: Path to the plugin file
            
        Returns:
            List of dependency names
        """
        dependencies = []
        try:
            with open(plugin_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Look for dependency declarations in comments
            # Format: # DEPENDENCIES: plugin1, plugin2, plugin3
            import re
            dep_pattern = r'#\s*DEPENDENCIES:\s*([^\n]+)'
            matches = re.findall(dep_pattern, content, re.IGNORECASE)
            
            for match in matches:
                deps = [dep.strip() for dep in match.split(',')]
                dependencies.extend(deps)
            
            # Remove duplicates and empty strings
            dependencies = list(filter(None, list(set(dependencies))))
            
            logger.debug("Plugin dependencies extracted",
                        plugin_path=plugin_path,
                        dependencies=dependencies)
            
        except Exception as e:
            logger.error("Failed to extract plugin dependencies",
                        plugin_path=plugin_path,
                        error=str(e))
        
        return dependencies
    
    def resolve_plugin_load_order(self) -> List[str]:
        """
        Resolve plugin load order based on dependencies.
        
        Returns:
            List of plugin files in dependency order
        """
        try:
            all_plugin_files = self.discover_plugins()
            plugin_deps = {}
            
            # Extract dependencies for each plugin
            for plugin_file in all_plugin_files:
                deps = self.get_plugin_dependencies(plugin_file)
                plugin_name = Path(plugin_file).stem
                plugin_deps[plugin_name] = deps
            
            # Topological sort
            sorted_plugins = []
            visited = set()
            temp_visited = set()
            
            def visit(plugin_name):
                if plugin_name in temp_visited:
                    raise ValueError(f"Circular dependency detected involving {plugin_name}")
                if plugin_name in visited:
                    return
                
                temp_visited.add(plugin_name)
                
                # Visit dependencies first
                for dep in plugin_deps.get(plugin_name, []):
                    visit(dep)
                
                temp_visited.remove(plugin_name)
                visited.add(plugin_name)
                sorted_plugins.append(plugin_name)
            
            # Visit all plugins
            for plugin_name in plugin_deps.keys():
                if plugin_name not in visited:
                    visit(plugin_name)
            
            # Convert back to file paths
            sorted_files = []
            for plugin_name in sorted_plugins:
                for plugin_file in all_plugin_files:
                    if Path(plugin_file).stem == plugin_name:
                        sorted_files.append(plugin_file)
                        break
            
            logger.info("Plugin load order resolved",
                       total_plugins=len(sorted_files),
                       order=[Path(f).stem for f in sorted_files])
            
            return sorted_files
            
        except Exception as e:
            logger.error("Failed to resolve plugin load order", error=str(e))
            # Fallback to simple discovery order
            return self.discover_plugins() 
```
  ```
  --- END OF FILE scribe/core\plugin_loader.py ---
  --- START OF FILE scribe/core\port_adapters.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
HMA v2.1 Port Adapter Implementations

Concrete implementations of all mandatory HMA ports for Scribe v2.1.
These adapters provide the technology-specific implementations while
maintaining the technology-agnostic port interfaces.
"""

import asyncio
import json
import time
import threading
from typing import Dict, Any, Optional, List, Callable
from pathlib import Path
import queue
from collections import defaultdict

from .hma_ports import (
    PluginExecutionPort, CredBrokerQueryPort, EventBusPort, 
    ObservabilityPort, ConfigurationPort, HealthCheckPort,
    CommandExecutionPort, FileSystemPort, LoggingPort, PluginContextPort,
    PluginExecutionContext, PortStatus
)
from .boundary_validator import BoundaryValidator
from .hma_telemetry import HMATelemetry
from .mtls import get_mtls_manager, MTLSConfig
from .logging_config import get_scribe_logger

logger = get_scribe_logger(__name__)


class SecurityError(Exception):
    """Exception raised when security requirements are not met."""
    pass


class ScribePluginExecutionAdapter(PluginExecutionPort):
    """Concrete implementation of plugin execution port"""
    
    def __init__(self, plugin_loader, security_manager, telemetry: HMATelemetry, config_manager=None, port_registry=None):
        self.plugin_loader = plugin_loader
        self.security_manager = security_manager
        self.telemetry = telemetry
        self.config_manager = config_manager
        self.port_registry = port_registry
        self.executing_plugins: Dict[str, bool] = {}
        self._lock = threading.RLock()
        
        # HMA v2.2 mandatory mTLS manager for inter-plugin communication
        self.mtls_manager = get_mtls_manager()
        self._configure_mtls()
    
    def _get_port_registry(self):
        """Get the port registry for plugin access"""
        return self.port_registry
    
    def _configure_mtls(self):
        """Configure mTLS for inter-plugin communication as required by HMA v2.2."""
        try:
            # Get mTLS configuration from config manager
            mtls_settings = self.config_manager.get('mtls', {}) if self.config_manager else {}
            
            if mtls_settings.get('enabled', False):
                cert_file = mtls_settings.get('cert_file')
                key_file = mtls_settings.get('key_file')
                ca_file = mtls_settings.get('ca_file')
                
                if cert_file and key_file and ca_file:
                    # Create mTLS configuration
                    mtls_config = MTLSConfig(
                        cert_file=cert_file,
                        key_file=key_file,
                        ca_file=ca_file
                    )
                    
                    # Add to mTLS manager
                    self.mtls_manager.add_configuration("plugin_execution", mtls_config)
                    
                    logger.info("mTLS configured for plugin execution",
                               cert_file=cert_file,
                               ca_file=ca_file)
                else:
                    logger.warning("mTLS enabled but certificate files not configured")
            else:
                logger.info("mTLS not enabled for plugin execution")
                
        except Exception as e:
            logger.error("Failed to configure mTLS", error=str(e))
            # Continue without mTLS but log the issue
    
    def _enforce_mtls_security(self, plugin_id: str) -> bool:
        """Enforce mTLS security for plugin communication."""
        try:
            # Check if mTLS is configured and required
            mtls_session = self.mtls_manager.get_session("plugin_execution")
            if mtls_session:
                logger.debug("mTLS security enforced for plugin",
                           plugin_id=plugin_id)
                return True
            else:
                # Log warning but allow execution (for backward compatibility)
                logger.warning("mTLS not configured for plugin communication",
                             plugin_id=plugin_id)
                return True
                
        except Exception as e:
            logger.error("mTLS security enforcement failed",
                        plugin_id=plugin_id,
                        error=str(e))
            return False
    
    async def execute_plugin(self, 
                           plugin_id: str, 
                           input_data: Dict[str, Any],
                           context: Optional[PluginExecutionContext] = None) -> Dict[str, Any]:
        """Execute plugin with validation and telemetry"""
        
        # HMA v2.2 mandatory OTEL boundary telemetry
        with self.telemetry.start_span(
            "plugin_execution_boundary", plugin_id
        ) as span:
            # Add HMA resource attributes as required by Part 1a Sec 1.4.1
            if hasattr(span, 'set_attribute'):
                span.set_attribute("hma.boundary.type", "plugin_execution")
                span.set_attribute("hma.source.component", "core")
                span.set_attribute("hma.target.component", plugin_id)
                span.set_attribute("hma.operation", "execute_plugin")
        
        with self.telemetry.trace_boundary_operation(
            "execute_plugin", "plugin_execution", "core", plugin_id
        ) as span:
            start_time = time.time()
            
            try:
                # Mark plugin as busy
                with self._lock:
                    self.executing_plugins[plugin_id] = True
                
                # HMA v2.2 mandatory mTLS enforcement for inter-plugin communication
                if not self._enforce_mtls_security(plugin_id):
                    raise SecurityError(f"mTLS security enforcement failed for plugin {plugin_id}")
                
                # Validate plugin access
                if not await self.security_manager.validate_plugin_access(plugin_id):
                    raise PermissionError(f"Plugin {plugin_id} access denied")
                
                # Validate input data
                if not await self.validate_plugin_input(plugin_id, input_data):
                    raise ValueError(f"Invalid input data for plugin {plugin_id}")
                
                # Get plugin instance
                plugin_info = self.plugin_loader.get_plugin(plugin_id)
                if not plugin_info:
                    raise ValueError(f"Plugin {plugin_id} not found")
                
                # Create plugin execution context
                execution_context = {
                    "file_path": input_data.get("file_path", ""),
                    "match_data": input_data.get("match"),
                    "request_id": context.request_id if context else None,
                    "correlation_id": context.correlation_id if context else None
                }
                
                # Create plugin instance with port-based access
                plugin_instance = plugin_info.create_instance(
                    input_data.get("params", {}),
                    # Pass the port registry from the main system
                    self._get_port_registry(),
                    execution_context
                )
                
                # Execute plugin
                if hasattr(plugin_instance, 'execute_async'):
                    result = await plugin_instance.execute_async(input_data)
                else:
                    # Run synchronous plugin in thread pool
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(
                        None, 
                        plugin_instance.execute,
                        input_data.get("file_content", ""),
                        input_data.get("match"),
                        input_data.get("file_path", ""),
                        input_data.get("params", {})
                    )
                
                # Record success metrics
                duration = (time.time() - start_time) * 1000
                self.telemetry.record_boundary_crossing(
                    "core", plugin_id, "execute", duration
                )
                
                return {
                    "success": True,
                    "result": result,
                    "plugin_id": plugin_id,
                    "execution_time_ms": duration
                }
                
            except Exception as e:
                self.telemetry.record_error("plugin_execution_failed", plugin_id, str(e))
                logger.error("Plugin execution failed", 
                           plugin_id=plugin_id, error=str(e))
                
                return {
                    "success": False,
                    "error": str(e),
                    "plugin_id": plugin_id,
                    "execution_time_ms": (time.time() - start_time) * 1000
                }
                
            finally:
                # Mark plugin as available
                with self._lock:
                    self.executing_plugins[plugin_id] = False
    
    def get_plugin_status(self, plugin_id: str) -> PortStatus:
        """Get current plugin execution status"""
        # HMA v2.2 mandatory OTEL boundary telemetry
        with self.telemetry.start_span("get_plugin_status_boundary", plugin_id) as span:
            if hasattr(span, 'set_attribute'):
                span.set_attribute("hma.boundary.type", "plugin_status")
                span.set_attribute("hma.operation", "get_status")
        
        with self._lock:
            if plugin_id in self.executing_plugins and self.executing_plugins[plugin_id]:
                return PortStatus.BUSY
            
        plugin_info = self.plugin_loader.get_plugin(plugin_id)
        if plugin_info:
            return PortStatus.AVAILABLE
        else:
            return PortStatus.OFFLINE
    
    async def validate_plugin_input(self, plugin_id: str, input_data: Dict[str, Any]) -> bool:
        """Validate input data against plugin schema"""
        # HMA v2.2 mandatory OTEL boundary telemetry
        with self.telemetry.start_span("validate_plugin_input_boundary", plugin_id) as span:
            if hasattr(span, 'set_attribute'):
                span.set_attribute("hma.boundary.type", "validation")
                span.set_attribute("hma.operation", "validate_input")
        
        try:
            plugin_info = self.plugin_loader.get_plugin(plugin_id)
            if not plugin_info:
                return False
            
            # Get plugin manifest for validation schema
            manifest = plugin_info.manifest
            if not manifest:
                return True  # No manifest, skip validation
            
            # Extract validation schema from manifest
            config_schema = (
                manifest
                .get("interface_contracts", {})
                .get("action_interface", {})
                .get("configuration_schema", {})
            )
            
            if config_schema:
                import jsonschema
                jsonschema.validate(input_data.get("params", {}), config_schema)
            
            return True
            
        except Exception as e:
            logger.warning("Plugin input validation failed", 
                         plugin_id=plugin_id, error=str(e))
            return False
    
    def list_available_plugins(self) -> List[str]:
        """List all available plugins"""
        return list(self.plugin_loader.get_all_plugins().keys())


class ScribeConfigurationAdapter(ConfigurationPort):
    """Configuration management adapter"""
    
    def __init__(self, config_manager, telemetry: HMATelemetry):
        self.config_manager = config_manager
        self.telemetry = telemetry
        self.change_callbacks: Dict[str, List[Callable]] = defaultdict(list)
    
    async def get_config_value(self, 
                             key: str, 
                             component_id: str,
                             default: Any = None) -> Any:
        """Get configuration value for component"""
        # HMA v2.2 mandatory OTEL boundary telemetry
        with self.telemetry.start_span("get_config_value_boundary", component_id) as span:
            if hasattr(span, 'set_attribute'):
                span.set_attribute("hma.boundary.type", "configuration")
                span.set_attribute("hma.operation", "get_config")
                span.set_attribute("hma.config.key", key)
        
        try:
            value = self.config_manager.get(key, default)
            
            self.telemetry.emit_metric(
                "hma_config_access_total", 1.0,
                {"component_id": component_id, "config_key": key}
            )
            
            return value
            
        except Exception as e:
            logger.error("Configuration access failed",
                        key=key, component_id=component_id, error=str(e))
            return default
    
    async def set_config_value(self, 
                             key: str, 
                             value: Any,
                             component_id: str) -> bool:
        """Set configuration value"""
        try:
            # Note: This would need to be implemented in the config manager
            # For now, just log the attempt
            logger.info("Configuration update requested",
                       key=key, component_id=component_id)
            return True
            
        except Exception as e:
            logger.error("Configuration update failed",
                        key=key, component_id=component_id, error=str(e))
            return False
    
    async def validate_config(self, 
                            config: Dict[str, Any],
                            schema_id: str) -> bool:
        """Validate configuration against schema"""
        return self.config_manager.validate_config_dict(config)
    
    def subscribe_to_config_changes(self, 
                                  callback: Callable,
                                  component_id: str) -> bool:
        """Subscribe to configuration changes"""
        try:
            self.change_callbacks[component_id].append(callback)
            return True
        except Exception as e:
            logger.error("Config subscription failed", error=str(e))
            return False

class ScribeHealthCheckAdapter(HealthCheckPort):
    """Health monitoring adapter"""
    
    def __init__(self, telemetry: HMATelemetry):
        self.telemetry = telemetry
        self.health_checks: Dict[str, Callable] = {}
        self.component_status: Dict[str, Dict[str, Any]] = {}
        self.lock = threading.RLock()
    
    async def check_component_health(self, component_id: str) -> Dict[str, Any]:
        """Check health status of a component"""
        # HMA v2.2 mandatory OTEL boundary telemetry
        with self.telemetry.start_span("check_component_health_boundary", component_id) as span:
            if hasattr(span, 'set_attribute'):
                span.set_attribute("hma.boundary.type", "health_check")
                span.set_attribute("hma.operation", "check_health")
                span.set_attribute("hma.component.id", component_id)
        
        try:
            if component_id in self.health_checks:
                health_check = self.health_checks[component_id]
                result = await health_check()
                
                with self.lock:
                    self.component_status[component_id] = {
                        **result,
                        "last_check": time.time()
                    }
                
                return result
            else:
                return {"status": "unknown", "message": "No health check registered"}
                
        except Exception as e:
            logger.error("Health check failed", 
                        component_id=component_id, error=str(e))
            return {"status": "error", "message": str(e)}
    
    async def register_health_check(self, 
                                  component_id: str,
                                  check_function: Callable) -> bool:
        """Register health check function"""
        try:
            self.health_checks[component_id] = check_function
            logger.info("Health check registered", component_id=component_id)
            return True
        except Exception as e:
            logger.error("Health check registration failed", 
                        component_id=component_id, error=str(e))
            return False
    
    async def get_system_health(self) -> Dict[str, Any]:
        """Get overall system health status"""
        overall_status = "healthy"
        component_statuses = {}
        
        for component_id in self.health_checks:
            health = await self.check_component_health(component_id)
            component_statuses[component_id] = health
            
            if health.get("status") != "healthy":
                overall_status = "unhealthy"
        
        return {
            "overall_status": overall_status,
            "components": component_statuses,
            "timestamp": time.time()
        }


class ScribeCommandExecutionAdapter(CommandExecutionPort):
    """Command execution adapter using SecurityManager"""
    
    def __init__(self, security_manager, telemetry: HMATelemetry):
        self.security_manager = security_manager
        self.telemetry = telemetry
    
    async def execute_command_safely(self, 
                                   command_list: List[str],
                                   cwd: Optional[str] = None,
                                   timeout: int = 30,
                                   allowed_env_vars: Optional[List[str]] = None) -> tuple[bool, str, str]:
        """Execute command with security controls"""
        # HMA v2.2 mandatory OTEL boundary telemetry
        with self.telemetry.start_span("command_execution_boundary", str(command_list[0] if command_list else "unknown")) as span:
            if hasattr(span, 'set_attribute'):
                span.set_attribute("hma.boundary.type", "command_execution")
                span.set_attribute("hma.operation", "execute_command")
                span.set_attribute("hma.command", str(command_list))
        
        return self.security_manager.execute_command_safely(
            command_list=command_list,
            cwd=cwd,
            timeout=timeout,
            allowed_env_vars=allowed_env_vars
        )
    
    async def validate_command_security(self, command_list: List[str]) -> bool:
        """Validate command against security policies"""
        try:
            # Use the security manager's validation (if available)
            if hasattr(self.security_manager, 'validate_command_security'):
                return await self.security_manager.validate_command_security(command_list)
            else:
                # Basic validation - ensure command is not empty and is a list
                return bool(command_list and isinstance(command_list, list))
        except Exception as e:
            logger.error("Command security validation failed", error=str(e))
            return False


class ScribeFileSystemAdapter(FileSystemPort):
    """File system adapter with security validation"""
    
    def __init__(self, security_manager, telemetry: HMATelemetry):
        self.security_manager = security_manager
        self.telemetry = telemetry
    
    async def read_file_safely(self, file_path: str) -> Optional[str]:
        """Read file with security validation"""
        # HMA v2.2 mandatory OTEL boundary telemetry
        with self.telemetry.start_span("file_read_boundary", file_path) as span:
            if hasattr(span, 'set_attribute'):
                span.set_attribute("hma.boundary.type", "file_system")
                span.set_attribute("hma.operation", "read_file")
                span.set_attribute("hma.file.path", file_path)
        
        try:
            # Validate file access
            if not await self.validate_file_access(file_path, "read"):
                logger.warning("File read access denied", file_path=file_path)
                return None
            
            # Read file content
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
                
        except Exception as e:
            logger.error("File read failed", file_path=file_path, error=str(e))
            return None
    
    async def write_file_safely(self, file_path: str, content: str) -> bool:
        """Write file with security validation"""  
        # HMA v2.2 mandatory OTEL boundary telemetry
        with self.telemetry.start_span("file_write_boundary", file_path) as span:
            if hasattr(span, 'set_attribute'):
                span.set_attribute("hma.boundary.type", "file_system")
                span.set_attribute("hma.operation", "write_file")
                span.set_attribute("hma.file.path", file_path)
        
        try:
            # Validate file access
            if not await self.validate_file_access(file_path, "write"):
                logger.warning("File write access denied", file_path=file_path)
                return False
            
            # Write file content
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
                return True
                
        except Exception as e:
            logger.error("File write failed", file_path=file_path, error=str(e))
            return False
    
    async def validate_file_access(self, file_path: str, operation: str) -> bool:
        """Validate file access permissions"""
        try:
            # Use security manager validation if available
            if hasattr(self.security_manager, 'validate_file_access'):
                return await self.security_manager.validate_file_access(file_path, operation)
            else:
                # Basic validation - check file exists for read, path is valid for write
                path = Path(file_path)
                if operation == "read":
                    return path.exists() and path.is_file()
                elif operation == "write":
                    return path.parent.exists() and path.parent.is_dir()
                return False
                
        except Exception as e:
            logger.error("File access validation failed", 
                        file_path=file_path, 
                        operation=operation, 
                        error=str(e))
            return False


class ScribeLoggingAdapter(LoggingPort):
    """Logging adapter providing structured logging interface"""
    
    def __init__(self, telemetry: HMATelemetry):
        self.telemetry = telemetry
        self.logger = get_scribe_logger("plugin_logging")
    
    def log_info(self, message: str, **context) -> None:
        """Log information with context"""
        self.logger.info(message, **context)
        self.telemetry.emit_metric(
            "hma_plugin_logs_total", 1.0,
            {"level": "info", **{k: str(v) for k, v in context.items()}}
        )
    
    def log_warning(self, message: str, **context) -> None:
        """Log warning with context"""
        self.logger.warning(message, **context)
        self.telemetry.emit_metric(
            "hma_plugin_logs_total", 1.0,
            {"level": "warning", **{k: str(v) for k, v in context.items()}}
        )
    
    def log_error(self, message: str, **context) -> None:
        """Log error with context"""
        self.logger.error(message, **context)
        self.telemetry.emit_metric(
            "hma_plugin_logs_total", 1.0,
            {"level": "error", **{k: str(v) for k, v in context.items()}}
        )
    
    def log_debug(self, message: str, **context) -> None:
        """Log debug information with context"""
        self.logger.debug(message, **context)
        self.telemetry.emit_metric(
            "hma_plugin_logs_total", 1.0,
            {"level": "debug", **{k: str(v) for k, v in context.items()}}
        )


class ScribePluginContextAdapter(PluginContextPort):
    """Plugin context adapter providing port access"""
    
    def __init__(self, plugin_id: str, port_registry, execution_context: Dict[str, Any]):
        self.plugin_id = plugin_id
        self.port_registry = port_registry
        self.execution_context = execution_context
    
    def get_plugin_id(self) -> str:
        """Get current plugin identifier"""
        return self.plugin_id
    
    def get_execution_context(self) -> Dict[str, Any]:
        """Get plugin execution context"""
        return self.execution_context.copy()
    
    def get_port(self, port_type: str) -> Any:
        """Get access to other ports through context"""
        return self.port_registry.get_port(port_type)
```
  ```
  --- END OF FILE scribe/core\port_adapters.py ---
  --- START OF FILE scribe/core\ports.py ---
  ```
  ```py
from abc import ABC, abstractmethod
from typing import Any
from .atomic_write import atomic_write

class IEventSource(ABC):
    @abstractmethod
    def start(self) -> None:
        pass

    @abstractmethod
    def stop(self) -> None:
        pass

class IFileWriter(ABC):
    @abstractmethod
    def write(self, path: str, content: str) -> bool:
        pass

class AtomicFileWriter(IFileWriter):
    def write(self, path: str, content: str) -> bool:
        return atomic_write(path, content) 
```
  ```
  --- END OF FILE scribe/core\ports.py ---
  --- START OF FILE scribe/core\rule_processor.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Rule Processor

Handles rule matching logic and file content processing.
Implements efficient regex matching with pre-compiled patterns.
"""

import re
import fnmatch
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Iterator
import structlog

from .logging_config import get_scribe_logger
from .config_manager import ConfigManager

logger = get_scribe_logger(__name__)


class CompiledRule:
    """A rule with pre-compiled regex pattern for efficient matching."""
    
    def __init__(self, rule_dict: Dict[str, Any]):
        """
        Initialize a compiled rule.
        
        Args:
            rule_dict: Rule configuration dictionary
        """
        self.id = rule_dict['id']
        self.name = rule_dict['name']
        self.enabled = rule_dict['enabled']
        self.file_glob = rule_dict['file_glob']
        self.trigger_pattern = rule_dict['trigger_pattern']
        self.actions = rule_dict['actions']
        self.error_handling = rule_dict.get('error_handling', {})
        
        # Pre-compile the regex pattern
        try:
            self.compiled_pattern = re.compile(self.trigger_pattern, re.MULTILINE)
            logger.debug("Rule pattern compiled successfully",
                        rule_id=self.id,
                        pattern=self.trigger_pattern)
        except re.error as e:
            logger.error("Failed to compile rule pattern",
                        rule_id=self.id,
                        pattern=self.trigger_pattern,
                        error=str(e))
            raise ValueError(f"Invalid regex pattern in rule {self.id}: {e}")
    
    def matches_file_path(self, file_path: str) -> bool:
        """
        Check if the file path matches this rule's file glob pattern.
        
        Args:
            file_path: Path to the file to check
            
        Returns:
            True if the file path matches the glob pattern
        """
        try:
            # Convert to Path object for consistent handling
            path_obj = Path(file_path)
            
            # Use fnmatch for glob pattern matching
            # Check both the full path and just the filename
            full_path_str = str(path_obj).replace('\\', '/')  # Normalize path separators
            filename = path_obj.name
            
            # Try matching against full path first, then filename
            matches_full = fnmatch.fnmatch(full_path_str, self.file_glob)
            matches_name = fnmatch.fnmatch(filename, self.file_glob)
            
            result = matches_full or matches_name
            
            if result:
                logger.debug("File path matches rule glob",
                           rule_id=self.id,
                           file_path=file_path,
                           glob_pattern=self.file_glob)
            
            return result
            
        except Exception as e:
            logger.error("Error matching file path against glob pattern",
                        rule_id=self.id,
                        file_path=file_path,
                        glob_pattern=self.file_glob,
                        error=str(e))
            return False
    
    def find_matches(self, content: str) -> Iterator[re.Match]:
        """
        Find all matches of this rule's pattern in the given content.
        
        Args:
            content: File content to search
            
        Yields:
            Match objects for each occurrence of the pattern
        """
        try:
            for match in self.compiled_pattern.finditer(content):
                logger.debug("Pattern match found",
                           rule_id=self.id,
                           match_start=match.start(),
                           match_end=match.end(),
                           matched_text=match.group(0)[:100])  # Limit log output
                yield match
                
        except Exception as e:
            logger.error("Error finding pattern matches",
                        rule_id=self.id,
                        error=str(e),
                        exc_info=True)
    
    def __repr__(self) -> str:
        return f"CompiledRule(id='{self.id}', enabled={self.enabled})"


class RuleMatch:
    """Represents a successful rule match with context."""
    
    def __init__(self, rule: CompiledRule, match: re.Match, file_path: str, file_content: str, event_id: Optional[str] = None):
        """
        Initialize a rule match.
        
        Args:
            rule: The rule that matched
            match: The regex match object
            file_path: Path to the file that matched
            file_content: Full content of the file
            event_id: Unique identifier for the event that triggered this match
        """
        self.rule = rule
        self.match = match
        self.file_path = file_path
        self.file_content = file_content
        self.event_id = event_id
        self.timestamp = None  # Will be set by processor
    
    def get_match_context(self, context_lines: int = 2) -> Dict[str, Any]:
        """
        Get context around the match for debugging/logging.
        
        Args:
            context_lines: Number of lines before/after to include
            
        Returns:
            Dictionary with match context information
        """
        lines = self.file_content.split('\n')
        match_text = self.match.group(0)
        
        # Find which line the match is on
        match_line_num = self.file_content[:self.match.start()].count('\n')
        
        # Get context lines
        start_line = max(0, match_line_num - context_lines)
        end_line = min(len(lines), match_line_num + context_lines + 1)
        
        context = {
            'rule_id': self.rule.id,
            'file_path': self.file_path,
            'match_text': match_text,
            'match_line': match_line_num + 1,  # 1-based line numbers
            'match_start': self.match.start(),
            'match_end': self.match.end(),
            'context_lines': lines[start_line:end_line],
            'groups': self.match.groups() if self.match.groups() else None
        }
        
        return context
    
    def __repr__(self) -> str:
        return f"RuleMatch(rule_id='{self.rule.id}', file='{self.file_path}')"


class RuleProcessor:
    """
    Processes files against configured rules to find matches.
    
    Handles rule compilation, file path matching, and content pattern matching
    with efficient pre-compiled regex patterns.
    """
    
    def __init__(self, config_manager: ConfigManager):
        """
        Initialize the rule processor.
        
        Args:
            config_manager: Configuration manager instance
        """
        self.config_manager = config_manager
        self._compiled_rules: List[CompiledRule] = []
        
        # Register for configuration changes
        self.config_manager.add_change_callback(self._on_config_change)
        
        # Initial rule compilation
        self._compile_rules()
        
        logger.info("RuleProcessor initialized",
                   rules_count=len(self._compiled_rules))
    
    def _compile_rules(self) -> None:
        """Compile all rules from the current configuration."""
        try:
            rules = self.config_manager.get_rules()
            compiled_rules = []
            
            for rule_dict in rules:
                try:
                    compiled_rule = CompiledRule(rule_dict)
                    compiled_rules.append(compiled_rule)
                except Exception as e:
                    logger.error("Failed to compile rule",
                                rule_id=rule_dict.get('id', 'unknown'),
                                error=str(e))
                    # Continue with other rules
            
            self._compiled_rules = compiled_rules
            
            enabled_count = sum(1 for rule in self._compiled_rules if rule.enabled)
            logger.info("Rules compiled successfully",
                       total_rules=len(self._compiled_rules),
                       enabled_rules=enabled_count)
            
        except Exception as e:
            logger.error("Failed to compile rules", error=str(e), exc_info=True)
            self._compiled_rules = []
    
    def _on_config_change(self, new_config: Dict[str, Any]) -> None:
        """Handle configuration changes by recompiling rules."""
        logger.info("Configuration changed, recompiling rules")
        self._compile_rules()
    
    def get_matching_rules(self, file_path: str) -> List[CompiledRule]:
        """
        Get all enabled rules that match the given file path.
        
        Args:
            file_path: Path to the file to check
            
        Returns:
            List of rules that match the file path
        """
        matching_rules = []
        
        for rule in self._compiled_rules:
            if not rule.enabled:
                continue
                
            if rule.matches_file_path(file_path):
                matching_rules.append(rule)
        
        if matching_rules:
            logger.debug("Found matching rules for file",
                        file_path=file_path,
                        matching_rule_ids=[rule.id for rule in matching_rules])
        
        return matching_rules
    
    def process_file(self, file_path: str, file_content: str, event_id: Optional[str] = None) -> List[RuleMatch]:
        """
        Process a file against all applicable rules.
        
        Args:
            file_path: Path to the file being processed
            file_content: Content of the file
            event_id: Unique identifier for the event that triggered this processing
            
        Returns:
            List of rule matches found in the file
        """
        matches = []
        
        try:
            # Get rules that match this file path
            matching_rules = self.get_matching_rules(file_path)
            
            if not matching_rules:
                logger.debug("No rules match file path", file_path=file_path)
                return matches
            
            # Process each matching rule
            for rule in matching_rules:
                try:
                    # Find all pattern matches in the content
                    for regex_match in rule.find_matches(file_content):
                        rule_match = RuleMatch(rule, regex_match, file_path, file_content, event_id)
                        matches.append(rule_match)
                        
                        logger.info("Rule match found",
                                   event_id=event_id,
                                   rule_id=rule.id,
                                   file_path=file_path,
                                   match_line=rule_match.get_match_context()['match_line'])
                
                except Exception as e:
                    logger.error("Error processing rule against file",
                                rule_id=rule.id,
                                file_path=file_path,
                                error=str(e),
                                exc_info=True)
            
            if matches:
                logger.info("File processing complete",
                           file_path=file_path,
                           total_matches=len(matches),
                           matched_rule_ids=[match.rule.id for match in matches])
            
        except Exception as e:
            logger.error("Error processing file",
                        file_path=file_path,
                        error=str(e),
                        exc_info=True)
        
        return matches
    
    def get_rule_by_id(self, rule_id: str) -> Optional[CompiledRule]:
        """
        Get a compiled rule by its ID.
        
        Args:
            rule_id: The rule ID to search for
            
        Returns:
            The compiled rule if found, None otherwise
        """
        for rule in self._compiled_rules:
            if rule.id == rule_id:
                return rule
        return None
    
    def get_enabled_rules(self) -> List[CompiledRule]:
        """Get all enabled compiled rules."""
        return [rule for rule in self._compiled_rules if rule.enabled]
    
    def get_all_rules(self) -> List[CompiledRule]:
        """Get all compiled rules (enabled and disabled)."""
        return self._compiled_rules.copy()
    
    def validate_rule_pattern(self, pattern: str) -> Tuple[bool, Optional[str]]:
        """
        Validate a regex pattern without compiling a full rule.
        
        Args:
            pattern: Regex pattern to validate
            
        Returns:
            Tuple of (is_valid, error_message)
        """
        try:
            re.compile(pattern, re.MULTILINE)
            return True, None
        except re.error as e:
            return False, str(e)
    
    def stop(self) -> None:
        """Stop the rule processor and cleanup resources."""
        # Remove configuration change callback
        self.config_manager.remove_change_callback(self._on_config_change)
        
        logger.info("RuleProcessor stopped")
    
    def __enter__(self):
        """Context manager entry."""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        self.stop() 
```
  ```
  --- END OF FILE scribe/core\rule_processor.py ---
  --- START OF FILE scribe/core\security_audit.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Engine Security Audit System

Implements comprehensive security auditing, threat detection, and compliance
monitoring for production deployment with automated security checks.
"""

import os
import hashlib
import time
import threading
import re
from pathlib import Path
from typing import Dict, Any, List, Optional, Set, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
import structlog

from .logging_config import get_scribe_logger
from .telemetry import get_telemetry_manager

logger = get_scribe_logger(__name__)


class SecurityLevel(Enum):
    """Security risk levels."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class AuditEventType(Enum):
    """Types of security audit events."""
    FILE_ACCESS = "file_access"
    PERMISSION_CHANGE = "permission_change"
    PROCESS_EXECUTION = "process_execution"
    NETWORK_CONNECTION = "network_connection"
    AUTHENTICATION = "authentication"
    CONFIGURATION_CHANGE = "configuration_change"
    PLUGIN_LOAD = "plugin_load"
    DATA_ACCESS = "data_access"
    SECURITY_VIOLATION = "security_violation"


@dataclass
class SecurityViolation:
    """Represents a security violation."""
    violation_id: str
    event_type: AuditEventType
    severity: SecurityLevel
    description: str
    timestamp: float
    component: str
    user: Optional[str] = None
    source_ip: Optional[str] = None
    details: Dict[str, Any] = field(default_factory=dict)
    mitigated: bool = False
    mitigation_action: Optional[str] = None


@dataclass
class AuditEvent:
    """Represents a security audit event."""
    event_id: str
    event_type: AuditEventType
    timestamp: float
    component: str
    action: str
    outcome: str  # "success", "failure", "blocked"
    user: Optional[str] = None
    source_ip: Optional[str] = None
    target_resource: Optional[str] = None
    details: Dict[str, Any] = field(default_factory=dict)


class SecurityRule:
    """Defines a security rule for monitoring."""
    
    def __init__(self,
                 rule_id: str,
                 name: str,
                 event_types: List[AuditEventType],
                 condition_func: Callable[[AuditEvent], bool],
                 violation_severity: SecurityLevel = SecurityLevel.MEDIUM,
                 description: str = ""):
        """
        Initialize security rule.
        
        Args:
            rule_id: Unique rule identifier
            name: Human-readable rule name
            event_types: Event types this rule applies to
            condition_func: Function that returns True if rule is violated
            violation_severity: Severity level of violations
            description: Rule description
        """
        self.rule_id = rule_id
        self.name = name
        self.event_types = event_types
        self.condition_func = condition_func
        self.violation_severity = violation_severity
        self.description = description
        self.enabled = True
        
        # Statistics
        self.total_evaluations = 0
        self.violations_detected = 0
        self.last_violation_time = 0.0
    
    def evaluate(self, event: AuditEvent) -> bool:
        """
        Evaluate if event violates this rule.
        
        Args:
            event: Audit event to evaluate
            
        Returns:
            True if rule is violated
        """
        if not self.enabled or event.event_type not in self.event_types:
            return False
        
        self.total_evaluations += 1
        
        try:
            violated = self.condition_func(event)
            if violated:
                self.violations_detected += 1
                self.last_violation_time = time.time()
            return violated
        except Exception as e:
            logger.error("Security rule evaluation failed",
                        rule_id=self.rule_id,
                        error=str(e))
            return False


class SecurityAuditor:
    """
    Comprehensive security auditing system for Scribe Engine.
    
    Features:
    - Security event logging
    - Real-time threat detection
    - Compliance monitoring
    - Automated response
    - Security metrics
    """
    
    def __init__(self):
        """Initialize security auditor."""
        self._lock = threading.RLock()
        
        # Event storage
        self._audit_events: List[AuditEvent] = []
        self._violations: List[SecurityViolation] = []
        self._max_events = 10000  # Keep last 10k events
        
        # Security rules
        self._security_rules: Dict[str, SecurityRule] = {}
        
        # Monitoring
        self._monitoring_thread: Optional[threading.Thread] = None
        self._running = False
        
        # File integrity monitoring
        self._file_hashes: Dict[str, str] = {}
        self._monitored_files: Set[Path] = set()
        
        # Statistics
        self._stats = {
            "total_events": 0,
            "total_violations": 0,
            "blocked_actions": 0,
            "last_scan_time": 0.0
        }
        
        # Initialize default security rules
        self._initialize_default_rules()
        
        logger.info("SecurityAuditor initialized")
    
    def _initialize_default_rules(self):
        """Initialize default security rules."""
        # Suspicious file access patterns
        self.add_security_rule(
            rule_id="suspicious_file_access",
            name="Suspicious File Access Pattern",
            event_types=[AuditEventType.FILE_ACCESS],
            condition_func=self._check_suspicious_file_access,
            violation_severity=SecurityLevel.HIGH,
            description="Detects access to sensitive files or suspicious patterns"
        )
        
        # Failed authentication attempts
        self.add_security_rule(
            rule_id="failed_auth_attempts",
            name="Multiple Failed Authentication Attempts",
            event_types=[AuditEventType.AUTHENTICATION],
            condition_func=self._check_failed_auth_attempts,
            violation_severity=SecurityLevel.HIGH,
            description="Detects brute force authentication attempts"
        )
        
        # Privilege escalation attempts
        self.add_security_rule(
            rule_id="privilege_escalation",
            name="Privilege Escalation Attempt",
            event_types=[AuditEventType.PERMISSION_CHANGE],
            condition_func=self._check_privilege_escalation,
            violation_severity=SecurityLevel.CRITICAL,
            description="Detects attempts to escalate privileges"
        )
        
        # Malicious plugin loading
        self.add_security_rule(
            rule_id="malicious_plugin",
            name="Malicious Plugin Detection",
            event_types=[AuditEventType.PLUGIN_LOAD],
            condition_func=self._check_malicious_plugin,
            violation_severity=SecurityLevel.CRITICAL,
            description="Detects potentially malicious plugin code"
        )
        
        # Configuration tampering
        self.add_security_rule(
            rule_id="config_tampering",
            name="Configuration Tampering",
            event_types=[AuditEventType.CONFIGURATION_CHANGE],
            condition_func=self._check_config_tampering,
            violation_severity=SecurityLevel.HIGH,
            description="Detects unauthorized configuration changes"
        )
    
    def _check_suspicious_file_access(self, event: AuditEvent) -> bool:
        """Check for suspicious file access patterns."""
        target = event.target_resource
        if not target:
            return False
        
        # Check for access to sensitive files
        sensitive_patterns = [
            r'.*\.key$',      # Private keys
            r'.*\.pem$',      # Certificates
            r'.*password.*',  # Password files
            r'.*secret.*',    # Secret files
            r'/etc/passwd',   # System password file
            r'/etc/shadow',   # System shadow file
            r'.*\.env$',      # Environment files
        ]
        
        for pattern in sensitive_patterns:
            if re.match(pattern, target, re.IGNORECASE):
                return True
        
        # Check for rapid file access (potential data exfiltration)
        recent_events = [e for e in self._audit_events[-100:] 
                        if (e.event_type == AuditEventType.FILE_ACCESS and 
                            time.time() - e.timestamp < 60)]  # Last minute
        
        if len(recent_events) > 50:  # More than 50 file accesses per minute
            return True
        
        return False
    
    def _check_failed_auth_attempts(self, event: AuditEvent) -> bool:
        """Check for multiple failed authentication attempts."""
        if event.outcome != "failure":
            return False
        
        # Count failed auth attempts from same source in last 5 minutes
        source_ip = event.source_ip or "unknown"
        recent_failures = [
            e for e in self._audit_events[-200:]
            if (e.event_type == AuditEventType.AUTHENTICATION and
                e.outcome == "failure" and
                e.source_ip == source_ip and
                time.time() - e.timestamp < 300)  # Last 5 minutes
        ]
        
        return len(recent_failures) >= 5  # 5 or more failures
    
    def _check_privilege_escalation(self, event: AuditEvent) -> bool:
        """Check for privilege escalation attempts."""
        details = event.details
        
        # Check for attempts to gain admin/root privileges
        escalation_indicators = [
            "admin", "root", "administrator", "sudo", "elevation",
            "privilege", "permission", "grant", "escalate"
        ]
        
        description = event.action.lower()
        for indicator in escalation_indicators:
            if indicator in description:
                return True
        
        # Check for unusual permission changes
        if "permission" in details:
            new_perms = details.get("new_permissions", "")
            if "777" in new_perms or "rwx" in new_perms.lower():
                return True
        
        return False
    
    def _check_malicious_plugin(self, event: AuditEvent) -> bool:
        """Check for malicious plugin indicators."""
        plugin_path = event.target_resource
        if not plugin_path:
            return False
        
        try:
            # Read plugin content for analysis
            with open(plugin_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # Check for suspicious patterns
            malicious_patterns = [
                r'eval\s*\(',           # eval() function
                r'exec\s*\(',           # exec() function
                r'__import__\s*\(',     # Dynamic imports
                r'subprocess\.call',     # Subprocess execution
                r'os\.system',          # OS command execution
                r'socket\.socket',      # Network connections
                r'base64\.decode',      # Encoded payloads
                r'urllib\.request',     # HTTP requests
                r'pickle\.loads',       # Unsafe deserialization
            ]
            
            for pattern in malicious_patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    return True
            
            # Check for obfuscated code (high entropy)
            lines = content.split('\n')
            suspicious_lines = 0
            
            for line in lines:
                if len(line) > 100:  # Very long lines
                    # Calculate entropy (simplified)
                    unique_chars = len(set(line))
                    if unique_chars > len(line) * 0.7:  # High character diversity
                        suspicious_lines += 1
            
            if suspicious_lines > len(lines) * 0.1:  # >10% suspicious lines
                return True
            
        except Exception as e:
            logger.warning("Could not analyze plugin for security",
                          plugin_path=plugin_path,
                          error=str(e))
        
        return False
    
    def _check_config_tampering(self, event: AuditEvent) -> bool:
        """Check for configuration tampering."""
        # Check for unauthorized configuration changes
        user = event.user
        details = event.details
        
        # If no user specified, it's suspicious
        if not user or user == "unknown":
            return True
        
        # Check for security-related config changes
        security_config_keys = [
            "security", "auth", "permission", "access", "admin",
            "password", "secret", "key", "certificate", "ssl", "tls"
        ]
        
        config_key = details.get("config_key", "").lower()
        for key in security_config_keys:
            if key in config_key:
                return True
        
        return False
    
    def add_security_rule(self,
                         rule_id: str,
                         name: str,
                         event_types: List[AuditEventType],
                         condition_func: Callable[[AuditEvent], bool],
                         violation_severity: SecurityLevel = SecurityLevel.MEDIUM,
                         description: str = "") -> bool:
        """
        Add a custom security rule.
        
        Args:
            rule_id: Unique rule identifier
            name: Human-readable rule name
            event_types: Event types this rule applies to
            condition_func: Function that returns True if rule is violated
            violation_severity: Severity level of violations
            description: Rule description
            
        Returns:
            True if rule was added successfully
        """
        with self._lock:
            if rule_id in self._security_rules:
                logger.warning("Security rule already exists", rule_id=rule_id)
                return False
            
            rule = SecurityRule(
                rule_id=rule_id,
                name=name,
                event_types=event_types,
                condition_func=condition_func,
                violation_severity=violation_severity,
                description=description
            )
            
            self._security_rules[rule_id] = rule
            logger.debug("Security rule added", rule_id=rule_id, name=name)
            return True
    
    def log_security_event(self,
                          event_type: AuditEventType,
                          component: str,
                          action: str,
                          outcome: str = "success",
                          user: Optional[str] = None,
                          source_ip: Optional[str] = None,
                          target_resource: Optional[str] = None,
                          details: Optional[Dict[str, Any]] = None) -> str:
        """
        Log a security audit event.
        
        Args:
            event_type: Type of security event
            component: Component generating the event
            action: Action being performed
            outcome: Outcome of the action
            user: User performing the action
            source_ip: Source IP address
            target_resource: Target resource
            details: Additional event details
            
        Returns:
            Event ID for tracking
        """
        event_id = f"{component}_{int(time.time() * 1000000)}"
        
        event = AuditEvent(
            event_id=event_id,
            event_type=event_type,
            timestamp=time.time(),
            component=component,
            action=action,
            outcome=outcome,
            user=user,
            source_ip=source_ip,
            target_resource=target_resource,
            details=details or {}
        )
        
        with self._lock:
            # Add to event log
            self._audit_events.append(event)
            self._stats["total_events"] += 1
            
            # Trim event log if too large
            if len(self._audit_events) > self._max_events:
                self._audit_events = self._audit_events[-self._max_events:]
            
            # Evaluate security rules
            self._evaluate_security_rules(event)
        
        # Log the event
        logger.info("Security audit event",
                   event_id=event_id,
                   event_type=event_type.value,
                   component=component,
                   action=action,
                   outcome=outcome,
                   user=user,
                   target_resource=target_resource)
        
        # Record telemetry
        telemetry = get_telemetry_manager()
        if telemetry:
            telemetry.boundary_calls_counter.add(1, {
                "interface_type": "internal",
                "protocol": "security_audit",
                "operation": event_type.value,
                "outcome": outcome
            })
        
        return event_id
    
    def _evaluate_security_rules(self, event: AuditEvent):
        """Evaluate security rules against an event."""
        for rule in self._security_rules.values():
            try:
                if rule.evaluate(event):
                    self._handle_security_violation(rule, event)
            except Exception as e:
                logger.error("Security rule evaluation error",
                           rule_id=rule.rule_id,
                           event_id=event.event_id,
                           error=str(e))
    
    def _handle_security_violation(self, rule: SecurityRule, event: AuditEvent):
        """Handle a detected security violation."""
        violation_id = f"violation_{int(time.time() * 1000000)}"
        
        violation = SecurityViolation(
            violation_id=violation_id,
            event_type=event.event_type,
            severity=rule.violation_severity,
            description=f"Security rule '{rule.name}' violated",
            timestamp=time.time(),
            component=event.component,
            user=event.user,
            source_ip=event.source_ip,
            details={
                "rule_id": rule.rule_id,
                "rule_name": rule.name,
                "event_id": event.event_id,
                "target_resource": event.target_resource,
                "event_details": event.details
            }
        )
        
        with self._lock:
            self._violations.append(violation)
            self._stats["total_violations"] += 1
        
        # Log security violation
        logger.warning("Security violation detected",
                      violation_id=violation_id,
                      rule_id=rule.rule_id,
                      severity=rule.violation_severity.value,
                      component=event.component,
                      user=event.user)
        
        # Automated response based on severity
        self._automated_response(violation, event)
    
    def _automated_response(self, violation: SecurityViolation, event: AuditEvent):
        """Implement automated response to security violations."""
        try:
            if violation.severity == SecurityLevel.CRITICAL:
                # Critical violations - immediate action
                logger.critical("CRITICAL security violation - implementing immediate response",
                               violation_id=violation.violation_id)
                
                # Could implement:
                # - Block user/IP
                # - Disable component
                # - Alert administrators
                # - Trigger incident response
                
                violation.mitigation_action = "critical_response_triggered"
                
            elif violation.severity == SecurityLevel.HIGH:
                # High severity - strong response
                logger.error("HIGH severity security violation",
                           violation_id=violation.violation_id)
                
                # Could implement:
                # - Rate limiting
                # - Enhanced monitoring
                # - User notification
                
                violation.mitigation_action = "enhanced_monitoring_enabled"
                
            elif violation.severity == SecurityLevel.MEDIUM:
                # Medium severity - moderate response
                logger.warning("MEDIUM severity security violation",
                             violation_id=violation.violation_id)
                
                violation.mitigation_action = "violation_logged"
            
            violation.mitigated = True
            
        except Exception as e:
            logger.error("Automated response failed",
                        violation_id=violation.violation_id,
                        error=str(e))
    
    def add_file_integrity_monitor(self, file_path: Union[str, Path]):
        """Add a file to integrity monitoring."""
        path = Path(file_path)
        
        if not path.exists():
            logger.warning("File does not exist for integrity monitoring",
                          file_path=str(path))
            return
        
        try:
            # Calculate initial hash
            with open(path, 'rb') as f:
                content = f.read()
                file_hash = hashlib.sha256(content).hexdigest()
            
            with self._lock:
                self._monitored_files.add(path)
                self._file_hashes[str(path)] = file_hash
            
            logger.debug("File added to integrity monitoring",
                        file_path=str(path),
                        initial_hash=file_hash[:16])
            
        except Exception as e:
            logger.error("Failed to add file to integrity monitoring",
                        file_path=str(path),
                        error=str(e))
    
    def check_file_integrity(self) -> List[Dict[str, Any]]:
        """Check integrity of monitored files."""
        violations = []
        
        with self._lock:
            for file_path in list(self._monitored_files):
                try:
                    path = Path(file_path)
                    
                    if not path.exists():
                        # File was deleted
                        violations.append({
                            "file_path": str(file_path),
                            "violation": "file_deleted",
                            "timestamp": time.time()
                        })
                        continue
                    
                    # Calculate current hash
                    with open(path, 'rb') as f:
                        content = f.read()
                        current_hash = hashlib.sha256(content).hexdigest()
                    
                    original_hash = self._file_hashes.get(str(file_path))
                    
                    if original_hash and current_hash != original_hash:
                        # File was modified
                        violations.append({
                            "file_path": str(file_path),
                            "violation": "file_modified",
                            "original_hash": original_hash,
                            "current_hash": current_hash,
                            "timestamp": time.time()
                        })
                        
                        # Update stored hash
                        self._file_hashes[str(file_path)] = current_hash
                        
                        # Log security event
                        self.log_security_event(
                            event_type=AuditEventType.FILE_ACCESS,
                            component="file_integrity_monitor",
                            action="file_modification_detected",
                            outcome="violation",
                            target_resource=str(file_path),
                            details={
                                "original_hash": original_hash,
                                "current_hash": current_hash
                            }
                        )
                
                except Exception as e:
                    logger.error("File integrity check failed",
                                file_path=str(file_path),
                                error=str(e))
        
        return violations
    
    def get_security_metrics(self) -> Dict[str, Any]:
        """Get security audit metrics."""
        with self._lock:
            # Calculate violation rates
            current_time = time.time()
            recent_violations = [
                v for v in self._violations
                if current_time - v.timestamp < 3600  # Last hour
            ]
            
            violation_by_severity = {}
            for violation in recent_violations:
                severity = violation.severity.value
                violation_by_severity[severity] = violation_by_severity.get(severity, 0) + 1
            
            rule_stats = {}
            for rule_id, rule in self._security_rules.items():
                rule_stats[rule_id] = {
                    "name": rule.name,
                    "enabled": rule.enabled,
                    "total_evaluations": rule.total_evaluations,
                    "violations_detected": rule.violations_detected,
                    "last_violation_time": rule.last_violation_time
                }
            
            return {
                "total_events": self._stats["total_events"],
                "total_violations": self._stats["total_violations"],
                "recent_violations": len(recent_violations),
                "violations_by_severity": violation_by_severity,
                "monitored_files": len(self._monitored_files),
                "security_rules": rule_stats,
                "last_integrity_check": self._stats.get("last_integrity_check", 0)
            }
    
    def get_violations(self, 
                      severity: Optional[SecurityLevel] = None,
                      limit: int = 100) -> List[Dict[str, Any]]:
        """Get security violations."""
        with self._lock:
            violations = self._violations
            
            if severity:
                violations = [v for v in violations if v.severity == severity]
            
            # Sort by timestamp (newest first) and limit
            violations = sorted(violations, key=lambda v: v.timestamp, reverse=True)[:limit]
            
            return [
                {
                    "violation_id": v.violation_id,
                    "event_type": v.event_type.value,
                    "severity": v.severity.value,
                    "description": v.description,
                    "timestamp": v.timestamp,
                    "component": v.component,
                    "user": v.user,
                    "source_ip": v.source_ip,
                    "mitigated": v.mitigated,
                    "mitigation_action": v.mitigation_action,
                    "details": v.details
                }
                for v in violations
            ]
    
    def start_monitoring(self):
        """Start security monitoring."""
        if self._running:
            return
        
        self._running = True
        
        self._monitoring_thread = threading.Thread(
            target=self._monitoring_worker,
            name="SecurityMonitor",
            daemon=True
        )
        self._monitoring_thread.start()
        
        logger.info("Security monitoring started")
    
    def _monitoring_worker(self):
        """Background security monitoring worker."""
        try:
            while self._running:
                # Perform periodic security checks
                
                # File integrity check
                if time.time() - self._stats.get("last_integrity_check", 0) > 300:  # Every 5 minutes
                    violations = self.check_file_integrity()
                    if violations:
                        logger.warning("File integrity violations detected",
                                     violations_count=len(violations))
                    
                    self._stats["last_integrity_check"] = time.time()
                
                # Sleep before next check
                time.sleep(60)  # Check every minute
                
        except Exception as e:
            logger.error("Security monitoring worker error", error=str(e), exc_info=True)
    
    def stop_monitoring(self):
        """Stop security monitoring."""
        if not self._running:
            return
        
        self._running = False
        
        if self._monitoring_thread and self._monitoring_thread.is_alive():
            self._monitoring_thread.join(timeout=5.0)
        
        logger.info("Security monitoring stopped")


# Global security auditor instance
_security_auditor: Optional[SecurityAuditor] = None
_auditor_lock = threading.RLock()


def get_security_auditor() -> SecurityAuditor:
    """Get or create global security auditor."""
    global _security_auditor
    
    with _auditor_lock:
        if _security_auditor is None:
            _security_auditor = SecurityAuditor()
            _security_auditor.start_monitoring()
        
        return _security_auditor


def log_security_event(event_type: AuditEventType,
                      component: str,
                      action: str,
                      outcome: str = "success",
                      user: Optional[str] = None,
                      source_ip: Optional[str] = None,
                      target_resource: Optional[str] = None,
                      details: Optional[Dict[str, Any]] = None) -> str:
    """
    Convenience function to log security events.
    
    Args:
        event_type: Type of security event
        component: Component generating the event
        action: Action being performed
        outcome: Outcome of the action
        user: User performing the action
        source_ip: Source IP address
        target_resource: Target resource
        details: Additional event details
        
    Returns:
        Event ID for tracking
    """
    auditor = get_security_auditor()
    return auditor.log_security_event(
        event_type=event_type,
        component=component,
        action=action,
        outcome=outcome,
        user=user,
        source_ip=source_ip,
        target_resource=target_resource,
        details=details
    )


def shutdown_security_auditor():
    """Shutdown the global security auditor."""
    global _security_auditor
    
    with _auditor_lock:
        if _security_auditor:
            _security_auditor.stop_monitoring()
            _security_auditor = None
```
  ```
  --- END OF FILE scribe/core\security_audit.py ---
  --- START OF FILE scribe/core\security_manager.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Security Manager

Implements security sandboxing for action execution including:
- Command whitelisting
- Path restrictions
- Environment variable scrubbing
- Parameter validation
"""

import os
import re
import shlex
import subprocess
import yaml
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Set
import structlog

from .logging_config import get_scribe_logger
from .config_manager import ConfigManager

logger = get_scribe_logger(__name__)


class SecurityViolation(Exception):
    """Exception raised when a security policy is violated."""
    
    def __init__(self, violation_type: str, message: str, details: Optional[Dict[str, Any]] = None):
        """
        Initialize the exception.
        
        Args:
            violation_type: Type of security violation
            message: Error message
            details: Additional details about the violation
        """
        self.violation_type = violation_type
        self.details = details or {}
        super().__init__(f"Security violation ({violation_type}): {message}")


class SecurityManager:
    """
    Manages security policies and enforcement for action execution.
    
    Provides command whitelisting, path restrictions, environment scrubbing,
    and other security measures to ensure safe action execution.
    """
    
    def __init__(self, config_manager: ConfigManager):
        """
        Initialize the security manager.
        
        Args:
            config_manager: Configuration manager for security settings
        """
        self.config_manager = config_manager
        
        # Cache security settings for performance
        self._security_config: Dict[str, Any] = {}
        self._allowed_commands: Set[str] = set()
        self._restricted_paths: List[str] = []
        self._dangerous_patterns: List[re.Pattern] = []
        
        # Register for configuration changes
        self.config_manager.add_change_callback(self._on_config_change)
        
        # Load initial security configuration
        self._load_security_config()
        
        logger.info("SecurityManager initialized",
                   allowed_commands=len(self._allowed_commands),
                   restricted_paths=len(self._restricted_paths),
                   dangerous_patterns=len(self._dangerous_patterns))
    
    def _load_security_config(self) -> None:
        """Load security configuration from config manager and external security policy."""
        try:
            self._security_config = self.config_manager.get_security_settings()
            
            # Load allowed commands
            allowed_commands = self._security_config.get('allowed_commands', [])
            self._allowed_commands = set(allowed_commands)
            
            # Load restricted paths
            self._restricted_paths = self._security_config.get('restricted_paths', [])
            
            # Load dangerous patterns from external security policy file
            security_policy = self._load_security_policy_file()
            dangerous_patterns = security_policy.get('dangerous_patterns', [])
            
            self._dangerous_patterns = []
            for pattern in dangerous_patterns:
                try:
                    compiled_pattern = re.compile(pattern, re.IGNORECASE)
                    self._dangerous_patterns.append(compiled_pattern)
                except re.error as e:
                    logger.error("Failed to compile dangerous pattern",
                               pattern=pattern,
                               error=str(e))
            
            # Load dangerous environment keys from external policy
            self._dangerous_env_keys = security_policy.get('dangerous_env_keys_to_always_scrub', [])
            
            logger.debug("Security configuration loaded",
                        allowed_commands=len(self._allowed_commands),
                        restricted_paths=len(self._restricted_paths),
                        dangerous_patterns=len(self._dangerous_patterns),
                        dangerous_env_keys=len(self._dangerous_env_keys))
            
        except Exception as e:
            logger.error("Failed to load security configuration",
                        error=str(e),
                        exc_info=True)
            # Use safe defaults
            self._security_config = {}
            self._allowed_commands = set()
            self._restricted_paths = []
            self._dangerous_patterns = []
            self._dangerous_env_keys = []
    
    def _load_security_policy_file(self) -> Dict[str, Any]:
        """Load security policy from external YAML file."""
        try:
            # Try to get policy file path from config, fallback to default
            policy_file_path = self.config_manager.get('security_policy_file', 
                                                      'config/security_policy.yaml')
            
            # Make path relative to the script directory if not absolute
            if not Path(policy_file_path).is_absolute():
                script_dir = Path(__file__).parent.parent
                policy_file_path = script_dir / policy_file_path
            
            with open(policy_file_path, 'r', encoding='utf-8') as f:
                policy = yaml.safe_load(f)
                
            logger.debug("Security policy loaded from file", 
                        policy_file=str(policy_file_path),
                        policy_version=policy.get('policy_metadata', {}).get('version'))
            
            return policy or {}
            
        except FileNotFoundError:
            logger.warning("Security policy file not found, using empty policy", 
                          policy_file=str(policy_file_path))
            return {}
        except yaml.YAMLError as e:
            logger.error("Failed to parse security policy YAML file",
                        policy_file=str(policy_file_path),
                        error=str(e))
            return {}
        except Exception as e:
            logger.error("Failed to load security policy file",
                        policy_file=str(policy_file_path),
                        error=str(e))
            return {}
    
    def _on_config_change(self, new_config: Dict[str, Any]) -> None:
        """Handle configuration changes by reloading security settings."""
        logger.info("Security configuration changed, reloading")
        self._load_security_config()
    
    def validate_command(self, command: str) -> Tuple[bool, Optional[str]]:
        """
        Validate a command against the whitelist.
        
        Args:
            command: Command to validate
            
        Returns:
            Tuple of (is_allowed, denial_reason)
        """
        if not command or not command.strip():
            return False, "Empty command not allowed"
        
        try:
            # Parse the command to extract the base command
            parsed_command = shlex.split(command.strip())
            if not parsed_command:
                return False, "Invalid command format"
            
            base_command = parsed_command[0]
            
            # Extract just the command name (remove path)
            command_name = Path(base_command).name
            
            # Check against whitelist
            if self._allowed_commands and command_name not in self._allowed_commands:
                return False, f"Command '{command_name}' is not in the allowed commands list"
            
            # Check for dangerous patterns
            for pattern in self._dangerous_patterns:
                if pattern.search(command):
                    return False, f"Command contains dangerous pattern: {pattern.pattern}"
            
            logger.debug("Command validation passed",
                        command=command,
                        base_command=command_name)
            
            return True, None
            
        except Exception as e:
            logger.error("Error validating command",
                        command=command,
                        error=str(e))
            return False, f"Command validation error: {e}"
    
    def validate_path(self, path: str, operation: str = "access") -> Tuple[bool, Optional[str]]:
        """
        Validate a file path against restrictions.
        
        Args:
            path: Path to validate
            operation: Type of operation (access, read, write, execute)
            
        Returns:
            Tuple of (is_allowed, denial_reason)
        """
        if not path:
            return False, "Empty path not allowed"
        
        try:
            # Normalize the path
            normalized_path = str(Path(path).resolve())
            
            # Check against restricted paths
            for restricted_path in self._restricted_paths:
                # Handle glob patterns
                if '*' in restricted_path or '?' in restricted_path:
                    import fnmatch
                    if fnmatch.fnmatch(normalized_path, restricted_path):
                        return False, f"Path matches restricted pattern: {restricted_path}"
                else:
                    # Handle directory restrictions
                    restricted_normalized = str(Path(restricted_path).resolve())
                    if normalized_path.startswith(restricted_normalized):
                        return False, f"Path is within restricted directory: {restricted_path}"
            
            # Additional security checks
            if '..' in path:
                return False, "Path traversal attempts not allowed"
            
            if path.startswith('/'):
                return False, "Absolute paths not allowed"
            
            logger.debug("Path validation passed",
                        path=path,
                        normalized_path=normalized_path,
                        operation=operation)
            
            return True, None
            
        except Exception as e:
            logger.error("Error validating path",
                        path=path,
                        error=str(e))
            return False, f"Path validation error: {e}"
    
    
    def validate_action_params(self, action_type: str, params: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
        """
        Validate action parameters for security issues.
        
        Args:
            action_type: Type of action being executed
            params: Parameters to validate
            
        Returns:
            Tuple of (is_allowed, denial_reason)
        """
        try:
            # Check for dangerous parameter values
            for param_name, param_value in params.items():
                if isinstance(param_value, str):
                    # Check for dangerous patterns
                    for pattern in self._dangerous_patterns:
                        if pattern.search(param_value):
                            return False, f"Parameter '{param_name}' contains dangerous pattern"
                    
                    # Check for command injection attempts
                    if any(char in param_value for char in ['|', '&', ';', '`', '$(']):
                        return False, f"Parameter '{param_name}' contains command injection characters"
                    
                    # Check for path traversal
                    if '..' in param_value or param_value.startswith('/'):
                        return False, f"Parameter '{param_name}' contains unsafe path"
            
            # Action-specific validation
            if action_type == 'execute_command':
                command = params.get('command', '')
                is_valid, reason = self.validate_command(command)
                if not is_valid:
                    return False, f"Command validation failed: {reason}"
            
            elif action_type in ['read_file', 'write_file', 'append_file']:
                file_path = params.get('file_path', '')
                is_valid, reason = self.validate_path(file_path, 'write' if 'write' in action_type else 'read')
                if not is_valid:
                    return False, f"Path validation failed: {reason}"
            
            logger.debug("Action parameter validation passed",
                        action_type=action_type,
                        param_count=len(params))
            
            return True, None
            
        except Exception as e:
            logger.error("Error validating action parameters",
                        action_type=action_type,
                        error=str(e))
            return False, f"Parameter validation error: {e}"
    
    def execute_command_safely(self, 
                              command_list: List[str], 
                              cwd: Optional[str] = None,
                              timeout: int = 30,
                              allowed_env_vars: Optional[List[str]] = None) -> Tuple[bool, str, str]:
        """
        Execute a command safely with security restrictions.
        
        Args:
            command_list: Command as list of strings to execute
            cwd: Working directory (must be validated)
            timeout: Timeout in seconds
            
        Returns:
            Tuple of (success, stdout, stderr)
        """
        # Validate command (only validate the first element - the executable)
        if not command_list or len(command_list) == 0:
            raise SecurityViolation("command_validation", "Empty command list not allowed", {'command_list': command_list})
        
        is_valid, reason = self.validate_command(command_list[0])
        if not is_valid:
            raise SecurityViolation("command_validation", reason, {'command': command_list[0]})
        
        # Validate working directory if provided
        if cwd:
            is_valid, reason = self.validate_path(cwd, 'access')
            if not is_valid:
                raise SecurityViolation("path_validation", reason, {'path': cwd})
        
        try:
            # Scrub environment
            safe_env = self.scrub_environment(allowed_env_vars=allowed_env_vars)
            
            # Execute with restrictions
            result = subprocess.run(
                command_list,
                shell=False,  # Safe execution without shell
                cwd=cwd,
                env=safe_env,
                capture_output=True,
                text=True,
                timeout=timeout,
                check=False  # Don't raise on non-zero exit
            )
            
            logger.info("Command executed safely",
                       command=command_list,
                       exit_code=result.returncode,
                       cwd=cwd,
                       timeout=timeout)
            
            return result.returncode == 0, result.stdout, result.stderr
            
        except subprocess.TimeoutExpired:
            logger.error("Command execution timed out",
                        command=command_list,
                        timeout=timeout)
            raise SecurityViolation("execution_timeout", f"Command timed out after {timeout} seconds")
        
        except Exception as e:
            logger.error("Command execution failed",
                        command=command_list,
                        error=str(e),
                        exc_info=True)
            raise SecurityViolation("execution_error", f"Command execution failed: {e}")
    
    def get_security_stats(self) -> Dict[str, Any]:
        """
        Get security manager statistics.
        
        Returns:
            Dictionary with security statistics
        """
        return {
            'allowed_commands': list(self._allowed_commands),
            'allowed_commands_count': len(self._allowed_commands),
            'restricted_paths': self._restricted_paths,
            'restricted_paths_count': len(self._restricted_paths),
            'dangerous_patterns_count': len(self._dangerous_patterns),
            'security_config': self._security_config
        }
    
    def stop(self) -> None:
        """Stop the security manager and cleanup resources."""
        # Remove configuration change callback
        self.config_manager.remove_change_callback(self._on_config_change)
        
        logger.info("SecurityManager stopped")
    
    def scrub_environment(self, env_vars: Dict[str, str] = None, allowed_env_vars: Optional[List[str]] = None) -> Dict[str, str]:
        """
        Scrub sensitive information from environment variables.
        
        This method is overloaded to handle two different use cases:
        1. When called with env_vars dict: scrubs sensitive values from provided env vars
        2. When called with allowed_env_vars list: creates safe environment from os.environ
        
        Args:
            env_vars: Dictionary of environment variables to scrub (optional)
            allowed_env_vars: List of allowed environment variable names from os.environ (optional)
            
        Returns:
            Scrubbed environment variables with sensitive data removed/masked
        """
        # If env_vars dict is provided, scrub it (test compatibility mode)
        if env_vars is not None:
            return self._scrub_env_dict(env_vars)
        
        # Otherwise use the original implementation
        return self._scrub_from_os_environ(allowed_env_vars)
    
    def _scrub_env_dict(self, env_vars: Dict[str, str]) -> Dict[str, str]:
        """
        Scrub dangerous system environment variables from provided dict
        
        Args:
            env_vars: Dictionary of environment variables
            
        Returns:
            Scrubbed environment variables with dangerous keys removed and safe defaults
        """
        scrubbed = {}
        
        # Get dangerous keys from externalized policy (fallback to empty list if not loaded)
        dangerous_keys = getattr(self, '_dangerous_env_keys', [])
        
        # Copy all non-dangerous environment variables
        for key, value in env_vars.items():
            if key not in dangerous_keys:
                scrubbed[key] = value
        
        # Set safe defaults for critical environment variables
        scrubbed['PATH'] = '/usr/bin:/bin'
        scrubbed['LC_ALL'] = 'C' 
        scrubbed['LANG'] = 'C'
        
        return scrubbed
    
    def _scrub_from_os_environ(self, allowed_env_vars: Optional[List[str]] = None) -> Dict[str, str]:
        """
        Original scrub_environment implementation for creating safe environment from os.environ
        """
        safe_env: Dict[str, str] = {}

        # Start with an empty environment or a very minimal one
        safe_env: Dict[str, str] = {}

        # Populate with allowed variables from os.environ
        if allowed_env_vars:
            logger.debug(f"Processing allowed_env_vars: {allowed_env_vars}")
            for var_name in allowed_env_vars:
                if var_name in os.environ:
                    safe_env[var_name] = os.environ[var_name]
                    logger.debug(f"Allowed and copied from os.environ: {var_name}={safe_env[var_name]}")
                else:
                    logger.warning(f"Allowed environment variable '{var_name}' not found in system environment.")
        else:
            logger.debug("No specific environment variables allowed by (caller-provided) allowed_env_vars list.")

        # Ensure essential variables have safe defaults if not explicitly allowed and set from os.environ
        if 'PATH' not in safe_env:
            safe_env['PATH'] = '/usr/bin:/bin'
            logger.debug("Default PATH set for safe execution.")
        
        if 'HOME' not in safe_env:
            safe_env['HOME'] = os.path.expanduser('~')
            logger.debug(f"Default HOME set: {safe_env['HOME']}")

        logger.info(f"Environment scrubbed successfully. Final environment contains {len(safe_env)} variables.")
        return safe_env 
```
  ```
  --- END OF FILE scribe/core\security_manager.py ---
  --- START OF FILE scribe/core\telemetry.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Engine OpenTelemetry Integration

Implements HMA v2.2 mandatory boundary telemetry requirements using OpenTelemetry.
Provides tracing, metrics, and logging for all boundary interfaces.
"""

import time
import threading
from typing import Dict, Any, Optional, List, Union
from contextlib import contextmanager
import structlog

try:
    from opentelemetry import trace, metrics
    from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
    from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter
    from opentelemetry.sdk.trace import TracerProvider
    from opentelemetry.sdk.trace.export import BatchSpanProcessor
    from opentelemetry.sdk.metrics import MeterProvider
    from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
    from opentelemetry.sdk.resources import Resource
    from opentelemetry.semconv.resource import ResourceAttributes
    from opentelemetry.instrumentation.requests import RequestsInstrumentor
    from opentelemetry.instrumentation.threading import ThreadingInstrumentor
    OTEL_AVAILABLE = True
except ImportError:
    OTEL_AVAILABLE = False

from .logging_config import get_scribe_logger

logger = get_scribe_logger(__name__)


class MockTracer:
    """Mock tracer for when OpenTelemetry is not available."""
    
    def start_span(self, name: str, **kwargs):
        return MockSpan()
    
    def start_as_current_span(self, name: str, **kwargs):
        return MockSpan()


class MockSpan:
    """Mock span for when OpenTelemetry is not available."""
    
    def __enter__(self):
        return self
    
    def __exit__(self, *args):
        pass
    
    def set_attribute(self, key: str, value: Any):
        pass
    
    def set_status(self, status):
        pass
    
    def record_exception(self, exception: Exception):
        pass


class MockMeter:
    """Mock meter for when OpenTelemetry is not available."""
    
    def create_counter(self, name: str, **kwargs):
        return MockCounter()
    
    def create_histogram(self, name: str, **kwargs):
        return MockHistogram()
    
    def create_gauge(self, name: str, **kwargs):
        return MockGauge()


class MockCounter:
    """Mock counter for when OpenTelemetry is not available."""
    
    def add(self, amount: Union[int, float], attributes: Optional[Dict[str, Any]] = None):
        pass


class MockHistogram:
    """Mock histogram for when OpenTelemetry is not available."""
    
    def record(self, amount: Union[int, float], attributes: Optional[Dict[str, Any]] = None):
        pass


class MockGauge:
    """Mock gauge for when OpenTelemetry is not available."""
    
    def set(self, amount: Union[int, float], attributes: Optional[Dict[str, Any]] = None):
        pass


class TelemetryManager:
    """
    Manages OpenTelemetry integration for Scribe Engine.
    
    Implements HMA v2.2 mandatory boundary telemetry requirements:
    - Traces all boundary interface calls
    - Collects performance metrics
    - Provides correlation IDs for distributed tracing
    """
    
    def __init__(self, 
                 service_name: str = "scribe-engine",
                 endpoint: Optional[str] = None,
                 sampling_rate: float = 1.0):
        """
        Initialize telemetry manager.
        
        Args:
            service_name: Name of the service for telemetry
            endpoint: OpenTelemetry collector endpoint
            sampling_rate: Trace sampling rate (0.0 to 1.0)
        """
        self.service_name = service_name
        self.endpoint = endpoint
        self.sampling_rate = sampling_rate
        self.enabled = OTEL_AVAILABLE and endpoint is not None
        
        self._lock = threading.RLock()
        self._initialized = False
        
        # Initialize telemetry components
        if self.enabled:
            self._initialize_telemetry()
        else:
            self._initialize_mock_telemetry()
        
        logger.info("TelemetryManager initialized",
                   enabled=self.enabled,
                   service_name=service_name,
                   endpoint=endpoint,
                   otel_available=OTEL_AVAILABLE)
    
    def _initialize_telemetry(self):
        """Initialize real OpenTelemetry components."""
        try:
            # Create resource
            resource = Resource.create({
                ResourceAttributes.SERVICE_NAME: self.service_name,
                ResourceAttributes.SERVICE_VERSION: "2.0.0",
                "scribe.component": "engine"
            })
            
            # Initialize tracing
            if self.endpoint:
                span_exporter = OTLPSpanExporter(endpoint=self.endpoint)
                span_processor = BatchSpanProcessor(span_exporter)
            else:
                span_processor = None
            
            tracer_provider = TracerProvider(resource=resource)
            if span_processor:
                tracer_provider.add_span_processor(span_processor)
            
            trace.set_tracer_provider(tracer_provider)
            self.tracer = trace.get_tracer(self.service_name)
            
            # Initialize metrics
            if self.endpoint:
                metric_exporter = OTLPMetricExporter(endpoint=self.endpoint)
                metric_reader = PeriodicExportingMetricReader(
                    exporter=metric_exporter,
                    export_interval_millis=10000  # 10 seconds
                )
            else:
                metric_reader = None
            
            meter_provider = MeterProvider(
                resource=resource,
                metric_readers=[metric_reader] if metric_reader else []
            )
            metrics.set_meter_provider(meter_provider)
            self.meter = metrics.get_meter(self.service_name)
            
            # Create standard metrics
            self._create_standard_metrics()
            
            # Auto-instrument common libraries
            RequestsInstrumentor().instrument()
            ThreadingInstrumentor().instrument()
            
            self._initialized = True
            logger.info("OpenTelemetry initialized successfully")
            
        except Exception as e:
            logger.error("Failed to initialize OpenTelemetry",
                        error=str(e),
                        exc_info=True)
            self._initialize_mock_telemetry()
    
    def _initialize_mock_telemetry(self):
        """Initialize mock telemetry components."""
        self.tracer = MockTracer()
        self.meter = MockMeter()
        self._create_mock_metrics()
        self._initialized = True
        logger.info("Mock telemetry initialized")
    
    def _create_standard_metrics(self):
        """Create standard metrics for Scribe Engine."""
        # Counters
        self.action_executions_counter = self.meter.create_counter(
            name="scribe_action_executions_total",
            description="Total number of action executions",
            unit="1"
        )
        
        self.action_failures_counter = self.meter.create_counter(
            name="scribe_action_failures_total",
            description="Total number of action execution failures",
            unit="1"
        )
        
        self.file_events_counter = self.meter.create_counter(
            name="scribe_file_events_total", 
            description="Total number of file system events processed",
            unit="1"
        )
        
        self.boundary_calls_counter = self.meter.create_counter(
            name="scribe_boundary_calls_total",
            description="Total number of boundary interface calls",
            unit="1"
        )
        
        # Histograms
        self.action_duration_histogram = self.meter.create_histogram(
            name="scribe_action_duration_seconds",
            description="Duration of action executions",
            unit="s"
        )
        
        self.file_processing_duration_histogram = self.meter.create_histogram(
            name="scribe_file_processing_duration_seconds",
            description="Duration of file processing",
            unit="s"
        )
        
        self.boundary_call_duration_histogram = self.meter.create_histogram(
            name="scribe_boundary_call_duration_seconds",
            description="Duration of boundary interface calls",
            unit="s"
        )
        
        # Gauges
        self.active_workers_gauge = self.meter.create_gauge(
            name="scribe_active_workers",
            description="Number of active worker threads",
            unit="1"
        )
        
        self.queue_size_gauge = self.meter.create_gauge(
            name="scribe_queue_size",
            description="Current size of event queue",
            unit="1"
        )
    
    def _create_mock_metrics(self):
        """Create mock metrics."""
        self.action_executions_counter = MockCounter()
        self.action_failures_counter = MockCounter()
        self.file_events_counter = MockCounter()
        self.boundary_calls_counter = MockCounter()
        self.action_duration_histogram = MockHistogram()
        self.file_processing_duration_histogram = MockHistogram()
        self.boundary_call_duration_histogram = MockHistogram()
        self.active_workers_gauge = MockGauge()
        self.queue_size_gauge = MockGauge()
    
    @contextmanager
    def trace_boundary_call(self, 
                           interface_type: str,
                           protocol: str,
                           endpoint: str,
                           operation: str,
                           attributes: Optional[Dict[str, Any]] = None):
        """
        Trace a boundary interface call with HMA v2.2 compliance.
        
        Args:
            interface_type: Type of interface (inbound/outbound/bidirectional)
            protocol: Protocol used (http/grpc/websocket/file_system/event_bus)
            endpoint: Endpoint identifier
            operation: Operation being performed
            attributes: Additional attributes to include in the trace
        """
        span_name = f"{interface_type}_{protocol}_{operation}"
        start_time = time.time()
        
        span_attributes = {
            "scribe.boundary.interface_type": interface_type,
            "scribe.boundary.protocol": protocol,
            "scribe.boundary.endpoint": endpoint,
            "scribe.boundary.operation": operation,
            "scribe.component": "boundary"
        }
        
        if attributes:
            span_attributes.update(attributes)
        
        with self.tracer.start_as_current_span(span_name, attributes=span_attributes) as span:
            try:
                # Record boundary call metric
                self.boundary_calls_counter.add(1, {
                    "interface_type": interface_type,
                    "protocol": protocol,
                    "endpoint": endpoint,
                    "operation": operation
                })
                
                yield span
                
                # Record success
                duration = time.time() - start_time
                self.boundary_call_duration_histogram.record(duration, {
                    "interface_type": interface_type,
                    "protocol": protocol,
                    "operation": operation,
                    "status": "success"
                })
                
            except Exception as e:
                # Record failure
                duration = time.time() - start_time
                self.boundary_call_duration_histogram.record(duration, {
                    "interface_type": interface_type,
                    "protocol": protocol,
                    "operation": operation,
                    "status": "error"
                })
                
                span.record_exception(e)
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                raise
    
    @contextmanager
    def trace_action_execution(self, action_type: str, rule_name: str, file_path: str):
        """
        Trace action execution with performance metrics.
        
        Args:
            action_type: Type of action being executed
            rule_name: Name of the rule triggering the action
            file_path: Path of the file being processed
        """
        span_name = f"action_{action_type}"
        start_time = time.time()
        
        span_attributes = {
            "scribe.action.type": action_type,
            "scribe.rule.name": rule_name,
            "scribe.file.path": file_path,
            "scribe.component": "action"
        }
        
        with self.tracer.start_as_current_span(span_name, attributes=span_attributes) as span:
            try:
                # Record action execution metric
                self.action_executions_counter.add(1, {
                    "action_type": action_type,
                    "rule_name": rule_name
                })
                
                yield span
                
                # Record success metrics
                duration = time.time() - start_time
                self.action_duration_histogram.record(duration, {
                    "action_type": action_type,
                    "rule_name": rule_name,
                    "status": "success"
                })
                
            except Exception as e:
                # Record failure metrics
                duration = time.time() - start_time
                self.action_duration_histogram.record(duration, {
                    "action_type": action_type,
                    "rule_name": rule_name,
                    "status": "error"
                })
                
                self.action_failures_counter.add(1, {
                    "action_type": action_type,
                    "rule_name": rule_name,
                    "error_type": type(e).__name__
                })
                
                span.record_exception(e)
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                raise
    
    @contextmanager
    def trace_file_processing(self, file_path: str, event_type: str):
        """
        Trace file processing operations.
        
        Args:
            file_path: Path of the file being processed
            event_type: Type of file system event
        """
        span_name = f"file_processing_{event_type}"
        start_time = time.time()
        
        span_attributes = {
            "scribe.file.path": file_path,
            "scribe.file.event_type": event_type,
            "scribe.component": "file_processor"
        }
        
        with self.tracer.start_as_current_span(span_name, attributes=span_attributes) as span:
            try:
                # Record file event metric
                self.file_events_counter.add(1, {
                    "event_type": event_type
                })
                
                yield span
                
                # Record success metrics
                duration = time.time() - start_time
                self.file_processing_duration_histogram.record(duration, {
                    "event_type": event_type,
                    "status": "success"
                })
                
            except Exception as e:
                # Record failure metrics
                duration = time.time() - start_time
                self.file_processing_duration_histogram.record(duration, {
                    "event_type": event_type,
                    "status": "error"
                })
                
                span.record_exception(e)
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                raise
    
    def update_worker_count(self, count: int):
        """Update active worker count gauge."""
        self.active_workers_gauge.set(count)
    
    def update_queue_size(self, size: int):
        """Update event queue size gauge."""
        self.queue_size_gauge.set(size)
    
    def get_current_trace_id(self) -> Optional[str]:
        """Get current trace ID for correlation."""
        if not self.enabled:
            return None
        
        try:
            current_span = trace.get_current_span()
            if current_span:
                trace_id = current_span.get_span_context().trace_id
                return format(trace_id, '032x')
        except Exception:
            pass
        
        return None
    
    def shutdown(self):
        """Shutdown telemetry components gracefully."""
        if not self._initialized:
            return
        
        try:
            if self.enabled and OTEL_AVAILABLE:
                # Flush any pending telemetry data
                tracer_provider = trace.get_tracer_provider()
                if hasattr(tracer_provider, 'shutdown'):
                    tracer_provider.shutdown()
                
                meter_provider = metrics.get_meter_provider()
                if hasattr(meter_provider, 'shutdown'):
                    meter_provider.shutdown()
            
            logger.info("Telemetry manager shutdown completed")
            
        except Exception as e:
            logger.error("Error during telemetry shutdown",
                        error=str(e),
                        exc_info=True)


# Global telemetry manager instance
_telemetry_manager: Optional[TelemetryManager] = None
_telemetry_lock = threading.RLock()


def initialize_telemetry(service_name: str = "scribe-engine",
                        endpoint: Optional[str] = None,
                        sampling_rate: float = 1.0) -> TelemetryManager:
    """
    Initialize global telemetry manager.
    
    Args:
        service_name: Name of the service for telemetry
        endpoint: OpenTelemetry collector endpoint
        sampling_rate: Trace sampling rate (0.0 to 1.0)
        
    Returns:
        TelemetryManager instance
    """
    global _telemetry_manager
    
    with _telemetry_lock:
        if _telemetry_manager is None:
            _telemetry_manager = TelemetryManager(
                service_name=service_name,
                endpoint=endpoint,
                sampling_rate=sampling_rate
            )
        
        return _telemetry_manager


def get_telemetry_manager() -> Optional[TelemetryManager]:
    """Get the global telemetry manager instance."""
    return _telemetry_manager


def shutdown_telemetry():
    """Shutdown the global telemetry manager."""
    global _telemetry_manager
    
    with _telemetry_lock:
        if _telemetry_manager:
            _telemetry_manager.shutdown()
            _telemetry_manager = None
```
  ```
  --- END OF FILE scribe/core\telemetry.py ---
  --- START OF FILE scribe/core\windows_atomic_write.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Windows-Enhanced Atomic Write System for HMA v2.1

Provides Windows-optimized atomic file operations with proper file locking,
long path support, and comprehensive retry mechanisms for Windows-specific issues.
"""

import os
import sys
import tempfile
import time
import ctypes
from pathlib import Path, PurePath
from typing import Union, Dict, Any, Optional
import threading

from .logging_config import get_scribe_logger

# Windows-specific imports
if os.name == 'nt':
    try:
        import win32file
        import win32api
        import win32con
        HAS_PYWIN32 = True
    except ImportError:
        HAS_PYWIN32 = False
        import warnings
        warnings.warn("pywin32 not available, Windows optimizations disabled")

logger = get_scribe_logger(__name__)

class WindowsFileHandler:
    """Windows-specific file handling with proper path support"""
    
    @staticmethod
    def normalize_path(path_input: Union[str, Path]) -> Path:
        """Properly normalize paths for Windows with long path support"""
        try:
            path = Path(path_input).resolve()
            
            # Handle long path names on Windows (>260 characters)
            if os.name == 'nt' and len(str(path)) > 260:
                # Use \\?\ prefix for long paths
                if not str(path).startswith('\\\\?\\'):
                    return Path(f"\\\\?\\{path}")
            
            return path
        except Exception as e:
            logger.error("Path normalization failed", path=str(path_input), error=str(e))
            return Path(path_input)
    
    @staticmethod
    def is_case_sensitive_match(path1: str, path2: str) -> bool:
        """Check if paths match considering Windows case insensitivity"""
        if os.name == 'nt':
            try:
                return Path(path1).resolve() == Path(path2).resolve()
            except Exception:
                return path1.lower() == path2.lower()
        else:
            return path1 == path2
    
    @staticmethod 
    def get_windows_permissions(file_path: str) -> Dict[str, bool]:
        """Get Windows file permissions"""
        try:
            return {
                'readable': os.access(file_path, os.R_OK),
                'writable': os.access(file_path, os.W_OK),
                'executable': os.access(file_path, os.X_OK)
            }
        except Exception as e:
            logger.warning("Failed to get Windows permissions", path=file_path, error=str(e))
            return {'readable': False, 'writable': False, 'executable': False}

class WindowsAtomicWriter:
    """Windows-optimized atomic write operations with comprehensive retry logic"""
    
    def __init__(self):
        self.max_retries = 15
        self.base_delay = 0.025  # Start with 25ms
        self.max_delay = 5.0     # Max 5 seconds
        self.backoff_multiplier = 1.5
        
        # Thread-local storage for tracking retries
        self._local = threading.local()
    
    def atomic_write(self, filepath: Union[str, Path], data: Union[str, bytes], 
                    encoding: str = 'utf-8', mode: str = 'w') -> bool:
        """
        Windows-optimized atomic write with comprehensive error handling
        
        Args:
            filepath: Target file path
            data: Data to write
            encoding: Text encoding (for text mode)
            mode: Write mode ('w' or 'wb')
            
        Returns:
            bool: True if successful, False otherwise
        """
        # Normalize path for Windows
        filepath = WindowsFileHandler.normalize_path(filepath)
        temp_path = None
        temp_fd = None
        
        try:
            # Validate mode
            if mode not in ('w', 'wb'):
                raise ValueError(f"Unsupported mode '{mode}'. Use 'w' for text or 'wb' for binary.")
            
            is_binary = mode == 'wb'
            
            # Ensure parent directory exists
            filepath.parent.mkdir(parents=True, exist_ok=True)
            
            # Create temp file in same directory for atomic rename
            temp_fd, temp_path = tempfile.mkstemp(
                dir=filepath.parent,
                prefix=f".{filepath.name}.tmp.",
                suffix=".scribe-atomic"
            )
            temp_path = Path(temp_path)
            
            logger.debug("Starting atomic write",
                        target_file=str(filepath),
                        temp_file=str(temp_path),
                        data_size=len(data),
                        mode=mode)
            
            # Write data to temporary file
            success = self._write_temp_file(temp_fd, data, encoding, is_binary)
            if not success:
                return False
            
            # Close file descriptor before rename (Windows requirement)
            os.close(temp_fd)
            temp_fd = None
            
            # Perform atomic move with Windows-specific retry logic
            return self._atomic_move_windows(temp_path, filepath)
            
        except Exception as e:
            logger.error("Atomic write failed",
                        target_file=str(filepath),
                        temp_file=str(temp_path) if temp_path else None,
                        error=str(e),
                        error_type=type(e).__name__)
            return False
        finally:
            # Cleanup
            if temp_fd is not None:
                try:
                    os.close(temp_fd)
                except:
                    pass
            
            if temp_path and temp_path.exists():
                try:
                    temp_path.unlink()
                    logger.debug("Temp file cleaned up", temp_file=str(temp_path))
                except Exception as cleanup_error:
                    logger.warning("Temp file cleanup failed",
                                 temp_file=str(temp_path),
                                 cleanup_error=str(cleanup_error))
    
    def _write_temp_file(self, temp_fd: int, data: Union[str, bytes], 
                        encoding: str, is_binary: bool) -> bool:
        """Write data to temporary file with proper encoding"""
        try:
            if is_binary:
                if isinstance(data, str):
                    data = data.encode(encoding)
                os.write(temp_fd, data)
            else:
                if isinstance(data, bytes):
                    data = data.decode(encoding)
                os.write(temp_fd, data.encode(encoding))
            
            # Force data to disk
            os.fsync(temp_fd)
            return True
            
        except Exception as e:
            logger.error("Failed to write temp file", error=str(e))
            return False
    
    def _atomic_move_windows(self, src: Path, dst: Path) -> bool:
        """Windows-specific atomic move with comprehensive retry logic"""
        
        for attempt in range(self.max_retries):
            try:
                # Method 1: Try standard os.replace (fastest)
                os.replace(str(src), str(dst))
                logger.info("Atomic write completed successfully",
                           target_file=str(dst),
                           attempts=attempt + 1)
                return True
                
            except PermissionError as e:
                if self._is_sharing_violation(e):
                    # File is locked by another process - wait and retry
                    delay = self._calculate_backoff_delay(attempt)
                    logger.debug("File sharing violation, retrying",
                               target_file=str(dst),
                               attempt=attempt + 1,
                               delay=delay,
                               error=str(e))
                    time.sleep(delay)
                    continue
                else:
                    # Permission issue - try Win32 fallback
                    if self._try_win32_move(src, dst):
                        logger.info("Atomic write completed via Win32 API",
                                   target_file=str(dst),
                                   attempts=attempt + 1)
                        return True
                    
                    # If Win32 also fails, continue retry loop
                    delay = self._calculate_backoff_delay(attempt)
                    time.sleep(delay)
                    continue
                    
            except FileExistsError:
                # Target exists - try to remove and retry
                try:
                    dst.unlink()
                    logger.debug("Removed existing target file", target_file=str(dst))
                    continue
                except Exception as unlink_error:
                    logger.debug("Failed to remove existing file",
                               target_file=str(dst),
                               error=str(unlink_error))
                    delay = self._calculate_backoff_delay(attempt)
                    time.sleep(delay)
                    continue
                    
            except OSError as e:
                if hasattr(e, 'winerror'):
                    if e.winerror == 32:  # ERROR_SHARING_VIOLATION
                        delay = self._calculate_backoff_delay(attempt)
                        logger.debug("Windows sharing violation, retrying",
                                   target_file=str(dst),
                                   attempt=attempt + 1,
                                   delay=delay)
                        time.sleep(delay)
                        continue
                    elif e.winerror == 5:  # ERROR_ACCESS_DENIED
                        # Try Win32 approach
                        if self._try_win32_move(src, dst):
                            return True
                        delay = self._calculate_backoff_delay(attempt)
                        time.sleep(delay)
                        continue
                
                # Unknown OS error - log and retry
                logger.warning("OS error during atomic move",
                             target_file=str(dst),
                             attempt=attempt + 1,
                             error=str(e))
                delay = self._calculate_backoff_delay(attempt)
                time.sleep(delay)
                continue
                
            except Exception as e:
                # Unexpected error
                logger.warning("Unexpected error during atomic move",
                             target_file=str(dst),
                             attempt=attempt + 1,
                             error=str(e),
                             error_type=type(e).__name__)
                if attempt == self.max_retries - 1:
                    break
                delay = self._calculate_backoff_delay(attempt)
                time.sleep(delay)
                continue
        
        logger.error("Atomic move failed after all retries",
                    source_file=str(src),
                    target_file=str(dst),
                    max_retries=self.max_retries)
        return False
    
    def _try_win32_move(self, src: Path, dst: Path) -> bool:
        """Fallback using Win32 API for stubborn files"""
        if not HAS_PYWIN32:
            return False
        
        try:
            # Use MoveFileEx with replace existing flag
            win32file.MoveFileEx(
                str(src),
                str(dst),
                win32file.MOVEFILE_REPLACE_EXISTING | win32file.MOVEFILE_WRITE_THROUGH
            )
            return True
        except Exception as e:
            logger.debug("Win32 move failed", error=str(e))
            return False
    
    def _is_sharing_violation(self, error: PermissionError) -> bool:
        """Check if error is a Windows file sharing violation"""
        error_msg = str(error).lower()
        sharing_indicators = [
            "being used by another process",
            "sharing violation",
            "access is denied"
        ]
        return any(indicator in error_msg for indicator in sharing_indicators)
    
    def _calculate_backoff_delay(self, attempt: int) -> float:
        """Calculate exponential backoff delay with jitter"""
        base_delay = self.base_delay * (self.backoff_multiplier ** attempt)
        # Add some jitter (Â±20%)
        import random
        jitter = random.uniform(0.8, 1.2)
        delay = min(base_delay * jitter, self.max_delay)
        return delay

# Global instance for convenience
_windows_writer = None

def get_windows_atomic_writer() -> WindowsAtomicWriter:
    """Get singleton Windows atomic writer instance"""
    global _windows_writer
    if _windows_writer is None:
        _windows_writer = WindowsAtomicWriter()
    return _windows_writer

def atomic_write_windows(filepath: Union[str, Path], data: Union[str, bytes],
                        encoding: str = 'utf-8', mode: str = 'w') -> bool:
    """
    Convenience function for Windows-optimized atomic writes
    
    This is a drop-in replacement for the original atomic_write function
    with Windows-specific optimizations.
    """
    writer = get_windows_atomic_writer()
    return writer.atomic_write(filepath, data, encoding, mode)
```
  ```
  --- END OF FILE scribe/core\windows_atomic_write.py ---
  --- START OF FILE scribe/deployment\docker\Dockerfile ---
  ```
  ```
# Scribe Engine v2.0 Production Dockerfile
# HMA v2.2 compliant containerized deployment

FROM python:3.12-slim as builder

# Set build arguments
ARG BUILD_VERSION=2.0.0
ARG BUILD_DATE
ARG VCS_REF

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create build directory
WORKDIR /build

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

# Production stage
FROM python:3.12-slim as production

# Set metadata labels
LABEL maintainer="Scribe Engine Development Team" \
      version="${BUILD_VERSION}" \
      description="Scribe Engine v2.0 - HMA v2.2 compliant automation engine" \
      build.date="${BUILD_DATE}" \
      build.version="${BUILD_VERSION}" \
      build.vcs-ref="${VCS_REF}"

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    ca-certificates \
    curl \
    procps \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Create non-root user for security
RUN groupadd -r scribe && useradd -r -g scribe -d /app -s /bin/bash scribe

# Set working directory
WORKDIR /app

# Copy Python dependencies from builder
COPY --from=builder /root/.local /home/scribe/.local

# Copy application code
COPY --chown=scribe:scribe tools/scribe/ ./scribe/
COPY --chown=scribe:scribe tools/reports/ ./reports/

# Create necessary directories
RUN mkdir -p /app/logs /app/data /app/config /app/tmp && \
    chown -R scribe:scribe /app

# Copy configuration files
COPY --chown=scribe:scribe tools/scribe/config/ ./config/
COPY --chown=scribe:scribe tools/scribe/schemas/ ./schemas/

# Set environment variables
ENV PYTHONPATH=/app \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    SCRIBE_CONFIG_PATH=/app/config/config.json \
    SCRIBE_LOG_LEVEL=INFO \
    SCRIBE_DATA_DIR=/app/data \
    SCRIBE_LOG_DIR=/app/logs \
    SCRIBE_TEMP_DIR=/app/tmp

# Switch to non-root user
USER scribe

# Add local bin to PATH
ENV PATH=/home/scribe/.local/bin:$PATH

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:9469/health || exit 1

# Expose health check port
EXPOSE 9469

# Default command
CMD ["python", "-m", "scribe.engine", "--config", "/app/config/config.json"]
```
  ```
  --- END OF FILE scribe/deployment\docker\Dockerfile ---
  --- START OF FILE scribe/deployment\docker\docker-compose.yml ---
  ```
  ```yml
version: '3.8'

services:
  scribe-engine:
    build:
      context: ../../..
      dockerfile: tools/scribe/deployment/docker/Dockerfile
      args:
        BUILD_VERSION: "2.0.0"
        BUILD_DATE: ${BUILD_DATE:-}
        VCS_REF: ${VCS_REF:-}
    
    image: scribe-engine:2.0.0
    
    container_name: scribe-engine
    
    restart: unless-stopped
    
    environment:
      - SCRIBE_CONFIG_PATH=/app/config/config.json
      - SCRIBE_LOG_LEVEL=${SCRIBE_LOG_LEVEL:-INFO}
      - SCRIBE_HEALTH_PORT=${SCRIBE_HEALTH_PORT:-9469}
      - SCRIBE_TELEMETRY_ENDPOINT=${SCRIBE_TELEMETRY_ENDPOINT:-}
      - SCRIBE_MTLS_ENABLED=${SCRIBE_MTLS_ENABLED:-false}
    
    ports:
      - "${SCRIBE_HEALTH_PORT:-9469}:9469"
    
    volumes:
      # Configuration
      - ./config:/app/config:ro
      - ./schemas:/app/schemas:ro
      
      # Data directories
      - scribe-data:/app/data
      - scribe-logs:/app/logs
      - scribe-tmp:/app/tmp
      
      # Watch directories (customize as needed)
      - ${SCRIBE_WATCH_DIR:-./watch}:/app/watch:ro
      
      # SSL certificates (if using mTLS)
      - ${SCRIBE_CERTS_DIR:-./certs}:/app/certs:ro
    
    networks:
      - scribe-network
    
    depends_on:
      - scribe-telemetry
      - scribe-monitoring
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9469/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    
    security_opt:
      - no-new-privileges:true
    
    tmpfs:
      - /tmp:noexec,nosuid,size=100m

  # OpenTelemetry Collector for telemetry
  scribe-telemetry:
    image: otel/opentelemetry-collector-contrib:latest
    
    container_name: scribe-telemetry
    
    restart: unless-stopped
    
    command: ["--config=/etc/otel-collector-config.yaml"]
    
    volumes:
      - ./telemetry/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "8888:8888"   # Prometheus metrics
      - "8889:8889"   # Prometheus exporter metrics
    
    networks:
      - scribe-network
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Prometheus for metrics collection
  scribe-monitoring:
    image: prom/prometheus:latest
    
    container_name: scribe-monitoring
    
    restart: unless-stopped
    
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=7d'
      - '--web.enable-lifecycle'
    
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    
    ports:
      - "9090:9090"
    
    networks:
      - scribe-network
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Grafana for metrics visualization
  scribe-grafana:
    image: grafana/grafana:latest
    
    container_name: scribe-grafana
    
    restart: unless-stopped
    
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    
    volumes:
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
      - grafana-data:/var/lib/grafana
    
    ports:
      - "3000:3000"
    
    networks:
      - scribe-network
    
    depends_on:
      - scribe-monitoring
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Jaeger for distributed tracing
  scribe-tracing:
    image: jaegertracing/all-in-one:latest
    
    container_name: scribe-tracing
    
    restart: unless-stopped
    
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    
    ports:
      - "16686:16686"  # Jaeger UI
      - "14250:14250"  # Jaeger gRPC
    
    networks:
      - scribe-network
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  scribe-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  scribe-data:
    driver: local
  scribe-logs:
    driver: local
  scribe-tmp:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
```
  ```
  --- END OF FILE scribe/deployment\docker\docker-compose.yml ---
  --- START OF FILE scribe/deployment\kubernetes\scribe-deployment.yaml ---
  ```
  ```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: scribe-engine
  namespace: scribe
  labels:
    app: scribe-engine
    version: "2.0.0"
    component: automation-engine
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  
  selector:
    matchLabels:
      app: scribe-engine
  
  template:
    metadata:
      labels:
        app: scribe-engine
        version: "2.0.0"
        component: automation-engine
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9469"
        prometheus.io/path: "/metrics"
    
    spec:
      serviceAccountName: scribe-engine
      
      # Security context
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
      
      # Init container for setup
      initContainers:
      - name: setup
        image: scribe-engine:2.0.0
        command: ['sh', '-c']
        args:
        - |
          echo "Setting up Scribe Engine..."
          mkdir -p /app/data /app/logs /app/tmp
          chmod 755 /app/data /app/logs /app/tmp
          echo "Setup completed"
        volumeMounts:
        - name: data-volume
          mountPath: /app/data
        - name: logs-volume
          mountPath: /app/logs
        - name: tmp-volume
          mountPath: /app/tmp
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
      
      containers:
      - name: scribe-engine
        image: scribe-engine:2.0.0
        imagePullPolicy: IfNotPresent
        
        # Security context
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        
        # Environment variables
        env:
        - name: SCRIBE_CONFIG_PATH
          value: "/app/config/config.json"
        - name: SCRIBE_LOG_LEVEL
          value: "INFO"
        - name: SCRIBE_HEALTH_PORT
          value: "9469"
        - name: SCRIBE_TELEMETRY_ENDPOINT
          value: "http://scribe-telemetry:4317"
        - name: SCRIBE_MTLS_ENABLED
          value: "true"
        - name: KUBERNETES_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        
        # Ports
        ports:
        - name: health
          containerPort: 9469
          protocol: TCP
        
        # Resource requests and limits
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
            ephemeral-storage: "1Gi"
          limits:
            memory: "1Gi"
            cpu: "500m"
            ephemeral-storage: "2Gi"
        
        # Health checks
        livenessProbe:
          httpGet:
            path: /health
            port: health
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /health
            port: health
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        startupProbe:
          httpGet:
            path: /health
            port: health
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 12
        
        # Volume mounts
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
          readOnly: true
        - name: schemas-volume
          mountPath: /app/schemas
          readOnly: true
        - name: data-volume
          mountPath: /app/data
        - name: logs-volume
          mountPath: /app/logs
        - name: tmp-volume
          mountPath: /app/tmp
        - name: watch-volume
          mountPath: /app/watch
          readOnly: true
        - name: certs-volume
          mountPath: /app/certs
          readOnly: true
      
      # Volumes
      volumes:
      - name: config-volume
        configMap:
          name: scribe-config
      - name: schemas-volume
        configMap:
          name: scribe-schemas
      - name: data-volume
        persistentVolumeClaim:
          claimName: scribe-data
      - name: logs-volume
        persistentVolumeClaim:
          claimName: scribe-logs
      - name: tmp-volume
        emptyDir:
          sizeLimit: 1Gi
      - name: watch-volume
        persistentVolumeClaim:
          claimName: scribe-watch
      - name: certs-volume
        secret:
          secretName: scribe-certs
          defaultMode: 0400
      
      # Pod scheduling
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - scribe-engine
              topologyKey: kubernetes.io/hostname
      
      # Tolerations
      tolerations:
      - key: "scribe/dedicated"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"

---
apiVersion: v1
kind: Service
metadata:
  name: scribe-engine
  namespace: scribe
  labels:
    app: scribe-engine
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9469"
    prometheus.io/path: "/metrics"
spec:
  type: ClusterIP
  ports:
  - name: health
    port: 9469
    targetPort: health
    protocol: TCP
  selector:
    app: scribe-engine

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: scribe-engine
  namespace: scribe
  labels:
    app: scribe-engine

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: scribe
  name: scribe-engine
  labels:
    app: scribe-engine
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["events"]
  verbs: ["create"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: scribe-engine
  namespace: scribe
  labels:
    app: scribe-engine
subjects:
- kind: ServiceAccount
  name: scribe-engine
  namespace: scribe
roleRef:
  kind: Role
  name: scribe-engine
  apiGroup: rbac.authorization.k8s.io

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: scribe-data
  namespace: scribe
  labels:
    app: scribe-engine
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: fast-ssd

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: scribe-logs
  namespace: scribe
  labels:
    app: scribe-engine
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
  storageClassName: standard

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: scribe-watch
  namespace: scribe
  labels:
    app: scribe-engine
spec:
  accessModes:
    - ReadOnlyMany
  resources:
    requests:
      storage: 50Gi
  storageClassName: shared-nfs

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: scribe-engine
  namespace: scribe
  labels:
    app: scribe-engine
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: scribe-engine

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: scribe-engine
  namespace: scribe
  labels:
    app: scribe-engine
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: scribe-engine
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
```
  ```
  --- END OF FILE scribe/deployment\kubernetes\scribe-deployment.yaml ---
  --- START OF FILE scribe/docs\decisions\ADR-001-nats-message-broker.md ---
  ```
  ```md
# ADR-001: NATS Message Broker Adoption

## Status
**Accepted** - July 24, 2025

## Context

The Scribe engine previously used a simplistic in-memory `queue.Queue` for inter-component communication. This approach had several limitations:

1. **Scalability**: In-memory queues don't scale across processes or machines
2. **Reliability**: No persistence or delivery guarantees
3. **Observability**: Limited visibility into message flow
4. **HMA Compliance**: HMA v2.2 recommends production-grade message brokers (Tier 2)

The HMA v2.2 specification recommends NATS as a Tier 2 technology for message-oriented middleware.

## Decision

We will adopt **NATS** as the primary message broker for the Scribe engine, replacing the legacy in-memory queue system.

### Implementation Details

1. **Library**: Use `nats-py` client library
2. **Adapter**: Implement `NatsEventBusAdapter` that implements the `EventBusPort` interface
3. **Configuration**: NATS server URL configurable via `config.json`
4. **Fallback**: Maintain backward compatibility methods for legacy tests
5. **Event Structure**: Use HMA-compliant event format with versioning

### Technology Tier Justification

- **Tier**: 2 (Recommended)
- **Category**: Message-Oriented Middleware
- **HMA Compliance**: Full compliance with HMA v2.2 messaging patterns
- **Alternative Considered**: Apache Kafka (heavier, more complex for our use case)

## Consequences

### Positive
- âœ… **Scalability**: Supports distributed deployments
- âœ… **Reliability**: Built-in message persistence and delivery guarantees
- âœ… **Performance**: High-throughput, low-latency messaging
- âœ… **Observability**: Rich metrics and monitoring capabilities
- âœ… **HMA Compliance**: Meets Tier 2 recommendations
- âœ… **Cloud Native**: Integrates well with Kubernetes environments

### Negative
- âŒ **Complexity**: Requires NATS server deployment and management
- âŒ **Dependencies**: External service dependency (can be mitigated with clustering)
- âŒ **Learning Curve**: Team needs to understand NATS concepts and operations

### Migration Impact
- Legacy `queue.Queue` usage replaced with NATS subjects
- Event publishing/subscribing patterns remain the same through port abstraction
- Backward compatibility maintained for existing tests
- Configuration changes required for production deployments

## Implementation Status

- âœ… `NatsEventBusAdapter` implemented in `core/adapters/nats_adapter.py`
- âœ… Integration with engine factory and startup/shutdown lifecycle
- âœ… HMA-compliant event structure with OTEL telemetry
- âœ… Connection management with auto-reconnect capabilities
- âœ… Backward compatibility methods for legacy tests

## Related ADRs
- ADR-002: Ports and Adapters Architecture
- ADR-005: HMA v2.2 Compliance Strategy
```
  ```
  --- END OF FILE scribe/docs\decisions\ADR-001-nats-message-broker.md ---
  --- START OF FILE scribe/docs\decisions\ADR-002-ports-and-adapters.md ---
  ```
  ```md
# ADR-002: Ports and Adapters Architecture Implementation

## Status
**Accepted** - July 24, 2025

## Context

The Scribe engine previously allowed plugins to directly access core components like `SecurityManager` and `ConfigManager`. This created tight coupling and violated HMA v2.2's explicit boundaries principle, which mandates that all interactions must go through well-defined ports.

Key issues with the previous approach:
1. **Tight Coupling**: Plugins had direct dependencies on concrete implementations
2. **Testing Difficulty**: Hard to mock dependencies for unit testing  
3. **HMA Non-Compliance**: Violated Part 3, Sec 9 (explicit boundaries)
4. **Technology Lock-in**: Difficult to swap implementations without changing plugin code

## Decision

We will implement a **strict ports-and-adapters-only interaction policy** throughout the Scribe engine.

### Architecture Changes

1. **Extended Port Definitions**: Define technology-agnostic interfaces for all operations:
   - `CommandExecutionPort`: Secure command execution
   - `FileSystemPort`: File operations with security validation
   - `LoggingPort`: Structured logging interface
   - `PluginContextPort`: Plugin execution context and port access

2. **Plugin Context Injection**: Replace direct dependency injection with port-based access:
   - Plugins receive a `PluginContextPort` instead of concrete dependencies
   - Context provides access to all other ports through `get_port(port_type)`
   - Maintains security and observability boundaries

3. **BaseAction Refactoring**: Update the plugin base class:
   - Constructor accepts `PluginContextPort` instead of direct dependencies
   - Provide convenience methods for common port operations
   - All operations go through ports (no direct SDK calls)

### Port Implementation Strategy

- **Port Registry**: Central registry for all port implementations
- **Adapter Pattern**: Each port has a concrete adapter implementation
- **Boundary Telemetry**: All port interactions include mandatory OTEL spans
- **Security Enforcement**: mTLS and access control at port boundaries

## Consequences

### Positive
- âœ… **HMA Compliance**: Full compliance with explicit boundaries principle
- âœ… **Loose Coupling**: Plugins depend only on port interfaces
- âœ… **Testability**: Easy to mock ports for unit testing
- âœ… **Flexibility**: Can swap implementations without changing plugin code
- âœ… **Observability**: All interactions monitored through port boundaries
- âœ… **Security**: Centralized security controls at port level

### Negative
- âŒ **Complexity**: More abstraction layers and indirection
- âŒ **Performance**: Small overhead from port abstraction (mitigated by modern CPUs)
- âŒ **Migration Effort**: Existing plugins need refactoring

### Developer Experience
- **Learning Curve**: Developers need to understand port concepts
- **Consistency**: Clearer patterns for accessing core functionality
- **Documentation**: Well-defined interfaces improve API documentation

## Implementation Status

### Completed
- âœ… Extended port definitions in `hma_ports.py`
- âœ… Port adapter implementations in `port_adapters.py`
- âœ… Updated engine factory to register all ports
- âœ… Refactored `BaseAction` class for port-based access
- âœ… Updated plugin loader to use context injection
- âœ… Converted `run_command_action.py` as reference implementation

### Port Adapters Implemented
- `ScribeCommandExecutionAdapter`: Secure command execution with OTEL telemetry
- `ScribeFileSystemAdapter`: File operations with security validation
- `ScribeLoggingAdapter`: Structured logging with metrics
- `ScribePluginContextAdapter`: Plugin context and port access

### Example Usage

```python
# Old approach (direct dependencies)
class MyAction(BaseAction):
    def __init__(self, action_type, params, config_manager, security_manager):
        self.config_manager = config_manager
        self.security_manager = security_manager
    
    def execute(self, ...):
        result = self.security_manager.execute_command_safely(cmd)

# New approach (port-based)
class MyAction(BaseAction):
    def __init__(self, action_type, params, plugin_context):
        super().__init__(action_type, params, plugin_context)
    
    async def execute(self, ...):
        result = await self.execute_command_safely(cmd)  # Through port
```

## Related ADRs
- ADR-001: NATS Message Broker Adoption
- ADR-005: HMA v2.2 Compliance Strategy
```
  ```
  --- END OF FILE scribe/docs\decisions\ADR-002-ports-and-adapters.md ---
  --- START OF FILE scribe/docs\decisions\ADR-003-shacl-json-bridge.md ---
  ```
  ```md
# ADR-003: SHACL to JSON Schema Compliance Bridge

## Status
**Accepted** - July 24, 2025

## Context

The Enhanced Frontmatter Action uses SHACL (Shapes Constraint Language) for semantic validation of frontmatter structures. While SHACL provides powerful RDF-based validation capabilities, it falls into HMA v2.2's Tier 3 (Alternative) technologies.

HMA v2.2 requires that Tier 3 technologies implement "compliance adapters" to bridge to Tier 1 mandatory standards. The challenge was to maintain SHACL's semantic validation power while ensuring interoperability with JSON Schema validation (Tier 1 mandatory).

### Problem Statement
- SHACL validation produces RDF-based reports that are not compatible with standard tooling
- HMA v2.2 mandates JSON Schema validation as Tier 1 technology
- Need to preserve SHACL's advanced validation capabilities while ensuring compliance

## Decision

We will implement a **SHACLToJSONSchemaAdapter** that transforms SHACL validation results into HMA v2.2 compliant JSON Schema validation reports.

### Adapter Architecture

1. **Compliance Bridge Pattern**: Transform Tier 3 outputs to Tier 1 formats
2. **Standardized Results**: Convert SHACL violation reports to standardized validation results
3. **HMA Report Format**: Generate HMA v2.2 compliant validation reports with full metadata
4. **Bidirectional Support**: Support both SHACL-first and JSON Schema-first workflows

### Technical Implementation

```python
# Core components
class ValidationResult:         # Standardized violation structure
class ComplianceReport:         # HMA v2.2 compliant report format
class SHACLToJSONSchemaAdapter: # Main bridge implementation

# Key methods
def transform_shacl_report()           # Convert SHACL to standard format
def create_json_schema_report()        # Generate JSON Schema report
def validate_with_compliance_bridge()  # End-to-end validation
```

### HMA Compliance Strategy

- **Input Tier**: 3 (Alternative) - SHACL RDF graphs
- **Output Tier**: 1 (Mandatory) - JSON Schema validation reports
- **Bridge Type**: Technology compliance adapter
- **Compliance Level**: Full HMA v2.2 compliance through transformation

## Consequences

### Positive
- âœ… **HMA Compliance**: Maintains Tier 1 compliance while using Tier 3 technology
- âœ… **Tool Interoperability**: JSON Schema reports work with standard tooling
- âœ… **Semantic Power**: Retains SHACL's advanced validation capabilities
- âœ… **Standardization**: Consistent validation report format across the system
- âœ… **Flexibility**: Can switch between SHACL and pure JSON Schema as needed

### Negative
- âŒ **Complexity**: Additional transformation layer adds complexity
- âŒ **Performance**: Some overhead from format transformation
- âŒ **Maintenance**: Need to maintain mapping between SHACL and JSON Schema concepts

### Use Cases Enabled
- Advanced semantic validation with standards compliance
- Integration with CI/CD pipelines expecting JSON Schema reports
- Compatibility with monitoring and alerting systems
- Future migration path from SHACL to pure JSON Schema if needed

## Implementation Status

### Completed Components
- âœ… `ValidationResult` dataclass for standardized violations
- âœ… `ComplianceReport` dataclass for HMA v2.2 reports
- âœ… `SHACLToJSONSchemaAdapter` main implementation
- âœ… SHACL violation extraction and transformation logic
- âœ… JSON Schema report generation
- âœ… Error handling and fallback reporting

### Key Features
- **Severity Mapping**: SHACL severities mapped to standard levels (ERROR, WARNING, INFO)
- **Metadata Preservation**: Source locations, constraint components, and focus nodes preserved
- **Report Versioning**: Full report metadata with timestamps and version tracking
- **Error Resilience**: Graceful handling of malformed SHACL reports

### Usage Example

```python
adapter = SHACLToJSONSchemaAdapter()

# Transform existing SHACL report
compliance_report = adapter.transform_shacl_report(shacl_graph, conforms)

# End-to-end validation with compliance bridge
compliance_report = adapter.validate_with_compliance_bridge(
    data, shacl_shapes, report_id="frontmatter_validation"
)

# Generate JSON Schema compatible report
json_report = adapter.create_json_schema_report(compliance_report)
```

## Future Considerations

1. **Performance Optimization**: Consider caching for frequently used shapes
2. **Schema Evolution**: Plan for SHACL to JSON Schema schema migration paths
3. **Validation Enhancement**: Add support for more SHACL constraint types
4. **Integration**: Consider deeper integration with frontmatter processing pipeline

## Related ADRs
- ADR-005: HMA v2.2 Compliance Strategy
- ADR-002: Ports and Adapters Architecture
```
  ```
  --- END OF FILE scribe/docs\decisions\ADR-003-shacl-json-bridge.md ---
  --- START OF FILE scribe/docs\decisions\ADR-004-python-async-concurrency.md ---
  ```
  ```md
# ADR-004: Python Async/Await for Concurrency

## Status
**Accepted** - July 24, 2025

## Context

The Scribe engine previously used a mixed concurrency model with threading and synchronous operations. This created several challenges:

1. **Thread Safety Issues**: Race conditions and deadlocks, especially on Windows
2. **Resource Contention**: Multiple threads competing for I/O resources
3. **Debugging Complexity**: Thread-based debugging is notoriously difficult
4. **Scalability Limits**: Thread overhead limits concurrent operations
5. **Mixed Patterns**: Inconsistent use of sync/async patterns across codebase

The migration to HMA v2.2 provided an opportunity to standardize on a modern concurrency model.

## Decision

We will standardize on **Python's async/await concurrency model** throughout the Scribe engine.

### Implementation Strategy

1. **Core Components**: All major components support async operations
   - Plugin execution through ports is async
   - File operations are async where beneficial
   - Network operations (NATS, HTTP) are naturally async

2. **Plugin Interface**: Plugins can implement either sync or async execution
   - `execute()` method for synchronous plugins (legacy compatibility)
   - `execute_async()` method for asynchronous plugins (preferred)
   - Automatic wrapping of sync plugins in thread pools

3. **Event Loop Management**: Dedicated event loops for different components
   - Engine startup/shutdown uses temporary event loops
   - NATS adapter manages its own event loop context
   - Proper cleanup and resource management

### Backward Compatibility

- Legacy synchronous plugins continue to work
- Automatic thread pool execution for sync plugins
- Gradual migration path from sync to async

## Consequences

### Positive
- âœ… **Performance**: Better I/O concurrency and resource utilization
- âœ… **Scalability**: Can handle many more concurrent operations
- âœ… **Reliability**: Eliminates many race conditions and deadlocks
- âœ… **Modern Python**: Aligns with modern Python best practices
- âœ… **Integration**: Better integration with async libraries (NATS, HTTP clients)
- âœ… **Debugging**: Easier to debug than thread-based code

### Negative
- âŒ **Learning Curve**: Team needs async/await expertise
- âŒ **Complexity**: Async code can be more complex to write correctly
- âŒ **Library Constraints**: Some libraries don't support async operations
- âŒ **Event Loop Management**: Need careful event loop lifecycle management

### Migration Challenges
- Converting existing synchronous code to async
- Managing mixed sync/async interfaces during transition
- Ensuring proper error handling in async contexts

## Implementation Status

### Completed Async Components
- âœ… **Plugin Execution**: `execute_plugin()` is async with automatic sync plugin wrapping
- âœ… **NATS Adapter**: Full async implementation with proper connection management
- âœ… **Port Interfaces**: Most port methods are async for I/O operations
- âœ… **Engine Lifecycle**: Async startup/shutdown with proper resource cleanup
- âœ… **File Processing Orchestrator**: Full async workflow coordination

### Async Patterns Implemented

#### Plugin Execution
```python
# Async plugin
async def execute_async(self, input_data):
    result = await self.some_async_operation()
    return result

# Legacy sync plugin (automatically wrapped)
def execute(self, file_content, match, file_path, params):
    return self.process_synchronously()
```

#### Port Operations
```python
# Command execution through port
async def execute_command_safely(self, command_list, **kwargs):
    command_port = self.get_command_port()
    return await command_port.execute_command_safely(command_list, **kwargs)
```

#### Event Loop Management
```python
# Engine startup
loop = asyncio.new_event_loop()
asyncio.set_event_loop(loop)
try:
    success = loop.run_until_complete(self.initialize_minimal_core())
finally:
    loop.close()
```

### Synchronous Compatibility
- Legacy plugins automatically executed in thread pools
- Synchronous methods available where async isn't beneficial
- Gradual migration without breaking existing functionality

## Best Practices Established

1. **Async by Default**: New code should use async/await unless there's a specific reason not to
2. **Proper Error Handling**: Use try/except blocks in async contexts
3. **Resource Cleanup**: Always use proper async context managers and cleanup
4. **Event Loop Isolation**: Don't share event loops between unrelated components
5. **Timeout Handling**: Always specify timeouts for async operations

## Performance Impact

### Positive
- **I/O Bound Operations**: Significant performance improvement for file and network operations
- **Concurrent Processing**: Can handle many more simultaneous plugin executions
- **Memory Efficiency**: Lower memory overhead compared to thread-based concurrency

### Benchmarks (Estimated)
- **Plugin Execution**: 3-5x improvement for I/O heavy plugins
- **Event Processing**: 10x improvement in event throughput
- **Memory Usage**: 40-60% reduction in memory overhead for concurrent operations

## Related ADRs
- ADR-001: NATS Message Broker Adoption (benefits from async)
- ADR-002: Ports and Adapters Architecture (async port interfaces)
- ADR-005: HMA v2.2 Compliance Strategy (modern architecture)
```
  ```
  --- END OF FILE scribe/docs\decisions\ADR-004-python-async-concurrency.md ---
  --- START OF FILE scribe/docs\decisions\ADR-005-hma-compliance-strategy.md ---
  ```
  ```md
# ADR-005: HMA v2.2 Compliance Strategy

## Status
**Accepted** - July 24, 2025

## Context

The Scribe engine underwent a comprehensive compliance audit against the Hexagonal Microkernel Architecture (HMA) v2.2 specification. The audit (SCRIBE-QA-2025-003) identified significant gaps in compliance, particularly around:

1. **Architectural Violations**: God object anti-patterns, missing minimalist core
2. **Technology Tier Misalignment**: Use of Tier 3 technologies without compliance bridges
3. **Mandatory Standards Gaps**: Incomplete OpenTelemetry and mTLS implementation
4. **Plugin Management**: Non-compliant manifests and validation

A comprehensive compliance strategy was needed to achieve full HMA v2.2 compliance while maintaining system functionality.

## Decision

We will implement a **comprehensive HMA v2.2 compliance strategy** following the three-phase remediation roadmap identified in the audit.

### Three-Tier Technology Framework

**Tier 1 (Mandatory)**
- âœ… **OpenTelemetry**: Boundary telemetry on all port interactions
- âœ… **mTLS**: Mutual TLS for inter-plugin communication  
- âœ… **JSON Schema**: Schema validation and compliance reporting

**Tier 2 (Recommended)**
- âœ… **NATS**: Message broker replacing in-memory queues
- âœ… **Structured Logging**: Standardized logging with context
- ðŸ”„ **Kubernetes**: Container orchestration (future implementation)
- ðŸ”„ **Vault**: Secrets management (future implementation)

**Tier 3 (Alternative with Compliance Bridges)**
- âœ… **SHACL**: Semantic validation with JSON Schema bridge

### Architectural Compliance

1. **Minimalist Core**: Refactored engine.py to pure L2 Core responsibilities
2. **Factory Pattern**: Externalized component creation to eliminate god objects
3. **Ports-Only Interactions**: Strict enforcement of port-based boundaries
4. **Layer Separation**: Clear L1/L2/L3 layer boundaries with proper responsibilities

### Implementation Phases

**Phase 1: Critical Compliance & Security**
- âœ… SEC-001: Dependency vulnerability remediation
- âœ… SEC-002: Externalized security policies  
- âœ… ARC-001: Minimalist core refactoring
- âœ… HMA-001: Full OTEL boundary telemetry and mTLS

**Phase 2: Technology Adoption & Architecture**
- âœ… HMA-002: NATS message broker implementation
- âœ… ARC-002: Ports-and-adapters-only enforcement
- âœ… HMA-003: Plugin manifest v2.2 compliance

**Phase 3: Pattern Completion & Cleanup**
- âœ… ARC-004: FileProcessingOrchestrator L2 plugin
- âœ… HMA-004: SHACL compliance bridge implementation
- âœ… BUG-002: TODO tracking and cleanup
- âœ… HMA-005: Architecture Decision Records

## Consequences

### Positive
- âœ… **Full HMA Compliance**: Meets all mandatory and recommended standards  
- âœ… **Future-Proof Architecture**: Modern, scalable, maintainable codebase
- âœ… **Technology Flexibility**: Clear guidelines for technology selection
- âœ… **Operational Excellence**: Enhanced observability, security, and reliability
- âœ… **Developer Experience**: Clear patterns and well-documented architecture

### Negative
- âŒ **Implementation Effort**: Significant refactoring and development work
- âŒ **Learning Curve**: Team needs to understand HMA v2.2 principles
- âŒ **Complexity**: More architectural layers and abstractions
- âŒ **Operational Overhead**: Additional components to deploy and manage

## Implementation Status

### Compliance Metrics

| Category | Status | Compliance Level |
|----------|--------|------------------|
| **Architecture** | âœ… Complete | 100% |
| **Technology Tiers** | âœ… Complete | 100% |
| **Security** | âœ… Complete | 100% |
| **Observability** | âœ… Complete | 100% |
| **Plugin Management** | âœ… Complete | 100% |

### Technical Achievements

**Architectural Compliance**
- âœ… Minimalist L2 Core implementation
- âœ… Factory pattern for dependency injection
- âœ… Strict ports-and-adapters boundaries
- âœ… L2 Orchestrator plugin pattern

**Technology Compliance** 
- âœ… Tier 1: OpenTelemetry + mTLS + JSON Schema
- âœ… Tier 2: NATS + Structured Logging  
- âœ… Tier 3: SHACL with compliance bridge

**Security Hardening**
- âœ… Dependency vulnerability remediation
- âœ… Externalized security policies
- âœ… mTLS enforcement at boundaries
- âœ… Secure command execution patterns

**Observability Enhancement**
- âœ… OTEL spans on all boundary crossings
- âœ… Structured logging with context
- âœ… Comprehensive metrics and monitoring
- âœ… Distributed tracing support

### Verification Methods

1. **Automated Compliance Checks**: Plugin manifest validation
2. **Architecture Reviews**: Regular review against HMA principles  
3. **Security Audits**: Periodic security vulnerability assessments
4. **Performance Monitoring**: OTEL metrics and distributed tracing
5. **Documentation Maintenance**: ADRs and technical documentation

## Long-term Benefits

### Operational Excellence
- **Scalability**: Horizontal scaling with Kubernetes and NATS
- **Reliability**: Circuit breakers, retries, and graceful degradation
- **Observability**: Full visibility into system behavior and performance
- **Security**: Defense in depth with mTLS, secure boundaries, and audit trails

### Developer Productivity  
- **Clear Patterns**: Well-defined architectural patterns and guidelines
- **Testing**: Easy mocking and testing through port interfaces
- **Debugging**: Distributed tracing for complex workflow debugging
- **Onboarding**: Comprehensive documentation and examples

### Business Value
- **Compliance**: Industry-standard architecture and security practices
- **Maintainability**: Modern, well-structured codebase
- **Extensibility**: Easy to add new plugins and capabilities
- **Integration**: Standard interfaces for third-party integrations

## Related ADRs
- ADR-001: NATS Message Broker Adoption
- ADR-002: Ports and Adapters Architecture  
- ADR-003: SHACL to JSON Schema Bridge
- ADR-004: Python Async/Await Concurrency

## Audit Resolution

**Original Audit**: SCRIBE-QA-2025-003 identified 21 issues across 4 categories  
**Resolution Status**: âœ… **All 21 issues resolved**  
**Compliance Level**: âœ… **100% HMA v2.2 compliant**  
**Verification Date**: July 24, 2025
```
  ```
  --- END OF FILE scribe/docs\decisions\ADR-005-hma-compliance-strategy.md ---
  --- START OF FILE scribe/docs\decisions\README.md ---
  ```
  ```md
# Architecture Decision Records (ADRs)

## Overview

This directory contains Architecture Decision Records (ADRs) documenting significant technology and architectural decisions made in the Scribe engine. These ADRs ensure transparency and provide context for future maintenance and evolution.

## ADR Format

Each ADR follows the standard template:

1. **Status**: Proposed, Accepted, Deprecated, Superseded
2. **Context**: The issue motivating this decision
3. **Decision**: The change we're proposing or have agreed to implement
4. **Consequences**: What becomes easier or more difficult to do because of this change

## ADR Index

| ID | Title | Status | Date |
|----|--------|--------|------|
| ADR-001 | [NATS Message Broker Adoption](ADR-001-nats-message-broker.md) | Accepted | 2025-07-24 |
| ADR-002 | [Ports and Adapters Architecture](ADR-002-ports-and-adapters.md) | Accepted | 2025-07-24 |
| ADR-003 | [SHACL to JSON Schema Bridge](ADR-003-shacl-json-bridge.md) | Accepted | 2025-07-24 |
| ADR-004 | [Python Async/Await for Concurrency](ADR-004-python-async-concurrency.md) | Accepted | 2025-07-24 |
| ADR-005 | [HMA v2.2 Compliance Strategy](ADR-005-hma-compliance-strategy.md) | Accepted | 2025-07-24 |

## HMA v2.2 Compliance

These ADRs document compliance with the Hexagonal Microkernel Architecture (HMA) v2.2 specification and justify technology choices within the three-tier framework:

- **Tier 1 (Mandatory)**: OpenTelemetry, mTLS, JSON Schema
- **Tier 2 (Recommended)**: NATS, Kubernetes, Vault, Structured Logging
- **Tier 3 (Alternative)**: SHACL, Custom implementations with compliance bridges
```
  ```
  --- END OF FILE scribe/docs\decisions\README.md ---
  --- START OF FILE scribe/engine.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Engine v2.2 - HMA Compliant Minimalist Core

This is the main entry point for the Scribe v2.2 automation engine.
It implements HMA v2.2 compliant architecture with true minimalist core,
following dependency injection principles and removing god object anti-patterns.
"""

import sys
import signal
import threading
import time
import asyncio
from typing import List, Optional, Dict, Any

# HMA v2.2 Core Components
from tools.scribe.core.minimal_core import HMAMinimalCore, CoreState
from tools.scribe.core.engine_factory import create_engine_components, EngineComponents
from tools.scribe.core.logging_config import configure_structured_logging, get_scribe_logger

# Configure structured logging
configure_structured_logging(log_level="INFO", include_stdlib_logs=True)
logger = get_scribe_logger(__name__)


class ScribeEngine:
    """
    HMA v2.2 Compliant Scribe Engine - Minimalist Core
    
    This class implements the HMA v2.2 architecture with:
    - True minimal core focused only on routing and lifecycle management
    - Dependency injection to eliminate god object anti-patterns
    - No component instantiation (handled by factory)
    - Pure L2 Core responsibilities only
    """
    
    def __init__(self, 
                 components: Optional[EngineComponents] = None,
                 config_path: str = "tools/scribe/config/config.json",
                 telemetry_config: Optional[Dict[str, Any]] = None,
                 health_port: int = 9090):
        """
        Initialize the HMA v2.2 compliant minimalist Scribe engine.
        
        Args:
            components: Pre-created engine components (via factory)
            config_path: Path to configuration file (fallback if components not provided)
            telemetry_config: Telemetry configuration (fallback if components not provided)
            health_port: Health server port (fallback if components not provided)
        """
        # Create components via factory if not provided (maintaining backward compatibility)
        if components is None:
            logger.info("No components provided, creating via factory")
            components = create_engine_components(
                config_path=config_path,
                telemetry_config=telemetry_config or {},
                health_port=health_port
            )
        
        # Injected components (no instantiation in core)
        self.components = components
        self.port_registry = components.port_registry
        self.minimal_core = None
        self.shutdown_event = threading.Event()
        
        # State management (minimal core responsibilities only)
        self.start_time = None
        self.is_running = False
        self.initialization_complete = False
        
        logger.info("Scribe Engine v2.2 minimalist core initialized")
    
    async def initialize_minimal_core(self) -> bool:
        """Initialize HMA v2.2 minimal core (components already created by factory)"""
        try:
            # Components already created by factory, just initialize minimal core
            core_config = self.components.config_manager.get_engine_settings()
            self.minimal_core = HMAMinimalCore(self.port_registry, core_config)
            
            # Load plugins through the existing plugin loader
            await self._load_plugins(self.components.plugin_loader)
            
            self.initialization_complete = True
            logger.info("HMA v2.2 minimal core initialized successfully")
            return True
            
        except Exception as e:
            logger.error("HMA minimal core initialization failed", error=str(e), exc_info=True)
            return False
    
    def start(self) -> None:
        """Start the HMA v2.2 compliant minimalist Scribe engine."""
        if self.is_running:
            logger.warning("Engine is already running")
            return
        
        try:
            self.start_time = time.time()
            
            logger.info("Starting Scribe Engine v2.2 minimalist core")
            
            # Initialize minimal core asynchronously
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                # Start NATS event bus adapter
                event_bus = self.port_registry.get_port("event_bus")
                if event_bus and hasattr(event_bus, 'start'):
                    if not loop.run_until_complete(event_bus.start()):
                        logger.warning("Failed to start NATS event bus - continuing with limited functionality")
                
                # Initialize minimal core (components already created)
                if not loop.run_until_complete(self.initialize_minimal_core()):
                    raise RuntimeError("Failed to initialize minimal core")
                
                # Start minimal core
                if not loop.run_until_complete(self.minimal_core.start()):
                    raise RuntimeError("Failed to start minimal core")
                
                self.is_running = True
                logger.info("Scribe Engine v2.2 started successfully")
                
            finally:
                loop.close()
            
        except Exception as e:
            logger.error("Failed to start Scribe Engine v2.2", error=str(e), exc_info=True)
            self.stop()
            raise
    
    def stop(self) -> None:
        """Stop the Scribe engine gracefully."""
        if not self.is_running:
            logger.warning("Engine is not running")
            return
        
        logger.info("Stopping Scribe engine")
        
        try:
            # Signal shutdown to all threads
            self.shutdown_event.set()
            
            # Stop HMA minimal core
            if self.minimal_core:
                logger.info("Stopping minimal core")
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    loop.run_until_complete(self.minimal_core.stop())
                finally:
                    loop.close()
            
            # Stop NATS event bus adapter
            if self.port_registry:
                event_bus = self.port_registry.get_port("event_bus")
                if event_bus and hasattr(event_bus, 'stop'):
                    # NATS adapter requires async stop
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        loop.run_until_complete(event_bus.stop())
                    finally:
                        loop.close()
            
            # Log final statistics
            self._log_final_stats()
            
            self.is_running = False
            logger.info("Scribe engine stopped successfully")
            
        except Exception as e:
            logger.error("Error during engine shutdown", error=str(e), exc_info=True)
    
    def _log_final_stats(self) -> None:
        """Log final engine statistics."""
        if self.start_time:
            uptime = time.time() - self.start_time
            
            stats = {
                "engine_uptime_seconds": round(uptime, 2),
                "initialization_complete": self.initialization_complete
            }
            
            # Add event bus stats if available
            if self.port_registry:
                event_bus = self.port_registry.get_port("event_bus")
                if event_bus and hasattr(event_bus, 'get_event_statistics'):
                    stats["event_bus_stats"] = event_bus.get_event_statistics()
            
            # Add core stats if available
            if self.minimal_core:
                stats["core_status"] = self.minimal_core.get_core_status()
            
            logger.info("Engine final statistics", **stats)
    
    def run_forever(self) -> None:
        """
        Run the engine until interrupted.
        
        This method starts the engine and blocks until a shutdown signal is received.
        """
        # Set up signal handlers for graceful shutdown (only in main thread)
        try:
            def signal_handler(signum, frame):
                logger.info("Received shutdown signal", signal=signum)
                self.stop()
            
            signal.signal(signal.SIGINT, signal_handler)
            signal.signal(signal.SIGTERM, signal_handler)
        except ValueError:
            # Not in main thread, signal handling not available
            logger.debug("Signal handling not available in background thread")
        
        try:
            self.start()
            
            # Main loop - wait for shutdown
            while self.is_running and not self.shutdown_event.is_set():
                time.sleep(1.0)  # Check every second
                
        except KeyboardInterrupt:
            logger.info("Received keyboard interrupt")
        except Exception as e:
            logger.error("Unexpected error in main loop", error=str(e), exc_info=True)
        finally:
            self.stop()
    
    def get_status(self) -> dict:
        """
        Get current engine status.
        
        Returns:
            Dictionary with current engine status and statistics
        """
        uptime = time.time() - self.start_time if self.start_time else 0
        
        status = {
            'is_running': self.is_running,
            'uptime_seconds': round(uptime, 2),
            'initialization_complete': self.initialization_complete,
            'engine_version': '2.2.0'
        }
        
        # Add HMA components status
        if self.minimal_core:
            status['hma_core'] = self.minimal_core.get_core_status()
        
        # Add event bus statistics
        if self.port_registry:
            event_bus = self.port_registry.get_port("event_bus")
            if event_bus and hasattr(event_bus, 'get_event_statistics'):
                event_stats = event_bus.get_event_statistics()
                status['queue_size'] = event_stats.get('queue_size', 0)
                status['event_bus_stats'] = event_stats
            else:
                status['queue_size'] = 0
        else:
            status['queue_size'] = 0
        
        # Add telemetry status if available
        if self.components and self.components.telemetry:
            status['telemetry_active'] = True
        
        return status
    
    async def _load_plugins(self, plugin_loader) -> None:
        """Load and register all plugins with the minimal core"""
        try:
            # Load available plugins
            plugins = plugin_loader.load_all_plugins()
            
            # Register each plugin with the minimal core
            for plugin_id, plugin_info in plugins.items():
                try:
                    # Get plugin manifest
                    manifest = plugin_info.manifest
                    if not manifest:
                        logger.warning("Plugin missing manifest, skipping", plugin_id=plugin_id)
                        continue
                    
                    # Register with minimal core
                    success = await self.minimal_core.register_plugin(manifest)
                    if success:
                        logger.info("Plugin registered successfully", 
                                   plugin_id=plugin_id, 
                                   plugin_type=manifest.get("plugin_metadata", {}).get("type"))
                    else:
                        logger.error("Plugin registration failed", plugin_id=plugin_id)
                        
                except Exception as e:
                    logger.error("Plugin loading failed", 
                               plugin_id=plugin_id, 
                               error=str(e))
            
            logger.info("Plugin loading completed", 
                       total_plugins=len(plugins),
                       registered_plugins=len(self.minimal_core.lifecycle_manager.plugins))
            
        except Exception as e:
            logger.error("Plugin loading process failed", error=str(e))
            raise
    


def main():
    """Main entry point for the Scribe engine."""
    logger.info("Scribe Engine v2.2 starting up")
    
    try:
        # Create engine components via factory (HMA v2.2 compliant)
        components = create_engine_components()
        
        # Create minimalist engine with dependency injection
        engine = ScribeEngine(components=components)
        
        engine.run_forever()
        
    except Exception as e:
        logger.error("Fatal error in Scribe engine", error=str(e), exc_info=True)
        sys.exit(1)
    
    logger.info("Scribe Engine v2.2 shutdown complete")


if __name__ == "__main__":
    main() 
```
  ```
  --- END OF FILE scribe/engine.py ---
  --- START OF FILE scribe/error_handling\llm_error_handler.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
LLM Error Handler

Implementation of Phase 3: Step 3.1.4 - Error Handling and Retry Mechanisms
From: ultimate-frontmatter-enhancement-guideline-20250617-0312.md

This module provides comprehensive error handling for LLM frontmatter generation
to ensure 100% success rate through robust retry mechanisms.
"""

import logging
import re
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime
import traceback

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMErrorHandler:
    """
    LLM Error Handler for comprehensive error handling with 100% success rate.
    
    Provides sophisticated error recovery mechanisms to ensure no scenario
    where frontmatter generation fails without a viable solution.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # Error type mappings for classification
        self.error_type_patterns = self._initialize_error_patterns()
        
        # Recovery strategies for different error types
        self.recovery_strategies = self._initialize_recovery_strategies()
        
        # Error statistics for performance monitoring
        self.error_statistics = {
            'total_errors': 0,
            'errors_by_type': {},
            'recovery_success_rate': {},
            'retry_counts': []
        }
    
    def handle_generation_errors(self, error_type: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Step 4: Comprehensive error handling for 100% success rate.
        
        Args:
            error_type: Classification of the error
            context: Error context including relevant information
            
        Returns:
            Error handling result with recovery recommendations
        """
        self.logger.info(f"Handling error type: {error_type}")
        self._record_error_statistics(error_type)
        
        # Route to specific error handler
        error_handlers = {
            'yaml_parsing_error': self._handle_yaml_errors,
            'shacl_validation_error': self._handle_shacl_errors,
            'llm_service_error': self._handle_llm_service_errors,
            'network_error': self._handle_network_errors,
            'timeout_error': self._handle_timeout_errors,
            'authentication_error': self._handle_authentication_errors,
            'rate_limit_error': self._handle_rate_limit_errors,
            'content_policy_error': self._handle_content_policy_errors,
            'unknown_error': self._handle_unknown_error
        }
        
        handler = error_handlers.get(error_type, self._handle_unknown_error)
        
        try:
            recovery_result = handler(context)
            self._record_recovery_success(error_type, True)
            return recovery_result
            
        except Exception as e:
            self.logger.error(f"Error handler failed for {error_type}: {e}")
            self._record_recovery_success(error_type, False)
            
            # Ultimate fallback - always return a viable solution
            return self._ultimate_fallback_handler(error_type, context)
    
    def _handle_yaml_errors(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Handle YAML parsing errors with specific corrections."""
        yaml_error = context.get('error_message', '')
        generated_content = context.get('generated_content', '')
        
        # Common YAML error patterns and fixes
        yaml_fixes = {
            'found character that cannot start any token': self._fix_invalid_yaml_characters,
            'found unexpected end of stream': self._fix_incomplete_yaml,
            'mapping values are not allowed here': self._fix_yaml_mapping_errors,
            'could not find expected': self._fix_yaml_structure_errors,
            'found undefined alias': self._fix_yaml_alias_errors
        }
        
        corrective_instructions = []
        
        # Identify specific YAML issues
        for error_pattern, fix_function in yaml_fixes.items():
            if error_pattern.lower() in yaml_error.lower():
                fix_result = fix_function(generated_content, yaml_error)
                corrective_instructions.extend(fix_result['instructions'])
        
        # Generic YAML corrections if specific fixes not found
        if not corrective_instructions:
            corrective_instructions = [
                "âŒ YAML SYNTAX ERROR - Apply these corrections:",
                "â€¢ Ensure proper indentation (use spaces, not tabs)",
                "â€¢ Check for missing colons after field names",
                "â€¢ Verify all string values are properly quoted if they contain special characters",
                "â€¢ Ensure the frontmatter starts with --- and ends with ---",
                "â€¢ Remove any trailing commas or invalid characters",
                "â€¢ Check for proper list formatting with - prefix for each item"
            ]
        
        return {
            'corrective_prompt': '\n'.join(corrective_instructions),
            'retry_recommended': True,
            'suggested_fixes': corrective_instructions,
            'error_severity': 'medium',
            'recovery_strategy': 'prompt_correction'
        }
    
    def _handle_shacl_errors(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Convert SHACL violations into corrective prompt instructions."""
        violations = context.get('violations', [])
        info_type = context.get('info_type', 'unknown')
        
        corrective_instructions = [
            "âŒ SHACL VALIDATION ERRORS - Correct these violations:",
        ]
        
        # Group violations by type for better organization
        violations_by_type = self._group_violations_by_type(violations)
        
        # Handle missing required fields
        if 'missing_required' in violations_by_type:
            corrective_instructions.append("\nðŸ”´ MISSING REQUIRED FIELDS:")
            for violation in violations_by_type['missing_required']:
                field = violation.get('field', 'unknown')
                message = violation.get('message', 'Field is required')
                corrective_instructions.append(f"â€¢ MUST ADD: {field} - {message}")
        
        # Handle forbidden fields
        if 'forbidden_field' in violations_by_type:
            corrective_instructions.append("\nðŸš« FORBIDDEN FIELDS:")
            for violation in violations_by_type['forbidden_field']:
                field = violation.get('field', 'unknown')
                message = violation.get('message', 'Field not allowed')
                corrective_instructions.append(f"â€¢ MUST REMOVE: {field} - {message}")
        
        # Handle pattern violations
        if 'pattern_violation' in violations_by_type:
            corrective_instructions.append("\nðŸ“ PATTERN VIOLATIONS:")
            for violation in violations_by_type['pattern_violation']:
                field = violation.get('field', 'unknown')
                pattern = violation.get('expected_pattern', 'N/A')
                actual = violation.get('actual_value', 'N/A')
                corrective_instructions.append(f"â€¢ FIX PATTERN: {field} must match '{pattern}', got '{actual}'")
        
        # Handle datatype violations
        if 'datatype_violation' in violations_by_type:
            corrective_instructions.append("\nðŸ”¢ DATATYPE VIOLATIONS:")
            for violation in violations_by_type['datatype_violation']:
                field = violation.get('field', 'unknown')
                expected_type = violation.get('expected_type', 'N/A')
                corrective_instructions.append(f"â€¢ FIX TYPE: {field} must be {expected_type}")
        
        # Add specific guidance for info-type
        corrective_instructions.extend([
            f"\nâœ… REQUIREMENTS FOR {info_type.upper()}:",
            f"â€¢ Ensure 'info-type' field is exactly '{info_type}'",
            "â€¢ Follow all mandatory field requirements",
            "â€¢ Remove any fields not allowed for this document type"
        ])
        
        return {
            'corrective_prompt': '\n'.join(corrective_instructions),
            'retry_recommended': True,
            'violations_summary': violations_by_type,
            'error_severity': 'high',
            'recovery_strategy': 'constraint_correction'
        }
    
    def _handle_llm_service_errors(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Handle LLM service-related errors."""
        service_error = context.get('error_message', '')
        error_code = context.get('error_code', 'unknown')
        
        # Determine retry strategy based on error type
        if 'rate limit' in service_error.lower() or error_code == '429':
            return self._handle_rate_limit_errors(context)
        elif 'authentication' in service_error.lower() or error_code in ['401', '403']:
            return self._handle_authentication_errors(context)
        elif 'timeout' in service_error.lower() or error_code == '408':
            return self._handle_timeout_errors(context)
        else:
            return {
                'corrective_prompt': "LLM service temporarily unavailable. Using deterministic fallback generation.",
                'retry_recommended': False,
                'error_severity': 'high',
                'recovery_strategy': 'deterministic_fallback'
            }
    
    def _handle_network_errors(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Handle network connectivity errors."""
        return {
            'corrective_prompt': "Network connectivity issue detected. Switching to offline generation mode.",
            'retry_recommended': True,
            'retry_delay': 5.0,  # Wait 5 seconds before retry
            'max_retries': 3,
            'error_severity': 'medium',
            'recovery_strategy': 'offline_fallback'
        }
    
    def _handle_timeout_errors(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Handle request timeout errors."""
        timeout_duration = context.get('timeout_duration', 30)
        
        return {
            'corrective_prompt': f"Request timeout after {timeout_duration}s. Using faster generation strategy.",
            'retry_recommended': True,
            'retry_delay': 2.0,
            'max_retries': 2,
            'suggested_prompt_optimization': 'reduce_complexity',
            'error_severity': 'medium',
            'recovery_strategy': 'simplified_generation'
        }
    
    def _handle_authentication_errors(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Handle authentication and authorization errors."""
        return {
            'corrective_prompt': "Authentication error. Using local deterministic generation.",
            'retry_recommended': False,
            'error_severity': 'high',
            'recovery_strategy': 'deterministic_fallback',
            'requires_manual_intervention': True,
            'manual_intervention_message': "Please check LLM service credentials."
        }
    
    def _handle_rate_limit_errors(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Handle rate limiting errors."""
        reset_time = context.get('rate_limit_reset', 60)
        
        return {
            'corrective_prompt': f"Rate limit exceeded. Retrying after {reset_time}s delay.",
            'retry_recommended': True,
            'retry_delay': reset_time,
            'max_retries': 1,
            'error_severity': 'low',
            'recovery_strategy': 'delayed_retry'
        }
    
    def _handle_content_policy_errors(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Handle content policy violations."""
        policy_violation = context.get('policy_violation', 'Unknown policy violation')
        
        return {
            'corrective_prompt': f"Content policy violation: {policy_violation}. Using safe generation mode.",
            'retry_recommended': True,
            'error_severity': 'medium',
            'recovery_strategy': 'safe_generation',
            'prompt_sanitization': True
        }
    
    def _handle_unknown_error(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Handle unclassified errors with generic recovery."""
        error_message = context.get('error_message', 'Unknown error occurred')
        
        self.logger.warning(f"Unknown error encountered: {error_message}")
        
        return {
            'corrective_prompt': f"Unclassified error detected: {error_message}. Using robust fallback generation.",
            'retry_recommended': True,
            'max_retries': 1,
            'error_severity': 'high',
            'recovery_strategy': 'deterministic_fallback'
        }
    
    def _ultimate_fallback_handler(self, error_type: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Ultimate fallback that always succeeds."""
        self.logger.warning(f"Using ultimate fallback for error type: {error_type}")
        
        return {
            'corrective_prompt': "All recovery strategies failed. Generating minimal valid frontmatter.",
            'retry_recommended': False,
            'error_severity': 'critical',
            'recovery_strategy': 'minimal_valid_generation',
            'guaranteed_success': True
        }
    
    def _fix_invalid_yaml_characters(self, content: str, error: str) -> Dict[str, Any]:
        """Fix invalid YAML characters."""
        return {
            'instructions': [
                "â€¢ Remove any non-ASCII characters or special symbols",
                "â€¢ Ensure field names contain only letters, numbers, hyphens, and underscores",
                "â€¢ Quote string values that contain special characters"
            ]
        }
    
    def _fix_incomplete_yaml(self, content: str, error: str) -> Dict[str, Any]:
        """Fix incomplete YAML structure."""
        return {
            'instructions': [
                "â€¢ Ensure the frontmatter ends with --- on its own line",
                "â€¢ Complete any unfinished field definitions",
                "â€¢ Add closing quotes or brackets where needed"
            ]
        }
    
    def _fix_yaml_mapping_errors(self, content: str, error: str) -> Dict[str, Any]:
        """Fix YAML mapping structure errors."""
        return {
            'instructions': [
                "â€¢ Ensure each field has a colon followed by a value",
                "â€¢ Use proper indentation for nested structures",
                "â€¢ Separate list items with proper - prefixes"
            ]
        }
    
    def _fix_yaml_structure_errors(self, content: str, error: str) -> Dict[str, Any]:
        """Fix general YAML structure issues."""
        return {
            'instructions': [
                "â€¢ Check for missing or extra colons",
                "â€¢ Verify proper indentation throughout",
                "â€¢ Ensure all brackets and quotes are properly closed"
            ]
        }
    
    def _fix_yaml_alias_errors(self, content: str, error: str) -> Dict[str, Any]:
        """Fix YAML alias and anchor errors."""
        return {
            'instructions': [
                "â€¢ Remove any YAML aliases (&alias) or references (*reference)",
                "â€¢ Use explicit values instead of aliases",
                "â€¢ Ensure no undefined references exist"
            ]
        }
    
    def _group_violations_by_type(self, violations: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """Group SHACL violations by type for organized handling."""
        grouped = {}
        
        for violation in violations:
            violation_type = violation.get('type', 'unknown')
            
            # Map violation types to categories
            if violation_type == 'missing_required_field':
                category = 'missing_required'
            elif violation_type == 'forbidden_field_present':
                category = 'forbidden_field'
            elif 'pattern' in violation_type.lower():
                category = 'pattern_violation'
            elif 'datatype' in violation_type.lower():
                category = 'datatype_violation'
            else:
                category = 'other'
            
            if category not in grouped:
                grouped[category] = []
            grouped[category].append(violation)
        
        return grouped
    
    def _record_error_statistics(self, error_type: str):
        """Record error statistics for monitoring."""
        self.error_statistics['total_errors'] += 1
        
        if error_type not in self.error_statistics['errors_by_type']:
            self.error_statistics['errors_by_type'][error_type] = 0
        self.error_statistics['errors_by_type'][error_type] += 1
    
    def _record_recovery_success(self, error_type: str, success: bool):
        """Record recovery success statistics."""
        if error_type not in self.error_statistics['recovery_success_rate']:
            self.error_statistics['recovery_success_rate'][error_type] = {'successes': 0, 'total': 0}
        
        self.error_statistics['recovery_success_rate'][error_type]['total'] += 1
        if success:
            self.error_statistics['recovery_success_rate'][error_type]['successes'] += 1
    
    def _initialize_error_patterns(self) -> Dict[str, List[str]]:
        """Initialize error detection patterns."""
        return {
            'yaml_parsing_error': [
                'yaml',
                'syntax error',
                'invalid yaml',
                'parsing error',
                'could not find expected'
            ],
            'shacl_validation_error': [
                'shacl',
                'validation failed',
                'constraint violation',
                'mincount',
                'maxcount',
                'pattern violation'
            ],
            'llm_service_error': [
                'api error',
                'service unavailable',
                'model error',
                'generation failed'
            ],
            'network_error': [
                'connection error',
                'network error',
                'dns error',
                'connection refused'
            ],
            'timeout_error': [
                'timeout',
                'timed out',
                'request timeout'
            ]
        }
    
    def _initialize_recovery_strategies(self) -> Dict[str, Dict[str, Any]]:
        """Initialize recovery strategies for different error types."""
        return {
            'prompt_correction': {
                'description': 'Correct prompt based on error feedback',
                'success_rate': 0.85,
                'retry_recommended': True
            },
            'constraint_correction': {
                'description': 'Fix SHACL constraint violations',
                'success_rate': 0.90,
                'retry_recommended': True
            },
            'deterministic_fallback': {
                'description': 'Use deterministic generation as fallback',
                'success_rate': 1.0,
                'retry_recommended': False
            },
            'delayed_retry': {
                'description': 'Retry with delay for rate limiting',
                'success_rate': 0.95,
                'retry_recommended': True
            }
        }
    
    def get_error_statistics(self) -> Dict[str, Any]:
        """Get comprehensive error handling statistics."""
        stats = self.error_statistics.copy()
        
        # Calculate overall success rates
        total_recoveries = sum(
            data['total'] for data in stats['recovery_success_rate'].values()
        )
        total_successes = sum(
            data['successes'] for data in stats['recovery_success_rate'].values()
        )
        
        stats['overall_recovery_rate'] = (
            total_successes / total_recoveries * 100 if total_recoveries > 0 else 0
        )
        
        return stats
    
    def classify_error(self, error_message: str, context: Dict[str, Any] = None) -> str:
        """Classify an error based on its message and context."""
        if context is None:
            context = {}
            
        error_message_lower = error_message.lower()
        
        for error_type, patterns in self.error_type_patterns.items():
            for pattern in patterns:
                if pattern.lower() in error_message_lower:
                    return error_type
        
        return 'unknown_error'


if __name__ == "__main__":
    # Example usage for testing
    error_handler = LLMErrorHandler()
    
    # Test YAML error handling
    yaml_error_context = {
        'error_message': 'found character that cannot start any token at line 3',
        'generated_content': '---\ntitle: Invalid YAML @#$\n---',
        'info_type': 'general-document'
    }
    
    result = error_handler.handle_generation_errors('yaml_parsing_error', yaml_error_context)
    
    print("Error Handling Result:")
    print("=" * 50)
    print(f"Recovery Strategy: {result['recovery_strategy']}")
    print(f"Retry Recommended: {result['retry_recommended']}")
    print(f"Error Severity: {result['error_severity']}")
    print("\nCorrective Instructions:")
    print(result['corrective_prompt'])
    
    # Get statistics
    stats = error_handler.get_error_statistics()
    print(f"\nError Statistics: {stats}")
```
  ```
  --- END OF FILE scribe/error_handling\llm_error_handler.py ---
  --- START OF FILE scribe/integrations\llm_integration.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
LLM Integration Architecture Design

Implementation of Phase 3: Step 3.1.1 - LLM Integration Architecture Design
From: ultimate-frontmatter-enhancement-guideline-20250617-0312.md

This module provides complete LLM integration with existing Scribe system for
schema-constrained frontmatter generation with SHACL validation.
"""

import json
import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
import logging
import hashlib
from datetime import datetime

# Import existing validation infrastructure
import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'validators'))
sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'builder'))

try:
    from graph_validator import GraphValidator
except ImportError:
    # Fallback implementation if graph_validator is not available
    class GraphValidator:
        def validate_frontmatter_shacl(self, frontmatter, info_type):
            return type('MockResult', (), {'conforms': True, 'violations': []})()

from shacl_parser import SHACLParser

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMClient:
    """
    Mock LLM Client for frontmatter generation.
    
    In production, this would integrate with actual LLM services like OpenAI, 
    Anthropic, or local models. For this implementation, it provides deterministic
    fallback generation to ensure 100% success rate.
    """
    
    def __init__(self, model_name: str = "mock-llm", api_key: Optional[str] = None):
        self.model_name = model_name
        self.api_key = api_key
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # Generation statistics
        self.generation_stats = {
            'total_generations': 0,
            'successful_generations': 0,
            'fallback_generations': 0
        }
    
    def generate(self, prompt: str, max_tokens: int = 500, temperature: float = 0.1) -> str:
        """
        Generate text based on prompt.
        
        Args:
            prompt: The input prompt for generation
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            
        Returns:
            Generated text content
        """
        self.generation_stats['total_generations'] += 1
        
        try:
            # In production, this would call actual LLM API
            # For now, use deterministic generation based on prompt analysis
            generated_content = self._mock_llm_generation(prompt)
            
            self.generation_stats['successful_generations'] += 1
            return generated_content
            
        except Exception as e:
            self.logger.warning(f"LLM generation failed: {e}, using fallback")
            self.generation_stats['fallback_generations'] += 1
            return self._fallback_generation(prompt)
    
    def _mock_llm_generation(self, prompt: str) -> str:
        """Mock LLM generation that analyzes prompt to create appropriate frontmatter."""
        prompt_lower = prompt.lower()
        
        # Extract info-type from prompt
        info_type = 'general-document'
        if 'standard-definition' in prompt_lower:
            info_type = 'standard-definition'
        elif 'technical-report' in prompt_lower:
            info_type = 'technical-report'
        elif 'policy-document' in prompt_lower:
            info_type = 'policy-document'
        
        # Generate appropriate frontmatter based on detected type
        base_frontmatter = {
            'title': 'Generated Document Title',
            'info-type': info_type,
            'version': '1.0.0',
            'date-created': datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'),
            'date-modified': datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'),
            'kb-id': f'GEN-{info_type.upper().replace("-", "-")}-{datetime.now().strftime("%Y%m%d-%H%M")}'
        }
        
        # Add type-specific fields
        if info_type in ['standard-definition', 'policy-document']:
            base_frontmatter.update({
                'standard_id': f'ST-{info_type[:3].upper()}-GENERATED-001',
                'criticality': 'P1-Important',
                'lifecycle_gatekeeper': 'Technical-Review'
            })
        
        # Convert to YAML
        return yaml.dump(base_frontmatter, default_flow_style=False)
    
    def _fallback_generation(self, prompt: str) -> str:
        """Deterministic fallback generation to ensure 100% success."""
        return yaml.dump({
            'title': 'Fallback Generated Title',
            'info-type': 'general-document',
            'version': '1.0.0',
            'date-created': datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'),
            'date-modified': datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'),
            'kb-id': f'FALLBACK-{datetime.now().strftime("%Y%m%d-%H%M")}'
        }, default_flow_style=False)
    
    def get_generation_stats(self) -> Dict[str, Any]:
        """Get generation statistics."""
        return self.generation_stats.copy()


class LLMSchemaIntegration:
    """
    LLM Schema Integration for schema-constrained frontmatter generation.
    
    Integrates LLM capabilities with existing SHACL validation infrastructure
    to provide automated, schema-compliant frontmatter generation.
    """
    
    def __init__(self, shacl_file: str, jsonld_context: str):
        self.shacl_file = Path(shacl_file)
        self.jsonld_context = Path(jsonld_context)
        
        # Load existing infrastructure
        self.shacl_validator = self._load_existing_graph_validator()
        self.schema_constraints = self._load_schema_constraints(shacl_file)
        self.context_mappings = self._load_context_mappings(jsonld_context)
        self.llm_client = self._initialize_llm_client()
        
        # Initialize logger
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # Profile generators cache
        self.profile_generators = {}
    
    def _load_existing_graph_validator(self):
        """Load existing graph validator from the repository."""
        try:
            return GraphValidator()
        except Exception as e:
            self.logger.warning(f"Could not load GraphValidator: {e}")
            return None
    
    def _load_schema_constraints(self, shacl_file: str) -> Dict[str, Any]:
        """Load and parse SHACL constraints."""
        try:
            parser = SHACLParser(shacl_file)
            constraints = parser.extract_validation_rules()
            self.logger.info(f"Loaded {len(constraints)} SHACL constraint sets")
            return constraints
        except Exception as e:
            self.logger.error(f"Failed to load SHACL constraints: {e}")
            return {}
    
    def _load_context_mappings(self, jsonld_context: str) -> Dict[str, Any]:
        """Load JSON-LD context mappings."""
        try:
            with open(jsonld_context, 'r', encoding='utf-8') as f:
                context_data = json.load(f)
            
            mappings = context_data.get('@context', {})
            self.logger.info(f"Loaded {len(mappings)} context mappings")
            return mappings
        except Exception as e:
            self.logger.error(f"Failed to load JSON-LD context: {e}")
            return {}
    
    def _initialize_llm_client(self) -> LLMClient:
        """Initialize LLM client."""
        return LLMClient()
    
    def initialize_schema_constrained_generation(self) -> Dict[str, Any]:
        """
        Step 1: Initialize LLM with schema awareness.
        
        Returns:
            Dictionary of profile-specific generators ready for use
        """
        self.logger.info("Initializing schema-constrained generation")
        
        # Sub-step 1.1: Load all SHACL profiles
        profiles = self._extract_all_profiles()
        
        # Sub-step 1.2: Convert SHACL constraints to LLM prompts
        constraint_prompts = self._convert_shacl_to_prompts(profiles)
        
        # Sub-step 1.3: Create profile-specific generators
        profile_generators = {}
        for profile_name, constraints in constraint_prompts.items():
            profile_generators[profile_name] = self._create_profile_generator(
                profile_name, constraints
            )
        
        self.profile_generators = profile_generators
        self.logger.info(f"Initialized {len(profile_generators)} profile generators")
        
        return profile_generators
    
    def _extract_all_profiles(self) -> Dict[str, Any]:
        """Extract all SHACL profiles from constraints."""
        profiles = {}
        
        for constraint_name, constraint_data in self.schema_constraints.items():
            # Extract profile information
            profile_info = {
                'name': constraint_name,
                'cardinality': constraint_data.get('cardinality', {}),
                'patterns': constraint_data.get('patterns', {}),
                'datatypes': constraint_data.get('datatypes', {}),
                'targets': constraint_data.get('targets', {}),
                'profile_type': constraint_data.get('profile_type', 'general-validation')
            }
            
            profiles[constraint_name] = profile_info
        
        return profiles
    
    def _convert_shacl_to_prompts(self, profiles: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
        """Convert SHACL constraints to LLM prompt instructions."""
        constraint_prompts = {}
        
        for profile_name, profile_data in profiles.items():
            # Build constraint instructions for this profile
            constraints = {
                'mandatory_fields': [],
                'forbidden_fields': [],
                'pattern_constraints': {},
                'value_constraints': {},
                'datatype_constraints': {},
                'description': f"Schema constraints for {profile_name}"
            }
            
            # Extract mandatory fields
            cardinality = profile_data.get('cardinality', {})
            for field_info in cardinality.get('mandatory_fields', []):
                field_name = field_info.get('field') if isinstance(field_info, dict) else field_info
                if field_name:
                    constraints['mandatory_fields'].append(field_name)
            
            # Extract forbidden fields
            for field_info in cardinality.get('forbidden_fields', []):
                field_name = field_info.get('field') if isinstance(field_info, dict) else field_info
                if field_name:
                    constraints['forbidden_fields'].append(field_name)
            
            # Extract pattern constraints
            patterns = profile_data.get('patterns', {})
            for field_name, pattern_info in patterns.items():
                if isinstance(pattern_info, dict):
                    if pattern_info.get('pattern'):
                        constraints['pattern_constraints'][field_name] = pattern_info['pattern']
                    elif pattern_info.get('hasValue'):
                        constraints['value_constraints'][field_name] = pattern_info['hasValue']
            
            # Extract datatype constraints
            datatypes = profile_data.get('datatypes', {})
            for field_name, datatype_info in datatypes.items():
                if isinstance(datatype_info, dict) and datatype_info.get('datatype'):
                    constraints['datatype_constraints'][field_name] = datatype_info['datatype']
            
            constraint_prompts[profile_name] = constraints
        
        return constraint_prompts
    
    def _create_profile_generator(self, profile_name: str, constraints: Dict[str, Any]) -> Dict[str, Any]:
        """Create a profile-specific generator configuration."""
        generator_config = {
            'profile_name': profile_name,
            'constraints': constraints,
            'template_prompt': self._build_template_prompt(profile_name, constraints),
            'validation_rules': self._build_validation_rules(constraints),
            'fallback_template': self._build_fallback_template(profile_name, constraints)
        }
        
        return generator_config
    
    def _build_template_prompt(self, profile_name: str, constraints: Dict[str, Any]) -> str:
        """Build LLM prompt template for this profile."""
        prompt_parts = [
            f"Generate YAML frontmatter for a document that must comply with the {profile_name} profile.",
            "",
            "CRITICAL REQUIREMENTS:",
        ]
        
        # Add mandatory fields
        if constraints['mandatory_fields']:
            prompt_parts.append("MANDATORY FIELDS (must be included):")
            for field in constraints['mandatory_fields']:
                prompt_parts.append(f"- {field}: [appropriate value]")
            prompt_parts.append("")
        
        # Add forbidden fields
        if constraints['forbidden_fields']:
            prompt_parts.append("FORBIDDEN FIELDS (must NOT be included):")
            for field in constraints['forbidden_fields']:
                prompt_parts.append(f"- {field}")
            prompt_parts.append("")
        
        # Add pattern constraints
        if constraints['pattern_constraints']:
            prompt_parts.append("PATTERN CONSTRAINTS:")
            for field, pattern in constraints['pattern_constraints'].items():
                prompt_parts.append(f"- {field}: must match pattern '{pattern}'")
            prompt_parts.append("")
        
        # Add value constraints
        if constraints['value_constraints']:
            prompt_parts.append("VALUE CONSTRAINTS:")
            for field, value in constraints['value_constraints'].items():
                prompt_parts.append(f"- {field}: must have value '{value}'")
            prompt_parts.append("")
        
        prompt_parts.extend([
            "OUTPUT REQUIREMENTS:",
            "- Generate ONLY valid YAML frontmatter",
            "- Use proper YAML syntax with correct indentation",
            "- Include all mandatory fields",
            "- Exclude all forbidden fields",
            "- Follow all pattern and value constraints",
            "- Use ISO 8601 format for dates",
            "",
            "Generate the frontmatter now:"
        ])
        
        return "\n".join(prompt_parts)
    
    def _build_validation_rules(self, constraints: Dict[str, Any]) -> Dict[str, Any]:
        """Build validation rules for generated content."""
        return {
            'required_fields': set(constraints['mandatory_fields']),
            'forbidden_fields': set(constraints['forbidden_fields']),
            'pattern_rules': constraints['pattern_constraints'],
            'value_rules': constraints['value_constraints'],
            'datatype_rules': constraints['datatype_constraints']
        }
    
    def _build_fallback_template(self, profile_name: str, constraints: Dict[str, Any]) -> Dict[str, Any]:
        """Build deterministic fallback template for this profile."""
        template = {
            'title': 'Auto-Generated Document',
            'version': '1.0.0',
            'date-created': '${TIMESTAMP}',
            'date-modified': '${TIMESTAMP}',
            'kb-id': '${KB_ID}'
        }
        
        # Add mandatory fields with safe defaults
        for field in constraints['mandatory_fields']:
            if field not in template:
                template[field] = self._get_safe_default_value(field, constraints)
        
        return template
    
    def _get_safe_default_value(self, field: str, constraints: Dict[str, Any]) -> Any:
        """Get safe default value for a field based on constraints."""
        # Check for value constraints first
        if field in constraints.get('value_constraints', {}):
            return constraints['value_constraints'][field]
        
        # Check for pattern constraints
        if field in constraints.get('pattern_constraints', {}):
            pattern = constraints['pattern_constraints'][field]
            return self._generate_value_for_pattern(field, pattern)
        
        # Field-specific defaults
        field_defaults = {
            'info-type': 'general-document',
            'info_type': 'general-document',
            'standard_id': 'AUTO-GEN-STD-001',
            'criticality': 'P2-Standard',
            'lifecycle_gatekeeper': 'Technical-Review',
            'tags': ['auto-generated'],
            'scope_application': 'repository',
            'primary_topic': 'auto-generated-content'
        }
        
        return field_defaults.get(field, 'auto-generated-value')
    
    def _generate_value_for_pattern(self, field: str, pattern: str) -> str:
        """Generate a value that matches the given pattern."""
        # Simple pattern matching for common cases
        if 'standard' in field.lower() and '[A-Z]{2}-[A-Z]' in pattern:
            return 'AG-STD-AUTO-001'
        elif 'date' in field.lower():
            return datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')
        elif 'version' in field.lower():
            return '1.0.0'
        else:
            return 'pattern-compliant-value'
    
    def generate_schema_constrained_frontmatter(self, document_content: str, 
                                              info_type: str, 
                                              additional_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Generate schema-constrained frontmatter for a document.
        
        Args:
            document_content: The content of the document
            info_type: The target info-type for the document
            additional_context: Additional context for generation
            
        Returns:
            Generated frontmatter data and metadata
        """
        self.logger.info(f"Generating schema-constrained frontmatter for info-type: {info_type}")
        
        # Find appropriate profile generator
        profile_generator = self._find_profile_generator_for_type(info_type)
        
        if not profile_generator:
            self.logger.warning(f"No profile generator found for {info_type}, using default")
            profile_generator = self._get_default_profile_generator()
        
        # Build context-aware prompt
        prompt = self._build_context_aware_prompt(
            document_content, info_type, profile_generator, additional_context
        )
        
        # Generate with validation loop
        result = self._generate_with_validation_loop(prompt, info_type, profile_generator)
        
        return result
    
    def _find_profile_generator_for_type(self, info_type: str) -> Optional[Dict[str, Any]]:
        """Find profile generator that matches the info-type."""
        # Direct match
        if info_type in self.profile_generators:
            return self.profile_generators[info_type]
        
        # Pattern matching for profile names
        for profile_name, generator in self.profile_generators.items():
            if info_type.lower() in profile_name.lower():
                return generator
            
            # Check if generator supports this info-type
            constraints = generator.get('constraints', {})
            value_constraints = constraints.get('value_constraints', {})
            if value_constraints.get('info_type') == info_type or value_constraints.get('info-type') == info_type:
                return generator
        
        return None
    
    def _get_default_profile_generator(self) -> Dict[str, Any]:
        """Get default profile generator for fallback."""
        return {
            'profile_name': 'default',
            'constraints': {
                'mandatory_fields': ['title', 'version', 'date-created', 'date-modified', 'kb-id', 'info-type'],
                'forbidden_fields': [],
                'pattern_constraints': {},
                'value_constraints': {},
                'datatype_constraints': {}
            },
            'template_prompt': self._build_template_prompt('default', {
                'mandatory_fields': ['title', 'version', 'date-created', 'date-modified', 'kb-id', 'info-type'],
                'forbidden_fields': [],
                'pattern_constraints': {},
                'value_constraints': {},
                'datatype_constraints': {}
            }),
            'validation_rules': {
                'required_fields': {'title', 'version', 'date-created', 'date-modified', 'kb-id', 'info-type'},
                'forbidden_fields': set(),
                'pattern_rules': {},
                'value_rules': {},
                'datatype_rules': {}
            },
            'fallback_template': {
                'title': 'Auto-Generated Document',
                'info-type': 'general-document',
                'version': '1.0.0',
                'date-created': '${TIMESTAMP}',
                'date-modified': '${TIMESTAMP}',
                'kb-id': '${KB_ID}'
            }
        }
    
    def _build_context_aware_prompt(self, document_content: str, info_type: str, 
                                  profile_generator: Dict[str, Any], 
                                  additional_context: Optional[Dict[str, Any]]) -> str:
        """Build context-aware prompt for LLM generation."""
        # Analyze document content
        content_analysis = self._analyze_document_content(document_content)
        
        # Get base template prompt
        base_prompt = profile_generator['template_prompt']
        
        # Build context section
        context_parts = [
            "DOCUMENT CONTEXT:",
            f"- Document length: {content_analysis['length']} characters",
            f"- Has headings: {content_analysis['has_headings']}",
            f"- Content type: {content_analysis['inferred_type']}",
            f"- Target info-type: {info_type}",
        ]
        
        if additional_context:
            context_parts.append("- Additional context:")
            for key, value in additional_context.items():
                context_parts.append(f"  - {key}: {value}")
        
        context_section = "\n".join(context_parts)
        
        # Combine with template prompt
        full_prompt = f"{context_section}\n\n{base_prompt}"
        
        return full_prompt
    
    def _analyze_document_content(self, content: str) -> Dict[str, Any]:
        """Analyze document content for context."""
        return {
            'length': len(content),
            'has_headings': '#' in content,
            'has_code_blocks': '```' in content,
            'has_lists': any(line.strip().startswith(('-', '*', '+')) for line in content.split('\n')),
            'inferred_type': self._infer_content_type(content),
            'word_count': len(content.split())
        }
    
    def _infer_content_type(self, content: str) -> str:
        """Infer content type from document content."""
        content_lower = content.lower()
        
        if 'standard definition' in content_lower or 'compliance' in content_lower:
            return 'standard-definition'
        elif 'analysis' in content_lower or 'report' in content_lower:
            return 'technical-report'
        elif 'policy' in content_lower or 'mandatory' in content_lower:
            return 'policy-document'
        else:
            return 'general-document'
    
    def _generate_with_validation_loop(self, prompt: str, info_type: str, 
                                     profile_generator: Dict[str, Any]) -> Dict[str, Any]:
        """Generate frontmatter with validation loop for 100% success."""
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                # Generate frontmatter
                generated_yaml = self.llm_client.generate(prompt)
                
                # Parse YAML
                frontmatter_dict = yaml.safe_load(generated_yaml)
                
                # Validate against constraints
                validation_result = self._validate_against_constraints(
                    frontmatter_dict, profile_generator['validation_rules']
                )
                
                if validation_result['valid']:
                    return {
                        'success': True,
                        'frontmatter': frontmatter_dict,
                        'attempts_used': attempt + 1,
                        'generation_method': 'llm',
                        'validation_result': validation_result
                    }
                else:
                    # Add validation feedback to prompt for retry
                    prompt = self._add_validation_feedback_to_prompt(prompt, validation_result)
                    
            except Exception as e:
                self.logger.warning(f"Generation attempt {attempt + 1} failed: {e}")
        
        # If all attempts failed, use deterministic fallback
        return self._generate_deterministic_fallback(info_type, profile_generator)
    
    def _validate_against_constraints(self, frontmatter: Dict[str, Any], 
                                    validation_rules: Dict[str, Any]) -> Dict[str, Any]:
        """Validate frontmatter against constraint rules."""
        validation_result = {
            'valid': True,
            'violations': [],
            'warnings': []
        }
        
        # Check required fields
        required_fields = validation_rules.get('required_fields', set())
        for field in required_fields:
            if field not in frontmatter:
                validation_result['violations'].append({
                    'type': 'missing_required_field',
                    'field': field,
                    'message': f"Required field '{field}' is missing"
                })
                validation_result['valid'] = False
        
        # Check forbidden fields
        forbidden_fields = validation_rules.get('forbidden_fields', set())
        for field in forbidden_fields:
            if field in frontmatter:
                validation_result['violations'].append({
                    'type': 'forbidden_field_present',
                    'field': field,
                    'message': f"Forbidden field '{field}' is present"
                })
                validation_result['valid'] = False
        
        # Check value constraints
        value_rules = validation_rules.get('value_rules', {})
        for field, expected_value in value_rules.items():
            if field in frontmatter and frontmatter[field] != expected_value:
                validation_result['violations'].append({
                    'type': 'incorrect_value',
                    'field': field,
                    'expected': expected_value,
                    'actual': frontmatter[field],
                    'message': f"Field '{field}' must have value '{expected_value}'"
                })
                validation_result['valid'] = False
        
        return validation_result
    
    def _add_validation_feedback_to_prompt(self, prompt: str, validation_result: Dict[str, Any]) -> str:
        """Add validation feedback to prompt for retry."""
        feedback_parts = [
            prompt,
            "",
            "VALIDATION ERRORS - Please fix:",
        ]
        
        for violation in validation_result['violations']:
            feedback_parts.append(f"- {violation['message']}")
        
        feedback_parts.extend([
            "",
            "Generate corrected YAML frontmatter:"
        ])
        
        return "\n".join(feedback_parts)
    
    def _generate_deterministic_fallback(self, info_type: str, 
                                       profile_generator: Dict[str, Any]) -> Dict[str, Any]:
        """Generate deterministic fallback frontmatter."""
        self.logger.info("Using deterministic fallback generation")
        
        # Get fallback template
        template = profile_generator['fallback_template'].copy()
        
        # Replace template variables
        timestamp = datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')
        kb_id = f"AUTO-{info_type.upper().replace('-', '-')}-{datetime.now().strftime('%Y%m%d-%H%M')}"
        
        for key, value in template.items():
            if isinstance(value, str):
                template[key] = value.replace('${TIMESTAMP}', timestamp).replace('${KB_ID}', kb_id)
        
        # Ensure info-type is set correctly
        template['info-type'] = info_type
        
        return {
            'success': True,
            'frontmatter': template,
            'attempts_used': 0,
            'generation_method': 'fallback',
            'validation_result': {'valid': True, 'violations': [], 'warnings': []}
        }
    
    def get_integration_stats(self) -> Dict[str, Any]:
        """Get integration statistics."""
        llm_stats = self.llm_client.get_generation_stats()
        
        return {
            'llm_client_stats': llm_stats,
            'profile_generators_loaded': len(self.profile_generators),
            'schema_constraints_loaded': len(self.schema_constraints),
            'context_mappings_loaded': len(self.context_mappings),
            'integration_ready': len(self.profile_generators) > 0
        }


if __name__ == "__main__":
    # Example usage for testing
    integration = LLMSchemaIntegration(
        'standards/registry/shacl-shapes.ttl',
        'standards/registry/contexts/fields.jsonld'
    )
    
    # Initialize schema-constrained generation
    generators = integration.initialize_schema_constrained_generation()
    print(f"Initialized {len(generators)} profile generators")
    
    # Test generation
    test_content = "# Test Document\n\nThis is a test document for frontmatter generation."
    result = integration.generate_schema_constrained_frontmatter(
        test_content, 
        'general-document'
    )
    
    print(f"Generation result: {result['success']}")
    print(f"Generated frontmatter: {result['frontmatter']}")
    
    # Get stats
    stats = integration.get_integration_stats()
    print(f"Integration stats: {stats}")
```
  ```
  --- END OF FILE scribe/integrations\llm_integration.py ---
  --- START OF FILE scribe/prompts\schema_constraint_prompts.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Schema Constraint Prompt Engineering

Implementation of Phase 3: Step 3.1.2 - Schema Constraint Prompt Engineering
From: ultimate-frontmatter-enhancement-guideline-20250617-0312.md

This module provides sophisticated prompt engineering for LLM frontmatter generation
with SHACL constraint awareness and validation loop integration.
"""

import re
import yaml
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SchemaConstraintPromptEngine:
    """
    Schema Constraint Prompt Engine for LLM frontmatter generation.
    
    Provides sophisticated prompt engineering that converts SHACL constraints
    into detailed LLM instructions for accurate frontmatter generation.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # Prompt templates for different constraint types
        self.constraint_templates = self._initialize_constraint_templates()
        
        # Field type mappings for better generation
        self.field_type_mappings = self._initialize_field_type_mappings()
        
        # Common validation patterns
        self.validation_patterns = self._initialize_validation_patterns()
    
    def build_schema_constrained_prompt(self, document_content: str, info_type: str, 
                                      constraints: Dict[str, Any]) -> str:
        """
        Step 2: Build LLM prompts with SHACL constraints.
        
        Args:
            document_content: The content of the document
            info_type: Target info-type for the document
            constraints: SHACL constraints dictionary
            
        Returns:
            Comprehensive prompt for LLM generation
        """
        self.logger.info(f"Building schema-constrained prompt for {info_type}")
        
        # Sub-step 2.1: Analyze document content for context
        content_analysis = self._analyze_document_content(document_content)
        
        # Sub-step 2.2: Extract mandatory fields for info-type
        mandatory_fields = constraints.get('mandatory_fields', [])
        
        # Sub-step 2.3: Extract forbidden fields for info-type
        forbidden_fields = constraints.get('forbidden_fields', [])
        
        # Sub-step 2.4: Build constraint instructions
        constraint_instructions = self._build_constraint_instructions(
            mandatory_fields, forbidden_fields, constraints
        )
        
        # Sub-step 2.5: Create context-aware prompt
        prompt = self._create_context_aware_prompt(
            content_analysis, info_type, constraint_instructions
        )
        
        return prompt
    
    def _analyze_document_content(self, document_content: str) -> Dict[str, Any]:
        """Sub-step 2.1: Analyze document content for context."""
        analysis = {
            'length': len(document_content),
            'word_count': len(document_content.split()),
            'has_headings': self._detect_headings(document_content),
            'heading_structure': self._extract_heading_structure(document_content),
            'content_type_indicators': self._detect_content_type_indicators(document_content),
            'language_complexity': self._assess_language_complexity(document_content),
            'technical_terms': self._extract_technical_terms(document_content),
            'document_purpose': self._infer_document_purpose(document_content),
            'suggested_title': self._suggest_title_from_content(document_content)
        }
        
        return analysis
    
    def _detect_headings(self, content: str) -> Dict[str, Any]:
        """Detect heading structure in the document."""
        heading_pattern = r'^(#{1,6})\s+(.+)$'
        headings = []
        
        for line in content.split('\n'):
            match = re.match(heading_pattern, line.strip())
            if match:
                level = len(match.group(1))
                text = match.group(2).strip()
                headings.append({
                    'level': level,
                    'text': text,
                    'line': line.strip()
                })
        
        return {
            'count': len(headings),
            'max_level': max([h['level'] for h in headings], default=0),
            'headings': headings,
            'main_heading': headings[0]['text'] if headings and headings[0]['level'] == 1 else None
        }
    
    def _extract_heading_structure(self, content: str) -> List[str]:
        """Extract heading structure for title suggestion."""
        heading_info = self._detect_headings(content)
        return [h['text'] for h in heading_info['headings']]
    
    def _detect_content_type_indicators(self, content: str) -> Dict[str, bool]:
        """Detect indicators of different content types."""
        content_lower = content.lower()
        
        indicators = {
            'has_standards_language': any(term in content_lower for term in [
                'standard', 'compliance', 'requirement', 'specification', 'shall', 'must'
            ]),
            'has_policy_language': any(term in content_lower for term in [
                'policy', 'procedure', 'governance', 'mandate', 'directive'
            ]),
            'has_technical_language': any(term in content_lower for term in [
                'implementation', 'architecture', 'system', 'algorithm', 'protocol'
            ]),
            'has_analysis_language': any(term in content_lower for term in [
                'analysis', 'report', 'findings', 'conclusion', 'recommendation'
            ]),
            'has_guide_language': any(term in content_lower for term in [
                'guide', 'tutorial', 'how to', 'step', 'example', 'instructions'
            ]),
            'has_code_examples': '```' in content,
            'has_diagrams': any(term in content_lower for term in [
                'diagram', 'figure', 'chart', 'graph'
            ]),
            'has_references': any(term in content_lower for term in [
                'reference', 'bibliography', 'citation', 'see also'
            ])
        }
        
        return indicators
    
    def _assess_language_complexity(self, content: str) -> Dict[str, Any]:
        """Assess the complexity of language used in the document."""
        words = content.split()
        sentences = re.split(r'[.!?]+', content)
        
        # Calculate metrics
        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0
        avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences) if sentences else 0
        
        # Detect technical terminology density
        technical_indicators = len(re.findall(r'\b[A-Z]{2,}\b', content))  # Acronyms
        code_blocks = len(re.findall(r'```[\s\S]*?```', content))
        
        complexity_score = (avg_word_length * 0.3 + avg_sentence_length * 0.4 + 
                          technical_indicators * 0.2 + code_blocks * 0.1)
        
        return {
            'average_word_length': avg_word_length,
            'average_sentence_length': avg_sentence_length,
            'technical_indicators': technical_indicators,
            'code_blocks': code_blocks,
            'complexity_score': complexity_score,
            'complexity_level': 'high' if complexity_score > 15 else 'medium' if complexity_score > 8 else 'low'
        }
    
    def _extract_technical_terms(self, content: str) -> List[str]:
        """Extract technical terms from the document."""
        # Extract acronyms and technical terms
        acronyms = re.findall(r'\b[A-Z]{2,}\b', content)
        
        # Extract terms in code blocks or backticks
        code_terms = re.findall(r'`([^`]+)`', content)
        
        # Extract capitalized terms (potential proper nouns/technical terms)
        capitalized_terms = re.findall(r'\b[A-Z][a-z]+(?:[A-Z][a-z]+)+\b', content)
        
        all_terms = list(set(acronyms + code_terms + capitalized_terms))
        return all_terms[:10]  # Limit to most significant terms
    
    def _infer_document_purpose(self, content: str) -> str:
        """Infer the primary purpose of the document."""
        content_lower = content.lower()
        
        purpose_indicators = {
            'specification': ['specification', 'spec', 'requirement', 'standard definition'],
            'guidance': ['guide', 'tutorial', 'how to', 'instruction', 'example'],
            'analysis': ['analysis', 'report', 'study', 'evaluation', 'assessment'],
            'policy': ['policy', 'procedure', 'governance', 'rule', 'regulation'],
            'documentation': ['documentation', 'manual', 'reference', 'api'],
            'reference': ['reference', 'glossary', 'index', 'catalog', 'directory']
        }
        
        purpose_scores = {}
        for purpose, indicators in purpose_indicators.items():
            score = sum(content_lower.count(indicator) for indicator in indicators)
            purpose_scores[purpose] = score
        
        if purpose_scores:
            primary_purpose = max(purpose_scores, key=purpose_scores.get)
            if purpose_scores[primary_purpose] > 0:
                return primary_purpose
        
        return 'general'
    
    def _suggest_title_from_content(self, content: str) -> str:
        """Suggest a title based on document content."""
        heading_info = self._detect_headings(content)
        
        # Use main heading if available
        if heading_info['main_heading']:
            return heading_info['main_heading']
        
        # Use first heading if available
        if heading_info['headings']:
            return heading_info['headings'][0]['text']
        
        # Extract first meaningful sentence
        sentences = re.split(r'[.!?]+', content)
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10 and len(sentence) < 100:
                return sentence
        
        return "Auto-Generated Document Title"
    
    def _build_constraint_instructions(self, mandatory_fields: List[str], 
                                     forbidden_fields: List[str], 
                                     constraints: Dict[str, Any]) -> Dict[str, Any]:
        """Sub-step 2.4: Build comprehensive constraint instructions."""
        instructions = {
            'mandatory_section': self._format_mandatory_fields(mandatory_fields, constraints),
            'forbidden_section': self._format_forbidden_fields(forbidden_fields),
            'pattern_section': self._format_pattern_constraints(constraints.get('pattern_constraints', {})),
            'value_section': self._format_value_constraints(constraints.get('value_constraints', {})),
            'datatype_section': self._format_datatype_constraints(constraints.get('datatype_constraints', {})),
            'validation_section': self._format_validation_rules(constraints),
            'examples_section': self._generate_constraint_examples(constraints)
        }
        
        return instructions
    
    def _format_mandatory_fields(self, mandatory_fields: List[str], 
                                constraints: Dict[str, Any]) -> str:
        """Format mandatory fields with detailed instructions."""
        if not mandatory_fields:
            return "No mandatory fields specified."
        
        field_instructions = []
        field_instructions.append("MANDATORY FIELDS (must be included):")
        
        for field in mandatory_fields:
            field_info = self._get_field_generation_info(field, constraints)
            field_instructions.append(f"- {field}: {field_info}")
        
        return "\n".join(field_instructions)
    
    def _get_field_generation_info(self, field: str, constraints: Dict[str, Any]) -> str:
        """Get detailed generation information for a specific field."""
        # Check for value constraints
        value_constraints = constraints.get('value_constraints', {})
        if field in value_constraints:
            return f"MUST be exactly '{value_constraints[field]}'"
        
        # Check for pattern constraints
        pattern_constraints = constraints.get('pattern_constraints', {})
        if field in pattern_constraints:
            pattern = pattern_constraints[field]
            return f"Must match pattern '{pattern}' - {self._explain_pattern(field, pattern)}"
        
        # Check for datatype constraints
        datatype_constraints = constraints.get('datatype_constraints', {})
        if field in datatype_constraints:
            datatype = datatype_constraints[field]
            return f"Must be of type {datatype} - {self._explain_datatype(field, datatype)}"
        
        # Use field-specific guidance
        return self._get_field_specific_guidance(field)
    
    def _explain_pattern(self, field: str, pattern: str) -> str:
        """Explain what a pattern constraint means."""
        pattern_explanations = {
            r'^[A-Z]{2}-[A-Z]{2,6}-[A-Z0-9\-]+$': 'Format: XX-YYYYY-ZZZZZ (e.g., ST-POLICY-EXAMPLE-001)',
            r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z$': 'ISO 8601 date format (e.g., 2023-12-01T10:30:00Z)',
            r'^P[0-4]-[A-Za-z\-]+$': 'Priority format P0-P4 followed by description (e.g., P1-Important)',
            r'^[A-Za-z\-]+$': 'Alphabetic text with hyphens allowed'
        }
        
        for regex, explanation in pattern_explanations.items():
            if pattern == regex:
                return explanation
        
        return f"Custom pattern: {pattern}"
    
    def _explain_datatype(self, field: str, datatype: str) -> str:
        """Explain what a datatype constraint means."""
        datatype_explanations = {
            'xsd:string': 'Text string',
            'xsd:dateTime': 'ISO 8601 date-time format',
            'xsd:date': 'ISO 8601 date format',
            'xsd:integer': 'Whole number',
            'xsd:boolean': 'true or false',
            'xsd:decimal': 'Decimal number'
        }
        
        return datatype_explanations.get(datatype, f'Type: {datatype}')
    
    def _get_field_specific_guidance(self, field: str) -> str:
        """Get field-specific generation guidance."""
        field_guidance = {
            'title': 'Descriptive title based on document content',
            'version': 'Semantic version (e.g., 1.0.0)',
            'date-created': 'Current timestamp in ISO 8601 format',
            'date-modified': 'Current timestamp in ISO 8601 format',
            'kb-id': 'Unique knowledge base identifier',
            'info-type': 'Document type classification',
            'standard_id': 'Standard identifier following repository pattern',
            'criticality': 'Priority level (P0-Critical, P1-Important, P2-Standard, P3-Low)',
            'lifecycle_gatekeeper': 'Review authority (Technical-Review, Architect-Review, etc.)',
            'tags': 'List of relevant classification tags',
            'scope_application': 'Scope of application for this document',
            'primary_topic': 'Primary topic or keyword for this document'
        }
        
        return field_guidance.get(field, 'Appropriate value for this field')
    
    def _format_forbidden_fields(self, forbidden_fields: List[str]) -> str:
        """Format forbidden fields instructions."""
        if not forbidden_fields:
            return "No forbidden fields specified."
        
        instructions = ["FORBIDDEN FIELDS (must NOT be included):"]
        for field in forbidden_fields:
            instructions.append(f"- {field}")
        
        return "\n".join(instructions)
    
    def _format_pattern_constraints(self, pattern_constraints: Dict[str, str]) -> str:
        """Format pattern constraint instructions."""
        if not pattern_constraints:
            return ""
        
        instructions = ["PATTERN CONSTRAINTS:"]
        for field, pattern in pattern_constraints.items():
            explanation = self._explain_pattern(field, pattern)
            instructions.append(f"- {field}: {explanation}")
        
        return "\n".join(instructions)
    
    def _format_value_constraints(self, value_constraints: Dict[str, str]) -> str:
        """Format value constraint instructions."""
        if not value_constraints:
            return ""
        
        instructions = ["VALUE CONSTRAINTS:"]
        for field, value in value_constraints.items():
            instructions.append(f"- {field}: MUST be exactly '{value}'")
        
        return "\n".join(instructions)
    
    def _format_datatype_constraints(self, datatype_constraints: Dict[str, str]) -> str:
        """Format datatype constraint instructions."""
        if not datatype_constraints:
            return ""
        
        instructions = ["DATATYPE CONSTRAINTS:"]
        for field, datatype in datatype_constraints.items():
            explanation = self._explain_datatype(field, datatype)
            instructions.append(f"- {field}: {explanation}")
        
        return "\n".join(instructions)
    
    def _format_validation_rules(self, constraints: Dict[str, Any]) -> str:
        """Format general validation rules."""
        rules = [
            "VALIDATION REQUIREMENTS:",
            "- Generate ONLY valid YAML frontmatter",
            "- Use proper YAML syntax with correct indentation",
            "- Use double quotes for string values when they contain special characters",
            "- Use ISO 8601 format for all date and datetime values",
            "- Ensure all mandatory fields are present with appropriate values",
            "- Do not include any forbidden fields",
            "- Follow all pattern and value constraints exactly"
        ]
        
        return "\n".join(rules)
    
    def _generate_constraint_examples(self, constraints: Dict[str, Any]) -> str:
        """Generate examples based on constraints."""
        examples = ["EXAMPLES:"]
        
        # Generate example for mandatory fields
        mandatory_fields = constraints.get('mandatory_fields', [])
        if mandatory_fields:
            example_frontmatter = {}
            
            for field in mandatory_fields:
                example_value = self._generate_example_value(field, constraints)
                example_frontmatter[field] = example_value
            
            examples.append("Example frontmatter structure:")
            examples.append("```yaml")
            examples.append(yaml.dump(example_frontmatter, default_flow_style=False))
            examples.append("```")
        
        return "\n".join(examples)
    
    def _generate_example_value(self, field: str, constraints: Dict[str, Any]) -> str:
        """Generate an example value for a field."""
        # Check value constraints first
        value_constraints = constraints.get('value_constraints', {})
        if field in value_constraints:
            return value_constraints[field]
        
        # Generate field-specific examples
        field_examples = {
            'title': 'Example Document Title',
            'version': '1.0.0',
            'date-created': '2023-12-01T10:30:00Z',
            'date-modified': '2023-12-01T10:30:00Z',
            'kb-id': 'DOC-EXAMPLE-20231201-1030',
            'info-type': 'general-document',
            'standard_id': 'ST-EXAMPLE-001',
            'criticality': 'P2-Standard',
            'lifecycle_gatekeeper': 'Technical-Review',
            'tags': ['example', 'documentation'],
            'scope_application': 'repository',
            'primary_topic': 'example-content'
        }
        
        return field_examples.get(field, 'example-value')
    
    def _create_context_aware_prompt(self, content_analysis: Dict[str, Any], 
                                   info_type: str, 
                                   constraint_instructions: Dict[str, Any]) -> str:
        """Sub-step 2.5: Create comprehensive context-aware prompt."""
        prompt_sections = []
        
        # Header section
        prompt_sections.append("CRITICAL: Generate YAML frontmatter that MUST pass SHACL validation")
        prompt_sections.append(f"TARGET INFO-TYPE: {info_type}")
        prompt_sections.append("")
        
        # Document context section
        prompt_sections.append("DOCUMENT CONTENT ANALYSIS:")
        prompt_sections.append(f"- Length: {content_analysis['length']} characters")
        prompt_sections.append(f"- Word count: {content_analysis['word_count']}")
        prompt_sections.append(f"- Complexity: {content_analysis['language_complexity']['complexity_level']}")
        prompt_sections.append(f"- Purpose: {content_analysis['document_purpose']}")
        prompt_sections.append(f"- Suggested title: {content_analysis['suggested_title']}")
        
        if content_analysis['technical_terms']:
            prompt_sections.append(f"- Technical terms: {', '.join(content_analysis['technical_terms'][:5])}")
        
        prompt_sections.append("")
        
        # Constraint sections
        for section_name, section_content in constraint_instructions.items():
            if section_content and section_content.strip():
                prompt_sections.append(section_content)
                prompt_sections.append("")
        
        # Output requirements
        prompt_sections.append("OUTPUT REQUIREMENTS:")
        prompt_sections.append("- Generate ONLY the YAML frontmatter block")
        prompt_sections.append("- Start with --- and end with ---")
        prompt_sections.append("- Use proper YAML syntax")
        prompt_sections.append("- Include ALL mandatory fields")
        prompt_sections.append("- Exclude ALL forbidden fields")
        prompt_sections.append("- Follow ALL pattern and value constraints")
        prompt_sections.append("")
        prompt_sections.append("Generate the frontmatter now:")
        
        return "\n".join(prompt_sections)
    
    def _initialize_constraint_templates(self) -> Dict[str, str]:
        """Initialize constraint templates for different scenarios."""
        return {
            'mandatory_field': "- {field}: {guidance}",
            'forbidden_field': "- {field} (MUST NOT be present)",
            'pattern_constraint': "- {field}: Must match pattern '{pattern}' - {explanation}",
            'value_constraint': "- {field}: MUST be exactly '{value}'",
            'datatype_constraint': "- {field}: Must be {datatype} - {explanation}"
        }
    
    def _initialize_field_type_mappings(self) -> Dict[str, str]:
        """Initialize field type mappings for better generation."""
        return {
            'date-created': 'datetime',
            'date-modified': 'datetime',
            'version': 'version',
            'kb-id': 'identifier',
            'standard_id': 'identifier',
            'title': 'title',
            'tags': 'list',
            'criticality': 'enum',
            'lifecycle_gatekeeper': 'enum'
        }
    
    def _initialize_validation_patterns(self) -> Dict[str, str]:
        """Initialize common validation patterns."""
        return {
            'iso_datetime': r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z$',
            'semantic_version': r'^\d+\.\d+\.\d+$',
            'standard_id': r'^[A-Z]{2}-[A-Z]{2,6}-[A-Z0-9\-]+$',
            'kb_id': r'^[A-Z0-9\-]+$',
            'criticality': r'^P[0-4]-[A-Za-z\-]+$'
        }
    
    def enhance_prompt_with_feedback(self, original_prompt: str, 
                                   validation_errors: List[Dict[str, Any]]) -> str:
        """Enhance prompt with validation feedback for retry attempts."""
        feedback_sections = [
            original_prompt,
            "",
            "VALIDATION ERRORS DETECTED - Please correct:",
        ]
        
        for error in validation_errors:
            error_type = error.get('type', 'unknown')
            field = error.get('field', 'unknown')
            message = error.get('message', 'Unknown error')
            
            if error_type == 'missing_required_field':
                feedback_sections.append(f"âŒ MISSING: {field} - {message}")
            elif error_type == 'forbidden_field_present':
                feedback_sections.append(f"âŒ FORBIDDEN: {field} - {message}")
            elif error_type == 'incorrect_value':
                expected = error.get('expected', 'N/A')
                actual = error.get('actual', 'N/A')
                feedback_sections.append(f"âŒ WRONG VALUE: {field} - Expected '{expected}', got '{actual}'")
            else:
                feedback_sections.append(f"âŒ ERROR: {field} - {message}")
        
        feedback_sections.extend([
            "",
            "Please generate corrected YAML frontmatter that addresses ALL the above errors:",
        ])
        
        return "\n".join(feedback_sections)
    
    def get_prompt_statistics(self, prompt: str) -> Dict[str, Any]:
        """Get statistics about the generated prompt."""
        return {
            'total_length': len(prompt),
            'word_count': len(prompt.split()),
            'line_count': len(prompt.split('\n')),
            'section_count': prompt.count(':'),
            'constraint_count': prompt.count('MUST'),
            'example_count': prompt.count('```'),
            'estimated_tokens': len(prompt.split()) * 1.3  # Rough token estimation
        }


if __name__ == "__main__":
    # Example usage for testing
    engine = SchemaConstraintPromptEngine()
    
    # Test document content
    test_content = """# Example Standard Document
    
    This document defines the standard for example processes.
    
    ## Requirements
    
    The following requirements must be met:
    - Compliance with regulations
    - Implementation of best practices
    """
    
    # Test constraints
    test_constraints = {
        'mandatory_fields': ['title', 'standard_id', 'version', 'criticality'],
        'forbidden_fields': ['draft_status'],
        'pattern_constraints': {
            'standard_id': '^ST-[A-Z]+-[0-9]+$'
        },
        'value_constraints': {
            'info-type': 'standard-definition'
        }
    }
    
    # Generate prompt
    prompt = engine.build_schema_constrained_prompt(
        test_content, 'standard-definition', test_constraints
    )
    
    print("Generated Prompt:")
    print("=" * 60)
    print(prompt)
    
    # Get statistics
    stats = engine.get_prompt_statistics(prompt)
    print(f"\nPrompt Statistics: {stats}")
```
  ```
  --- END OF FILE scribe/prompts\schema_constraint_prompts.py ---
  --- START OF FILE scribe/pyproject.toml ---
  ```
  ```toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "scribe-engine"
version = "2.0.0-dev"
description = "HMA-compliant event-driven automation engine"
readme = "README.md"
requires-python = ">=3.9"
license = {text = "MIT"}
authors = [
    {name = "Scribe Development Team", email = "dev@scribe-engine.org"},
]
keywords = ["automation", "hma", "hexagonal", "microkernel", "event-driven"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Libraries :: Application Frameworks",
    "Topic :: System :: Monitoring",
]

dependencies = [
    "structlog==23.1.0",
    "watchdog==3.0.0",
    "jsonschema==4.17.3",
    "portalocker==2.7.0",
    "pyyaml==6.0.1",
    "requests==2.31.0",
    # HMA v2.2 mandatory dependencies for SHACL validation
    "pyshacl==0.25.0",
    "rdflib==7.0.0",
    # HMA v2.2 mandatory dependencies for messaging
    "nats-py==2.3.1",
]

[project.optional-dependencies]
dev = [
    "pytest==7.4.0",
    "pytest-cov==4.1.0",
    "pytest-timeout==2.1.0",
    "pytest-xdist==3.3.1",
    "black==23.7.0",
    "isort==5.12.0",
    "flake8==6.0.0",
    "mypy==1.5.1",
    "bandit==1.7.5",
    "safety==2.3.5",
]

opentelemetry = [
    "opentelemetry-api==1.20.0",
    "opentelemetry-sdk==1.20.0",
    "opentelemetry-instrumentation==0.41b0",
    "opentelemetry-exporter-prometheus==1.12.0rc1",
    "opentelemetry-exporter-jaeger==1.20.0",
]

security = [
    "cryptography==41.0.4",
    "hvac==1.2.1",  # HashiCorp Vault client
]

messaging = [
    "nats-py==2.3.1",
    "aiokafka==0.8.11",
]

[project.urls]
Homepage = "https://github.com/scribe-engine/scribe"
Documentation = "https://docs.scribe-engine.org"
Repository = "https://github.com/scribe-engine/scribe.git"
Issues = "https://github.com/scribe-engine/scribe/issues"

[project.scripts]
scribe = "tools.scribe.engine:main"

[tool.setuptools.packages.find]
where = ["."]
include = ["tools.scribe*"]

[tool.setuptools.package-data]
"tools.scribe" = ["*.json", "*.yaml", "*.yml"]

[tool.pytest.ini_options]
minversion = "7.0"
addopts = "-ra -q --strict-markers --strict-config"
testpaths = [
    "../../test-environment/scribe-tests",
]
pythonpath = [
    ".",
    "../..",
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "windows: marks tests as Windows-specific",
    "security: marks tests as security-related",
]

[tool.coverage.run]
source = ["tools/scribe"]
omit = [
    "*/tests/*",
    "*/test_*",
    "*/conftest.py",
    "*/scribe_engine.egg-info/*",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if settings.DEBUG",
    "raise AssertionError",
    "raise NotImplementedError",
    "if 0:",
    "if __name__ == .__main__.:",
    "class .*\\bProtocol\\):",
    "@(abc\\.)?abstractmethod",
]

[tool.black]
line-length = 88
target-version = ['py39', 'py310', 'py311', 'py312']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
  | scribe_engine\.egg-info
)/
'''

[tool.isort]
profile = "black"
multi_line_output = 3
line_length = 88
known_first_party = ["tools.scribe"]

[tool.mypy]
python_version = "3.9"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[[tool.mypy.overrides]]
module = [
    "watchdog.*",
    "structlog.*",
    "portalocker.*",
]
ignore_missing_imports = true

[tool.bandit]
exclude_dirs = ["tests", "test-environment"]
skips = ["B101", "B601"]  # Skip assert_used and shell injection for tests

[tool.flake8]
max-line-length = 88
exclude = [
    ".git",
    "__pycache__",
    "*.egg-info",
    "build",
    "dist",
]
ignore = [
    "E203",  # whitespace before ':'
    "E501",  # line too long (handled by black)
    "W503",  # line break before binary operator
]
```
  ```
  --- END OF FILE scribe/pyproject.toml ---
  --- START OF FILE scribe/remediation-plan.md ---
  ```
  ```md
## âœ… **COMPLETED** - HMA v2.2 Compliance Remediation Plan

### **STATUS: 100% COMPLETE** - July 25, 2025

This remediation plan has been **successfully executed** and all identified compliance violations have been resolved. The Scribe v2.2 codebase is now **100% HMA v2.2 compliant** with comprehensive test validation.

### **Remediation Summary**

The remediation addressed the three critical compliance gaps:

1. âœ… **Architectural Refactoring Complete**: All 6 plugins refactored to use HMA v2.2 constructor patterns with mandatory Ports & Adapters interaction model
2. âœ… **Production Tier 3 Compliance**: SHACL validator mock replaced with real pyshacl integration and comprehensive validation
3. âœ… **Complete Tier 1 Standard Enforcement**: Boundary validation and OpenTelemetry telemetry implemented on all L1/L4 boundaries

All requirements have been met with **comprehensive test coverage** (19/19 core compliance tests passing) and **production-ready implementation**.

### Priority 1 (Critical): Enforce Strict Ports & Adapters Interaction Model Across All Plugins

This is the most severe architectural violation. It breaks the core principle of decoupling business logic from infrastructure and bypasses security and observability controls.

-   **Problem:** Multiple plugins in `scribe/actions/` do not use the HMA-compliant `PluginContextPort` for dependency injection. They instantiate dependencies directly or use outdated constructors, creating tight coupling.
-   **Risk:** Violates core HMA principles, prevents technology swapping, makes testing difficult, and bypasses security/telemetry controls enforced at port boundaries.
-   **Implementation Guide:**
    1.  **Identify Non-Compliant Plugins:** The following plugins require immediate refactoring:
        -   `enhanced_frontmatter_action.py`
        -   `graph_validation_action.py`
        -   `naming_enforcement_action.py`
        -   `reconciliation_action.py`
        -   `roadmap_populator_action.py`
        -   `view_generation_action.py`
    2.  **Refactor Constructors:** For each plugin identified above, modify its `__init__` method to match the HMA v2.2 signature defined in `actions/base.py`:
        ```python
        # BEFORE (Example from graph_validation_action.py)
        def __init__(self, action_type: str, params: Dict[str, Any], plugin_context):
            super().__init__(action_type, params, plugin_context)
            # ... direct access to self.context.get_port("configuration") ...
            # This part is compliant, but other plugins are not.
            # The issue is in plugins like enhanced_frontmatter_action.py:
            # def __init__(self):
            #     super().__init__()
            #     self.llm_integration = LLMSchemaIntegration(...) # VIOLATION

        # AFTER (Required for all plugins)
        def __init__(self, action_type: str, params: Dict[str, Any], plugin_context: 'PluginContextPort'):
            super().__init__(action_type, params, plugin_context)
            # Dependencies are now accessed via ports, not instantiated here.
        ```
    3.  **Eliminate Direct Instantiation:** Remove all lines within plugin constructors that directly instantiate classes (e.g., `LLMSchemaIntegration()`, `GraphValidator()`, `NamingEnforcerV2()`). These functionalities must be wrapped in Adapters and accessed via Ports.
    4.  **Adopt Port-Based Access:** Replace all direct method calls to core components or external libraries with asynchronous calls to ports obtained from the `plugin_context`.
        ```python
        # BEFORE
        validation_errors = self.validator_instance.validate_all_documents()

        # AFTER
        validation_port = self.context.get_port("validation") # Assuming a ValidationPort exists
        validation_result = await validation_port.validate_graph(file_path)
        ```
    5.  **Standardize `execute` Method:** Ensure the primary execution method in every plugin is asynchronous and matches the signature in `BaseAction`, returning the modified file content string.
        ```python
        async def execute(self, file_content: str, match: re.Match, file_path: str, params: Dict[str, Any]) -> str:
            # ... implementation ...
            return modified_content
        ```

### Priority 2 (Critical): Implement Production-Ready Tier 3 Compliance Adapter for SHACL

-   **Problem:** The `shacl_adapter.py` is a mock implementation. It returns simulated results instead of performing actual SHACL validation.
-   **Risk:** A core feature of the `enhanced_frontmatter_action` is non-functional. The system provides a false sense of data integrity, as semantic validation is not actually occurring.
-   **Implementation Guide:**
    1.  **Update Dependencies:** Add `pyshacl` and `rdflib` with appropriate versions to the `[project.dependencies]` section of `scribe/pyproject.toml`.
    2.  **Implement Real Validation:** In `scribe/adapters/shacl_adapter.py`, remove the `_simulate_shacl_validation` method. Modify `validate_with_compliance_bridge` to call the actual `pyshacl.validate()` function.
    3.  **Implement Violation Parsing:** In `_extract_shacl_violations`, replace the simulation logic with code that parses the RDF results graph returned by `pyshacl`. This will likely involve using `rdflib` to execute SPARQL queries against the results graph to extract violation details.
    4.  **Ensure Correct Transformation:** Verify that the real violation data is correctly transformed into the HMA-compliant `ComplianceReport` and `ValidationResult` data classes defined in the adapter.

### Priority 3 (High): Enforce Tier 1 Standards at All System Boundaries

-   **Problem:** While utilities for validation and telemetry exist, they are not consistently applied at all L1/L4 adapter boundaries, particularly in the file watcher.
-   **Risk:** Incomplete observability hinders debugging and monitoring. Unvalidated data entering the system from the file system poses a security risk and violates a mandatory HMA principle.
-   **Implementation Guide:**
    1.  **Validate Incoming File Events:**
        -   In `scribe/watcher.py`, the `ScribeEventHandler` must be provided with an instance of the `BoundaryValidator`.
        -   Before calling `self.event_bus_port.publish_event`, the handler must construct the event payload and validate it against the `l1_file_system_input` schema.
        -   If validation fails, the event must be rejected, and an error-level log and an OpenTelemetry error span must be emitted.
    2.  **Instrument All Boundaries with OpenTelemetry:**
        -   Review all public methods in L1 and L4 adapters (`watcher.py`, `core/port_adapters.py`, `adapters/shacl_adapter.py`).
        -   Wrap each public method with a `HMATelemetry` span. The span must include HMA-specific attributes (`hma.boundary.type`, `hma.operation`, `hma.source.component`, `hma.target.component`).
        -   **Example for `watcher.py`:**
            ```python
            # In ScribeEventHandler.on_modified
            with telemetry.trace_boundary_operation("on_modified", "l1_driving_adapter", "file_system", "scribe_core") as span:
                # ... existing logic ...
            ```

### Priority 4 (High): Establish Production-Ready Dependencies and Testing

-   **Problem:** The `pyproject.toml` file is missing critical production dependencies, and the testing framework outlined in `TODO.md` has not been created.
-   **Risk:** The application cannot be reliably built, deployed, or maintained without a complete dependency list and an automated test suite. This introduces a high risk of regressions.
-   **Implementation Guide:**
    1.  **Finalize Dependencies:** Audit the entire codebase, including the newly implemented SHACL adapter, and add all missing production dependencies (`nats-py`, `pyshacl`, `rdflib`, `opentelemetry-*`, etc.) to `scribe/pyproject.toml`. Pin versions for build reproducibility.
    2.  **Create Testing Framework:**
        -   Create the `tests/` directory at the project root.
        -   Configure `pytest` as specified in `pyproject.toml`.
    3.  **Implement Foundational Tests:**
        -   Create unit tests for all Port Adapters in `core/port_adapters.py`. Mock external dependencies (e.g., `nats-py`, `pyshacl`) to test the adapter logic in isolation.
        -   Create an integration test for the `file_processing_orchestrator` plugin to verify that it can correctly load and execute a chain of (mocked) L3 capability plugins.
        -   Establish a CI pipeline (e.g., GitHub Actions) that runs these tests and reports on code coverage, with an initial target of >70%.

---

**Key Constraints & Requirements:**

1.  **Test-Driven Development (with a specific location):**
    *   *All testing (writing and execution) must occur within the *test-environment* folder located in the project's root directory.*  
		- The folder **test-environment\scribe-tests** has already been created and must be used for this purpose.  
		- All old tests have been archived.  
		- A **new test suite** is required.  

    	**Note:** This is a strict location requirement.

2.  **GitHub Actions (Incremental Addition):**
    *   The project already has some GitHub Actions defined, but they're disabled.
    *   *Add* new GitHub Actions for your upgrade process (e.g., automated testing, deployment), but *do not* enable the pre-existing, disabled ones. Keep them as they are.

3.  **Centralized Logging:**
    *   *All* logs generated during the upgrade process must be saved *only* within the `tools\reports` folder.  No logging anywhere else is allowed.

4.  **Scribe Code Isolation:**
    *   All code related to the Scribe system (the current version is already backed up, so you can make changes to existing files without any limitations) must reside within the `tools\scribe` folder. This includes all source code, configuration files, etc.

5.  **Conda Environment:**
    *   The project uses a Conda virtual environment named `conda-kb`.
    *   You *must* activate this environment (`conda activate conda-kb`) before starting any work.
    *   All new packages and dependencies needed for the upgrade *must* be installed within this `conda-kb` environment.

**In essence:** You need to address all Scribe issues, but within a tightly controlled environment and with specific restrictions on where code lives, where tests are run, where logs are stored, and which virtual environment to use.  You also need to leverage GitHub Actions for automation, but without touching the existing (disabled) ones.
```
  ```
  --- END OF FILE scribe/remediation-plan.md ---
  --- START OF FILE scribe/schemas\README.md ---
  ```
  ```md
# Scribe Schemas - JSON Schema Validation Definitions

This directory contains all JSON Schema definitions used for validation throughout the Scribe engine. These schemas ensure HMA v2.2 compliance and maintain data integrity across all components.

## Overview

JSON Schema validation is a **Tier 1 mandatory** technology in HMA v2.2. All configuration files, plugin manifests, and API inputs/outputs are validated against these schemas to ensure:

- **Data Integrity**: Prevent malformed data from causing system failures
- **API Consistency**: Ensure consistent data structures across all interfaces
- **Security**: Validate inputs to prevent injection attacks and malformed data
- **Documentation**: Schemas serve as living documentation of data structures

## Schema Files

### Core System Schemas

| Schema File | Purpose | Used By |
|-------------|---------|---------|
| **scribe_config.schema.json** | Main engine configuration validation | `config/config.json` |
| **plugin_manifest.schema.json** | Plugin manifest validation (HMA v2.2) | All `manifest.json` files |

## Configuration Schema (scribe_config.schema.json)

Validates the main engine configuration with complete HMA v2.2 compliance requirements.

### Schema Structure

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Scribe Engine Configuration Schema v2.2",
  "description": "HMA v2.2 compliant configuration schema for Scribe engine",
  "type": "object",
  "required": ["config_version", "hma_compliance", "engine"],
  "properties": {
    "config_version": {
      "type": "string",
      "const": "2.2",
      "description": "Configuration schema version (must be 2.2)"
    },
    "hma_compliance": {
      "$ref": "#/definitions/hma_compliance"
    },
    "engine": {
      "$ref": "#/definitions/engine_config"
    }
  }
}
```

### Key Validation Rules

#### HMA Compliance Section

```json
{
  "hma_compliance": {
    "type": "object",
    "required": ["version", "tier_1_technologies"],
    "properties": {
      "version": {
        "type": "string",
        "const": "2.2"
      },
      "tier_1_technologies": {
        "type": "object",
        "required": ["telemetry", "security", "validation"],
        "properties": {
          "telemetry": {
            "type": "string",
            "enum": ["opentelemetry"]
          },
          "security": {
            "type": "string", 
            "enum": ["mtls"]
          },
          "validation": {
            "type": "string",
            "enum": ["json_schema"]
          }
        }
      }
    }
  }
}
```

#### Engine Configuration

```json
{
  "engine_config": {
    "type": "object",
    "required": ["watch_paths", "file_patterns"],
    "properties": {
      "watch_paths": {
        "type": "array",
        "items": {
          "type": "string",
          "minLength": 1
        },
        "minItems": 1,
        "description": "Directories to monitor for file changes"
      },
      "file_patterns": {
        "type": "array",
        "items": {
          "type": "string",
          "pattern": "^\\*\\.[a-zA-Z0-9]+$"
        },
        "minItems": 1,
        "description": "File patterns to process (e.g., '*.md', '*.txt')"
      },
      "max_workers": {
        "type": "integer",
        "minimum": 1,
        "maximum": 32,
        "default": 4
      }
    }
  }
}
```

## Plugin Manifest Schema (plugin_manifest.schema.json)

Validates all plugin manifest files for HMA v2.2 compliance.

### Schema Overview

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "HMA v2.2 Plugin Manifest Schema", 
  "description": "Validation schema for HMA v2.2 compliant plugin manifests",
  "type": "object",
  "required": [
    "manifest_version",
    "plugin_metadata", 
    "hma_compliance",
    "interface_contracts"
  ]
}
```

### Manifest Validation Rules

#### Version Requirements

```json
{
  "manifest_version": {
    "type": "string",
    "const": "2.2",
    "description": "Manifest version must be 2.2 for HMA compliance"
  }
}
```

#### Plugin Metadata

```json
{
  "plugin_metadata": {
    "type": "object",
    "required": ["name", "version", "type", "description"],
    "properties": {
      "name": {
        "type": "string",
        "pattern": "^[a-z][a-z0-9_]*$",
        "description": "Plugin name in snake_case"
      },
      "version": {
        "type": "string",
        "pattern": "^\\d+\\.\\d+\\.\\d+$",
        "description": "Semantic version (e.g., '2.2.0')"
      },
      "type": {
        "type": "string",
        "enum": ["L2-Orchestrator", "L3-Capability"],
        "description": "HMA plugin type classification"
      }
    }
  }
}
```

#### HMA Compliance Section

```json
{
  "hma_compliance": {
    "type": "object",
    "required": ["hma_version", "tier_classification", "boundary_interfaces"],
    "properties": {
      "hma_version": {
        "type": "string",
        "const": "2.2"
      },
      "tier_classification": {
        "type": "object",
        "required": ["mandatory", "recommended", "alternative"],
        "properties": {
          "mandatory": {
            "type": "array",
            "items": {
              "type": "string",
              "enum": ["json_schema", "otel_boundary", "mtls"]
            }
          },
          "recommended": {
            "type": "array",
            "items": {
              "type": "string",
              "enum": ["structured_logging", "health_checks", "nats_messaging", "kubernetes"]
            }
          },
          "alternative": {
            "type": "array",
            "items": {
              "type": "string"
            }
          }
        }
      },
      "boundary_interfaces": {
        "type": "array",
        "items": {
          "$ref": "#/definitions/boundary_interface"
        }
      }
    }
  }
}
```

#### Boundary Interface Definition

```json
{
  "boundary_interface": {
    "type": "object",
    "required": ["port_type", "direction", "validation", "telemetry"],
    "properties": {
      "port_type": {
        "type": "string",
        "enum": [
          "PluginExecutionPort",
          "CommandExecutionPort", 
          "FileSystemPort",
          "LoggingPort",
          "EventBusPort",
          "PluginContextPort",
          "ConfigurationPort",
          "HealthCheckPort"
        ]
      },
      "direction": {
        "type": "string",
        "enum": ["inbound", "outbound", "bidirectional"]
      },
      "validation": {
        "type": "string",
        "description": "Validation method for this interface"
      },
      "telemetry": {
        "type": "string",
        "enum": ["otel_spans", "otel_metrics", "otel_logs"]
      }
    }
  }
}
```

## Schema Validation

### Validation Process

All schemas are used for validation at multiple points:

1. **Configuration Loading**: `config.json` validated against `scribe_config.schema.json`
2. **Plugin Loading**: All `manifest.json` files validated against `plugin_manifest.schema.json`
3. **Runtime Validation**: API inputs validated against relevant schemas
4. **Development**: Pre-commit hooks validate all JSON files

### Validation Implementation

```python
import json
import jsonschema
from pathlib import Path

class SchemaValidator:
    """Centralized schema validation for Scribe engine."""
    
    def __init__(self, schema_dir: str = "schemas"):
        self.schema_dir = Path(schema_dir)
        self._schemas = {}
        self._load_schemas()
    
    def _load_schemas(self):
        """Load all schema files."""
        for schema_file in self.schema_dir.glob("*.schema.json"):
            schema_name = schema_file.stem.replace(".schema", "")
            with open(schema_file) as f:
                self._schemas[schema_name] = json.load(f)
    
    def validate(self, data: dict, schema_name: str) -> bool:
        """Validate data against named schema."""
        if schema_name not in self._schemas:
            raise ValueError(f"Schema '{schema_name}' not found")
        
        try:
            jsonschema.validate(data, self._schemas[schema_name])
            return True
        except jsonschema.ValidationError as e:
            print(f"Validation error: {e}")
            return False
    
    def validate_config(self, config_path: str) -> bool:
        """Validate configuration file."""
        with open(config_path) as f:
            config = json.load(f)
        return self.validate(config, "scribe_config")
    
    def validate_manifest(self, manifest_path: str) -> bool:
        """Validate plugin manifest."""
        with open(manifest_path) as f:
            manifest = json.load(f)
        return self.validate(manifest, "plugin_manifest")
```

### Command Line Validation

```bash
# Validate configuration
python -c "
from schemas.validator import SchemaValidator
validator = SchemaValidator()
print('Config valid:', validator.validate_config('config/config.json'))
"

# Validate all plugin manifests
find actions -name 'manifest.json' -exec python -c "
import json, jsonschema, sys
manifest = json.load(open(sys.argv[1]))
schema = json.load(open('schemas/plugin_manifest.schema.json'))
try:
    jsonschema.validate(manifest, schema)
    print(f'{sys.argv[1]}: VALID')
except jsonschema.ValidationError as e:
    print(f'{sys.argv[1]}: INVALID - {e}')
" {} \;
```

## Schema Development

### Creating New Schemas

1. **Identify Data Structure**: Determine what needs validation
2. **Define Requirements**: Specify required vs optional fields
3. **Add Constraints**: Define validation rules and patterns
4. **Test Thoroughly**: Test with valid and invalid data
5. **Document Usage**: Update this README with usage information

### Schema Best Practices

#### Use Semantic Constraints

```json
{
  "version": {
    "type": "string",
    "pattern": "^\\d+\\.\\d+\\.\\d+$",
    "description": "Semantic version format (major.minor.patch)"
  }
}
```

#### Provide Clear Descriptions

```json
{
  "watch_paths": {
    "type": "array",
    "items": {"type": "string"},
    "minItems": 1,
    "description": "List of directories to monitor for file changes. Must contain at least one path."
  }
}
```

#### Use Enums for Fixed Values

```json
{
  "log_level": {
    "type": "string",
    "enum": ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
    "default": "INFO"
  }
}
```

#### Define Reusable Components

```json
{
  "definitions": {
    "plugin_type": {
      "type": "string",
      "enum": ["L2-Orchestrator", "L3-Capability"]
    }
  },
  "properties": {
    "type": {"$ref": "#/definitions/plugin_type"}
  }
}
```

## Error Handling

### Validation Error Types

```python
# Common validation errors and handling
try:
    jsonschema.validate(data, schema)
except jsonschema.ValidationError as e:
    if e.validator == "required":
        print(f"Missing required field: {e.message}")
    elif e.validator == "enum":
        print(f"Invalid value. Allowed values: {e.schema['enum']}")
    elif e.validator == "pattern":
        print(f"Invalid format. Expected pattern: {e.schema['pattern']}")
    else:
        print(f"Validation error: {e.message}")
```

### Custom Error Messages

```json
{
  "hma_version": {
    "type": "string",
    "const": "2.2",
    "errorMessage": "HMA version must be '2.2' for compliance. Current version indicates non-compliant plugin."
  }
}
```

## Schema Versioning

### Version Management

- **Schema Versioning**: Schemas versioned independently of the application
- **Backward Compatibility**: New versions maintain backward compatibility when possible
- **Breaking Changes**: Breaking changes require new major version and migration path

### Migration Support

```json
{
  "anyOf": [
    {"$ref": "#/definitions/v2_1_format"},
    {"$ref": "#/definitions/v2_2_format"}
  ],
  "errorMessage": "Configuration must conform to either v2.1 or v2.2 format"
}
```

## Performance Considerations

### Schema Caching

```python
# Cache compiled schemas for better performance
from functools import lru_cache

@lru_cache(maxsize=128)
def get_compiled_schema(schema_path: str):
    """Get cached compiled schema."""
    with open(schema_path) as f:
        schema = json.load(f)
    return jsonschema.Draft7Validator(schema)
```

### Validation Optimization

- **Early Termination**: Use `ErrorTree` for early validation termination
- **Partial Validation**: Validate only changed fields when possible
- **Async Validation**: Use async validation for large datasets

## Testing

### Schema Test Suite

```python
import pytest
import json
from pathlib import Path

class TestSchemas:
    
    @pytest.mark.parametrize("schema_file", Path("schemas").glob("*.schema.json"))
    def test_schema_validity(self, schema_file):
        """Test that all schemas are valid JSON Schema."""
        with open(schema_file) as f:
            schema = json.load(f)
        
        # Validate schema structure
        jsonschema.Draft7Validator.check_schema(schema)
    
    def test_config_validation(self):
        """Test configuration validation with valid/invalid examples."""
        valid_config = {
            "config_version": "2.2",
            "hma_compliance": {"version": "2.2"},
            "engine": {"watch_paths": ["."], "file_patterns": ["*.md"]}
        }
        
        invalid_config = {
            "config_version": "1.0"  # Invalid version
        }
        
        validator = SchemaValidator()
        assert validator.validate(valid_config, "scribe_config")
        assert not validator.validate(invalid_config, "scribe_config")
```

## Related Documentation

- [Configuration Guide](../config/README.md) - How to use configuration schemas
- [Plugin Development](../actions/README.md) - Plugin manifest requirements
- [HMA v2.2 Specification](../../HMA v2.2/) - Architecture requirements
- [JSON Schema Specification](https://json-schema.org/) - JSON Schema documentation
```
  ```
  --- END OF FILE scribe/schemas\README.md ---
  --- START OF FILE scribe/schemas\plugin_manifest.schema.json ---
  ```
  ```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://scribe.engine/schemas/plugin-manifest.schema.json",
  "title": "Scribe Plugin Manifest",
  "description": "HMA v2.2 compliant plugin manifest schema for Scribe engine plugins",
  "type": "object",
  "required": [
    "manifest_version",
    "plugin_metadata",
    "hma_compliance",
    "runtime_requirements",
    "interface_contracts"
  ],
  "properties": {
    "manifest_version": {
      "type": "string",
      "pattern": "^2\\.0$",
      "description": "Manifest schema version - must be 2.0 for HMA v2.2 compliance"
    },
    "plugin_metadata": {
      "type": "object",
      "required": ["name", "version", "description", "author"],
      "properties": {
        "name": {
          "type": "string",
          "pattern": "^[a-z0-9_]+$",
          "description": "Plugin identifier in snake_case"
        },
        "version": {
          "type": "string",
          "pattern": "^\\d+\\.\\d+\\.\\d+(-[a-zA-Z0-9]+)?$",
          "description": "Semantic version string"
        },
        "description": {
          "type": "string",
          "minLength": 10,
          "description": "Human-readable plugin description"
        },
        "author": {
          "type": "string",
          "description": "Plugin author or organization"
        },
        "license": {
          "type": "string",
          "description": "SPDX license identifier"
        },
        "homepage": {
          "type": "string",
          "format": "uri",
          "description": "Plugin homepage URL"
        }
      }
    },
    "hma_compliance": {
      "type": "object",
      "required": ["hma_version", "tier_classification", "boundary_interfaces"],
      "properties": {
        "hma_version": {
          "type": "string",
          "pattern": "^2\\.2$",
          "description": "HMA version this plugin complies with"
        },
        "tier_classification": {
          "type": "object",
          "required": ["mandatory", "recommended", "alternative"],
          "properties": {
            "mandatory": {
              "type": "array",
              "items": {
                "type": "string",
                "enum": ["json_schema", "opentelemetry", "mtls", "event_bus", "circuit_breaker"]
              },
              "description": "Tier 1 mandatory technologies used"
            },
            "recommended": {
              "type": "array",
              "items": {
                "type": "string",
                "enum": ["structured_logging", "health_checks", "metrics", "tracing"]
              },
              "description": "Tier 2 recommended technologies used"
            },
            "alternative": {
              "type": "array",
              "items": {
                "type": "string"
              },
              "description": "Tier 3 alternative technologies used"
            }
          }
        },
        "boundary_interfaces": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["type", "protocol", "telemetry_enabled"],
            "properties": {
              "type": {
                "type": "string",
                "enum": ["inbound", "outbound", "bidirectional"]
              },
              "protocol": {
                "type": "string",
                "enum": ["http", "grpc", "websocket", "file_system", "event_bus"]
              },
              "endpoint": {
                "type": "string",
                "description": "Interface endpoint or pattern"
              },
              "telemetry_enabled": {
                "type": "boolean",
                "description": "Whether OpenTelemetry boundary telemetry is enabled"
              }
            }
          }
        }
      }
    },
    "runtime_requirements": {
      "type": "object",
      "required": ["python_version", "dependencies"],
      "properties": {
        "python_version": {
          "type": "string",
          "pattern": "^>=3\\.(8|9|10|11|12)$",
          "description": "Minimum Python version requirement"
        },
        "dependencies": {
          "type": "object",
          "properties": {
            "required": {
              "type": "array",
              "items": {
                "type": "object",
                "required": ["name", "version"],
                "properties": {
                  "name": {
                    "type": "string"
                  },
                  "version": {
                    "type": "string"
                  },
                  "tier": {
                    "type": "string",
                    "enum": ["mandatory", "recommended", "alternative"]
                  }
                }
              }
            },
            "optional": {
              "type": "array",
              "items": {
                "type": "object",
                "required": ["name", "version"],
                "properties": {
                  "name": {
                    "type": "string"
                  },
                  "version": {
                    "type": "string"
                  },
                  "feature": {
                    "type": "string",
                    "description": "Feature this optional dependency enables"
                  }
                }
              }
            }
          }
        },
        "platform_support": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": ["windows", "linux", "macos", "cross_platform"]
          },
          "description": "Supported platforms"
        },
        "resource_limits": {
          "type": "object",
          "properties": {
            "max_memory_mb": {
              "type": "integer",
              "minimum": 1
            },
            "max_cpu_percent": {
              "type": "integer",
              "minimum": 1,
              "maximum": 100
            },
            "max_file_handles": {
              "type": "integer",
              "minimum": 1
            }
          }
        }
      }
    },
    "interface_contracts": {
      "type": "object",
      "required": ["action_interface"],
      "properties": {
        "action_interface": {
          "type": "object",
          "required": ["entry_point", "configuration_schema"],
          "properties": {
            "entry_point": {
              "type": "string",
              "pattern": "^[a-zA-Z_][a-zA-Z0-9_]*\\.[a-zA-Z_][a-zA-Z0-9_]*$",
              "description": "Python module.class path for action entry point"
            },
            "configuration_schema": {
              "type": "object",
              "description": "JSON Schema for plugin configuration validation"
            },
            "event_patterns": {
              "type": "array",
              "items": {
                "type": "object",
                "required": ["event_type", "file_patterns"],
                "properties": {
                  "event_type": {
                    "type": "string",
                    "enum": ["file_created", "file_modified", "file_deleted", "file_moved"]
                  },
                  "file_patterns": {
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "priority": {
                    "type": "integer",
                    "minimum": 1,
                    "maximum": 10,
                    "default": 5
                  }
                }
              }
            },
            "output_schema": {
              "type": "object",
              "description": "JSON Schema for action output validation"
            }
          }
        },
        "lifecycle_hooks": {
          "type": "object",
          "properties": {
            "on_load": {
              "type": "string",
              "description": "Method called when plugin is loaded"
            },
            "on_unload": {
              "type": "string",
              "description": "Method called when plugin is unloaded"
            },
            "on_config_change": {
              "type": "string",
              "description": "Method called when configuration changes"
            },
            "health_check": {
              "type": "string",
              "description": "Method for health status checking"
            }
          }
        }
      }
    },
    "security": {
      "type": "object",
      "properties": {
        "permissions": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": [
              "file_read",
              "file_write", 
              "file_delete",
              "network_access",
              "process_spawn",
              "environment_access",
              "registry_access"
            ]
          }
        },
        "sandbox_compatible": {
          "type": "boolean",
          "default": true,
          "description": "Whether plugin can run in sandboxed environment"
        },
        "mtls_required": {
          "type": "boolean",
          "default": false,
          "description": "Whether plugin requires mTLS for network communications"
        }
      }
    }
  }
}
```
  ```
  --- END OF FILE scribe/schemas\plugin_manifest.schema.json ---
  --- START OF FILE scribe/schemas\scribe_config.schema.json ---
  ```
  ```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://scribe.engine/schemas/scribe-config.schema.json",
  "title": "Scribe Engine Configuration",
  "description": "Configuration schema for Scribe v2.0 engine with HMA v2.2 compliance",
  "type": "object",
  "required": ["engine", "logging", "rules"],
  "properties": {
    "engine": {
      "type": "object",
      "required": ["watch_paths", "file_patterns"],
      "properties": {
        "watch_paths": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "minItems": 1,
          "description": "Paths to monitor for file system events"
        },
        "file_patterns": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "File patterns to match (glob syntax)"
        },
        "queue_maxsize": {
          "type": "integer",
          "minimum": 0,
          "default": 100,
          "description": "Maximum size of event queue (0 = unlimited)"
        },
        "health_port": {
          "type": "integer",
          "minimum": 1024,
          "maximum": 65535,
          "default": 9469,
          "description": "Port for health check endpoint"
        },
        "worker_count": {
          "type": "integer",
          "minimum": 1,
          "maximum": 32,
          "default": 4,
          "description": "Number of worker threads"
        },
        "retry_attempts": {
          "type": "integer",
          "minimum": 0,
          "maximum": 10,
          "default": 3,
          "description": "Maximum retry attempts for failed actions"
        },
        "retry_delay_seconds": {
          "type": "number",
          "minimum": 0.1,
          "maximum": 60,
          "default": 1.0,
          "description": "Delay between retry attempts"
        }
      }
    },
    "logging": {
      "type": "object",
      "properties": {
        "level": {
          "type": "string",
          "enum": ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
          "default": "INFO",
          "description": "Logging level"
        },
        "format": {
          "type": "string",
          "enum": ["structured", "plain"],
          "default": "structured",
          "description": "Log format type"
        },
        "output_file": {
          "type": "string",
          "description": "Path to log output file (optional)"
        },
        "max_file_size_mb": {
          "type": "integer",
          "minimum": 1,
          "maximum": 1000,
          "default": 10,
          "description": "Maximum log file size in MB"
        },
        "backup_count": {
          "type": "integer",
          "minimum": 0,
          "maximum": 100,
          "default": 5,
          "description": "Number of backup log files to keep"
        }
      }
    },
    "rules": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["name", "patterns", "actions"],
        "properties": {
          "name": {
            "type": "string",
            "minLength": 1,
            "description": "Unique name for the rule"
          },
          "description": {
            "type": "string",
            "description": "Human-readable description of the rule"
          },
          "patterns": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "minItems": 1,
            "description": "File patterns this rule applies to"
          },
          "actions": {
            "type": "array",
            "items": {
              "type": "object",
              "required": ["type", "params"],
              "properties": {
                "type": {
                  "type": "string",
                  "description": "Action type (must match plugin action type)"
                },
                "params": {
                  "type": "object",
                  "description": "Action-specific parameters"
                },
                "enabled": {
                  "type": "boolean",
                  "default": true,
                  "description": "Whether this action is enabled"
                },
                "priority": {
                  "type": "integer",
                  "minimum": 1,
                  "maximum": 10,
                  "default": 5,
                  "description": "Action execution priority"
                }
              }
            },
            "minItems": 1,
            "description": "Actions to execute when pattern matches"
          },
          "enabled": {
            "type": "boolean",
            "default": true,
            "description": "Whether this rule is enabled"
          },
          "circuit_breaker": {
            "type": "object",
            "properties": {
              "enabled": {
                "type": "boolean",
                "default": true,
                "description": "Whether circuit breaker is enabled for this rule"
              },
              "failure_threshold": {
                "type": "integer",
                "minimum": 1,
                "maximum": 100,
                "default": 5,
                "description": "Number of failures before opening circuit"
              },
              "recovery_timeout_seconds": {
                "type": "integer",
                "minimum": 1,
                "maximum": 3600,
                "default": 60,
                "description": "Seconds to wait before attempting recovery"
              },
              "success_threshold": {
                "type": "integer",
                "minimum": 1,
                "maximum": 10,
                "default": 3,
                "description": "Number of successes needed to close circuit"
              }
            }
          }
        }
      },
      "description": "List of rules defining file patterns and actions"
    },
    "security": {
      "type": "object",
      "properties": {
        "allowed_paths": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "Paths where file operations are allowed"
        },
        "blocked_commands": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "Commands that are blocked from execution"
        },
        "max_file_size_mb": {
          "type": "integer",
          "minimum": 1,
          "maximum": 1000,
          "default": 100,
          "description": "Maximum file size for operations in MB"
        },
        "enable_sandbox": {
          "type": "boolean",
          "default": false,
          "description": "Whether to enable sandboxed execution"
        }
      }
    },
    "telemetry": {
      "type": "object",
      "properties": {
        "enabled": {
          "type": "boolean",
          "default": true,
          "description": "Whether OpenTelemetry is enabled"
        },
        "endpoint": {
          "type": "string",
          "format": "uri",
          "description": "OpenTelemetry collector endpoint"
        },
        "service_name": {
          "type": "string",
          "default": "scribe-engine",
          "description": "Service name for telemetry"
        },
        "sampling_rate": {
          "type": "number",
          "minimum": 0.0,
          "maximum": 1.0,
          "default": 1.0,
          "description": "Trace sampling rate (0.0 to 1.0)"
        }
      }
    },
    "performance": {
      "type": "object",
      "properties": {
        "max_concurrent_actions": {
          "type": "integer",
          "minimum": 1,
          "maximum": 100,
          "default": 10,
          "description": "Maximum concurrent action executions"
        },
        "action_timeout_seconds": {
          "type": "integer",
          "minimum": 1,
          "maximum": 3600,
          "default": 300,
          "description": "Timeout for action execution"
        },
        "memory_limit_mb": {
          "type": "integer",
          "minimum": 100,
          "maximum": 8192,
          "default": 1024,
          "description": "Memory limit for engine process"
        },
        "enable_caching": {
          "type": "boolean",
          "default": true,
          "description": "Whether to enable result caching"
        },
        "cache_ttl_seconds": {
          "type": "integer",
          "minimum": 60,
          "maximum": 86400,
          "default": 3600,
          "description": "Cache time-to-live in seconds"
        }
      }
    }
  }
}
```
  ```
  --- END OF FILE scribe/schemas\scribe_config.schema.json ---
  --- START OF FILE scribe/utils\README.md ---
  ```
  ```md
# Scribe Utilities

This directory contains shared utility functions and modules used across the Scribe codebase. All utilities follow HMA v2.2 standards and provide technology-agnostic functionality.

## Available Utilities

### frontmatter_parser.py

**Purpose**: Consolidated YAML frontmatter parsing functionality for use across action plugins.

**Features**:
- Standardized parsing with error handling
- Preprocessing for special keys (e.g., `@key` formatting)
- Consistent YAML processing across plugins
- Cross-platform compatibility

**API**:
```python
from tools.scribe.utils.frontmatter_parser import (
    parse_frontmatter,
    has_frontmatter,
    remove_frontmatter,
    apply_frontmatter
)

# Parse frontmatter from document content
frontmatter_dict = parse_frontmatter(content)

# Check if content has frontmatter
if has_frontmatter(content):
    # Process accordingly

# Remove frontmatter from content
clean_content = remove_frontmatter(content)

# Apply frontmatter to content
updated_content = apply_frontmatter(content, frontmatter_dict)
```

**Used by**:
- `reconciliation_action.py` - Document reconciliation and indexing
- `enhanced_frontmatter_action.py` - LLM-enhanced frontmatter generation

**Migration Note**: This utility was created as part of DEP-007 cleanup to consolidate duplicated frontmatter parsing logic previously found in multiple plugins.

## Development Guidelines

### Adding New Utilities

1. **Scope**: Utilities should be technology-agnostic and reusable across plugins
2. **Documentation**: Include comprehensive docstrings and usage examples
3. **Testing**: Add unit tests with edge case coverage
4. **Import Path**: Use consistent import paths: `from tools.scribe.utils.module_name import function`

### Design Principles

- **Pure Functions**: Prefer stateless, pure functions where possible
- **Error Handling**: Robust error handling with meaningful error messages
- **Type Hints**: Use proper type annotations for all functions
- **Cross-Platform**: Ensure compatibility across Windows, macOS, and Linux

## Testing

```bash
# Test utilities
cd tools/scribe
python -m pytest utils/ -v
```

## Related Documentation

- [Plugin Development](../actions/README.md) - Using utilities in plugins
- [Architecture Decision Records](../docs/decisions/) - Design decisions for utilities
```
  ```
  --- END OF FILE scribe/utils\README.md ---
  --- START OF FILE scribe/utils\frontmatter_parser.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
Scribe Frontmatter Parser Utility

Consolidated YAML frontmatter parsing functionality for use across
action plugins. Provides standardized parsing with error handling
and preprocessing for special keys.
"""

import yaml
import re
from typing import Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)


def parse_frontmatter(content: str) -> Optional[Dict[str, Any]]:
    """
    Parse YAML frontmatter from document content.
    
    Handles both standard YAML frontmatter and special preprocessing
    for keys starting with @ symbols.
    
    Args:
        content: Document content that may contain frontmatter
        
    Returns:
        Dictionary of frontmatter data or None if not found/invalid
    """
    frontmatter_str = _extract_frontmatter_block(content)
    if not frontmatter_str:
        return None
    
    try:
        # Preprocess for keys starting with @ (common in JSON-LD contexts)
        processed_frontmatter_str = _preprocess_special_keys(frontmatter_str)
        
        # Parse YAML
        frontmatter_data = yaml.safe_load(processed_frontmatter_str)
        
        # Ensure we return a dict (not None, string, etc.)
        return frontmatter_data if isinstance(frontmatter_data, dict) else None
        
    except yaml.YAMLError as e:
        logger.warning(f"Could not parse frontmatter as YAML: {e}")
        return None
    except Exception as e:
        logger.warning(f"Unexpected error parsing frontmatter: {e}")
        return None


def _extract_frontmatter_block(content: str) -> Optional[str]:
    """
    Extract the raw frontmatter block from document content.
    
    Looks for YAML frontmatter delimited by --- markers.
    
    Args:
        content: Document content
        
    Returns:
        Raw frontmatter string or None if not found
    """
    if not content.strip().startswith('---'):
        return None
    
    # Find the end marker
    end_marker = content.find('---', 4)  # Start search after first ---
    if end_marker == -1:
        return None
    
    # Extract frontmatter content between markers
    frontmatter_str = content[4:end_marker].strip()
    return frontmatter_str if frontmatter_str else None


def _preprocess_special_keys(frontmatter_str: str) -> str:
    """
    Preprocess frontmatter to handle special key formats.
    
    Specifically handles keys starting with @ by quoting them
    for proper YAML parsing (common in JSON-LD contexts).
    
    Args:
        frontmatter_str: Raw frontmatter string
        
    Returns:
        Preprocessed frontmatter string
    """
    processed_lines = []
    
    for line in frontmatter_str.splitlines():
        # Check if line starts with @ and contains a colon
        if line.strip().startswith('@') and ':' in line:
            # Split on first colon
            key_part, value_part = line.split(':', 1)
            key = key_part.strip()
            value = value_part.strip()
            
            # Quote the key to make it valid YAML
            processed_lines.append(f'"{key}": {value}')
        else:
            processed_lines.append(line)
    
    return "\n".join(processed_lines)


def has_frontmatter(content: str) -> bool:
    """
    Check if content contains YAML frontmatter.
    
    Args:
        content: Document content to check
        
    Returns:
        True if frontmatter is present, False otherwise
    """
    return _extract_frontmatter_block(content) is not None


def remove_frontmatter(content: str) -> str:
    """
    Remove frontmatter from document content.
    
    Args:
        content: Document content that may contain frontmatter
        
    Returns:
        Content with frontmatter removed
    """
    if not content.strip().startswith('---'):
        return content
    
    # Find the end marker
    end_marker = content.find('---', 4)
    if end_marker == -1:
        return content
    
    # Return content after the closing ---
    remaining_content = content[end_marker + 3:]
    return remaining_content.lstrip('\n')


def apply_frontmatter(content: str, frontmatter: Dict[str, Any]) -> str:
    """
    Apply frontmatter to document content.
    
    Replaces existing frontmatter if present, or adds new frontmatter
    to the beginning of the document.
    
    Args:
        content: Document content
        frontmatter: Dictionary of frontmatter data
        
    Returns:
        Content with frontmatter applied
    """
    # Convert frontmatter to YAML
    frontmatter_yaml = yaml.dump(frontmatter, default_flow_style=False, sort_keys=False)
    
    # Remove existing frontmatter if present
    content_without_frontmatter = remove_frontmatter(content)
    
    # Add new frontmatter
    return f"---\n{frontmatter_yaml}---\n\n{content_without_frontmatter.lstrip()}"
```
  ```
  --- END OF FILE scribe/utils\frontmatter_parser.py ---
  --- START OF FILE scribe/validation\llm_shacl_validator.py ---
  ```
  ```py
#!/usr/bin/env python3
"""
LLM SHACL Validator

Implementation of Phase 3: Step 3.1.3 - SHACL Validation Loop Implementation
From: ultimate-frontmatter-enhancement-guideline-20250617-0312.md

This module provides validation loop with retry mechanisms and deterministic fallback
to ensure 100% success rate for frontmatter generation.
"""

import yaml
import logging
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime
import sys
import os

# Add tools path for importing existing validators
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

# Handle GraphValidator import with proper fallback
GraphValidator = None
try:
    from validators.graph_validator import GraphValidator as ActualGraphValidator
    GraphValidator = ActualGraphValidator
except ImportError:
    # Fallback if graph_validator not available
    class MockGraphValidator:
        def validate_frontmatter_shacl(self, frontmatter_dict, info_type):
            class MockResult:
                def __init__(self):
                    self.conforms = True
                    self.violations = []
            return MockResult()
    GraphValidator = MockGraphValidator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMSHACLValidator:
    """
    LLM SHACL Validator with validation loop and 100% success guarantee.
    
    Provides comprehensive validation with retry mechanisms and deterministic
    fallback to ensure no scenario where automation fails.
    """
    
    def __init__(self, shacl_shapes_path='standards/registry/shacl-shapes.ttl'):
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.shacl_shapes_path = shacl_shapes_path
        
        # Initialize validator with fallback handling
        try:
            self.shacl_validator = GraphValidator()
        except Exception as e:
            self.logger.warning(f"Could not initialize GraphValidator: {e}")
            self.shacl_validator = None
        
        # Initialize mock LLM client (replace with actual implementation)
        self.llm_client = self._initialize_mock_llm_client()
        
        # Field defaults for deterministic fallback
        self.field_defaults = self._initialize_field_defaults()
        
        # Validation patterns for common fields
        self.validation_patterns = self._initialize_validation_patterns()
    
    def validate_with_retry_loop(self, initial_prompt: str, info_type: str, 
                               max_attempts: int = 5) -> Dict[str, Any]:
        """
        Step 3: Implement validation loop with 100% success guarantee.
        
        Args:
            initial_prompt: The initial LLM prompt for frontmatter generation
            info_type: Target info-type for the document
            max_attempts: Maximum retry attempts before fallback
            
        Returns:
            Result dictionary with success status and generated frontmatter
        """
        self.logger.info(f"Starting validation loop for info-type: {info_type}")
        
        current_prompt = initial_prompt
        validation_errors = []
        
        for attempt in range(max_attempts):
            self.logger.info(f"Validation attempt {attempt + 1}/{max_attempts}")
            
            try:
                # Sub-step 3.1: Generate frontmatter
                generated_frontmatter = self._generate_frontmatter_with_llm(current_prompt)
                
                # Sub-step 3.2: Parse YAML
                try:
                    frontmatter_dict = yaml.safe_load(generated_frontmatter)
                    if not isinstance(frontmatter_dict, dict):
                        raise yaml.YAMLError("Generated content is not a valid YAML dictionary")
                        
                except yaml.YAMLError as e:
                    self.logger.warning(f"YAML parsing error on attempt {attempt + 1}: {e}")
                    current_prompt = self._add_yaml_error_feedback(current_prompt, str(e))
                    continue
                
                # Sub-step 3.3: Validate against SHACL
                validation_result = self._validate_against_shacl(frontmatter_dict, info_type)
                
                # Sub-step 3.4: Check for success
                if validation_result['conforms']:
                    self.logger.info(f"Validation successful on attempt {attempt + 1}")
                    return {
                        'success': True,
                        'frontmatter': frontmatter_dict,
                        'attempts_used': attempt + 1,
                        'validation_method': 'llm_generation'
                    }
                
                # Sub-step 3.5: Add validation errors to prompt for retry
                validation_errors = validation_result['violations']
                current_prompt = self._add_validation_feedback(current_prompt, validation_errors)
                
            except Exception as e:
                self.logger.warning(f"Unexpected error on attempt {attempt + 1}: {e}")
                current_prompt = self._add_error_feedback(current_prompt, str(e))
        
        # If all attempts failed, use deterministic fallback
        self.logger.warning(f"All {max_attempts} attempts failed, using deterministic fallback")
        return self._generate_deterministic_fallback(info_type)
    
    def _generate_frontmatter_with_llm(self, prompt: str) -> str:
        """Generate frontmatter using LLM client."""
        try:
            # Mock LLM generation - replace with actual LLM client
            return self.llm_client.generate(prompt)
        except Exception as e:
            self.logger.error(f"LLM generation failed: {e}")
            raise
    
    def _validate_against_shacl(self, frontmatter_dict: Dict[str, Any], 
                               info_type: str) -> Dict[str, Any]:
        """Sub-step 3.3: Validate frontmatter against SHACL constraints."""
        if self.shacl_validator:
            try:
                # Use existing graph validator
                validation_result = self.shacl_validator.validate_frontmatter_shacl(
                    frontmatter_dict, info_type
                )
                
                return {
                    'conforms': validation_result.conforms,
                    'violations': self._parse_violations(validation_result.violations) if hasattr(validation_result, 'violations') else []
                }
                
            except Exception as e:
                self.logger.warning(f"SHACL validation error: {e}")
                # Fallback to basic validation
                return self._basic_validation_fallback(frontmatter_dict, info_type)
        else:
            # Use basic validation when SHACL validator not available
            return self._basic_validation_fallback(frontmatter_dict, info_type)
    
    def _basic_validation_fallback(self, frontmatter_dict: Dict[str, Any], 
                                 info_type: str) -> Dict[str, Any]:
        """Basic validation when SHACL validator is not available."""
        violations = []
        
        # Check mandatory fields based on info-type
        mandatory_fields = self._get_mandatory_fields_for_type(info_type)
        for field in mandatory_fields:
            if field not in frontmatter_dict:
                violations.append({
                    'type': 'missing_required_field',
                    'field': field,
                    'message': f"Field '{field}' is required for {info_type}"
                })
        
        # Check forbidden fields based on info-type
        forbidden_fields = self._get_forbidden_fields_for_type(info_type)
        for field in forbidden_fields:
            if field in frontmatter_dict:
                violations.append({
                    'type': 'forbidden_field_present',
                    'field': field,
                    'message': f"Field '{field}' is not allowed for {info_type}"
                })
        
        return {
            'conforms': len(violations) == 0,
            'violations': violations
        }
    
    def _parse_violations(self, violations: List[Any]) -> List[Dict[str, Any]]:
        """Parse SHACL violations into standardized format."""
        parsed_violations = []
        
        for violation in violations:
            # Convert SHACL violation object to dictionary
            violation_dict = {
                'type': 'shacl_violation',
                'field': str(getattr(violation, 'focusNode', 'unknown')),
                'message': str(getattr(violation, 'message', 'SHACL constraint violation'))
            }
            parsed_violations.append(violation_dict)
        
        return parsed_violations
    
    def _add_yaml_error_feedback(self, original_prompt: str, yaml_error: str) -> str:
        """Add YAML parsing error feedback to prompt."""
        feedback = f"""
{original_prompt}

âŒ YAML PARSING ERROR DETECTED:
{yaml_error}

CORRECTION REQUIRED:
- Ensure valid YAML syntax with proper indentation
- Use proper quoting for string values
- Check for missing colons, commas, or brackets
- Generate ONLY valid YAML frontmatter block

Please regenerate with correct YAML syntax:
"""
        return feedback
    
    def _add_validation_feedback(self, original_prompt: str, 
                               validation_errors: List[Dict[str, Any]]) -> str:
        """Sub-step 3.5: Add validation errors to prompt for retry."""
        feedback_sections = [
            original_prompt,
            "",
            "âŒ VALIDATION ERRORS DETECTED - Please correct:",
        ]
        
        for error in validation_errors:
            error_type = error.get('type', 'unknown')
            field = error.get('field', 'unknown')
            message = error.get('message', 'Unknown error')
            
            if error_type == 'missing_required_field':
                feedback_sections.append(f"â€¢ MISSING REQUIRED: {field} - {message}")
            elif error_type == 'forbidden_field_present':
                feedback_sections.append(f"â€¢ FORBIDDEN FIELD: {field} - {message}")
            elif error_type == 'incorrect_value':
                expected = error.get('expected', 'N/A')
                actual = error.get('actual', 'N/A')
                feedback_sections.append(f"â€¢ WRONG VALUE: {field} - Expected '{expected}', got '{actual}'")
            else:
                feedback_sections.append(f"â€¢ ERROR: {field} - {message}")
        
        feedback_sections.extend([
            "",
            "Please generate corrected YAML frontmatter that addresses ALL the above errors:",
        ])
        
        return "\n".join(feedback_sections)
    
    def _add_error_feedback(self, original_prompt: str, error_message: str) -> str:
        """Add general error feedback to prompt."""
        feedback = f"""
{original_prompt}

âŒ GENERATION ERROR DETECTED:
{error_message}

Please try again with a corrected approach:
"""
        return feedback
    
    def _generate_deterministic_fallback(self, info_type: str) -> Dict[str, Any]:
        """100% success guarantee - deterministic generation if LLM fails."""
        self.logger.info(f"Generating deterministic fallback for {info_type}")
        
        # Generate minimal valid frontmatter based on SHACL constraints
        mandatory_fields = self._get_mandatory_fields_for_type(info_type)
        fallback_frontmatter = {}
        
        # Add mandatory fields with safe default values
        for field in mandatory_fields:
            fallback_frontmatter[field] = self._get_safe_default_value(field, info_type)
        
        # Ensure info-type is always set
        fallback_frontmatter['info-type'] = info_type
        
        # Add generated timestamp for traceability
        timestamp = datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')
        fallback_frontmatter['date-created'] = timestamp
        fallback_frontmatter['date-modified'] = timestamp
        
        return {
            'success': True,
            'frontmatter': fallback_frontmatter,
            'fallback_used': True,
            'validation_method': 'deterministic_fallback'
        }
    
    def _get_mandatory_fields_for_type(self, info_type: str) -> List[str]:
        """Get mandatory fields for a specific info-type."""
        mandatory_fields_map = {
            'general-document': ['title', 'info-type', 'kb-id'],
            'standard-definition': ['title', 'info-type', 'standard_id', 'version', 'kb-id'],
            'policy-document': ['title', 'info-type', 'standard_id', 'version', 'kb-id'],
            'technical-report': ['title', 'info-type', 'version', 'kb-id'],
            'meeting-notes': ['title', 'info-type', 'kb-id'],
        }
        
        return mandatory_fields_map.get(info_type, ['title', 'info-type', 'kb-id'])
    
    def _get_forbidden_fields_for_type(self, info_type: str) -> List[str]:
        """Get forbidden fields for a specific info-type."""
        forbidden_fields_map = {
            'general-document': ['standard_id'],
            'technical-report': ['standard_id'],
            'meeting-notes': ['standard_id', 'version'],
        }
        
        return forbidden_fields_map.get(info_type, [])
    
    def _get_safe_default_value(self, field: str, info_type: str) -> str:
        """Get safe default value for a field."""
        timestamp = datetime.now().strftime('%Y%m%d-%H%M')
        
        field_defaults = {
            'title': f'Auto-Generated {info_type.title()} Title',
            'version': '1.0.0',
            'date-created': datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'),
            'date-modified': datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'),
            'kb-id': f'AUTO-{info_type.upper().replace("-", "")}-{timestamp}',
            'standard_id': f'ST-AUTO-{timestamp}',
            'info-type': info_type,
            'criticality': 'P2-Standard',
            'lifecycle_gatekeeper': 'Technical-Review',
            'tags': ['auto-generated'],
            'scope_application': 'repository',
            'primary_topic': f'{info_type}-content'
        }
        
        return field_defaults.get(field, f'auto-generated-{field}')
    
    def _initialize_mock_llm_client(self):
        """Initialize mock LLM client for testing."""
        class MockLLMClient:
            def generate(self, prompt: str) -> str:
                # Simple mock generation - replace with actual LLM client
                if 'standard-definition' in prompt:
                    return '''---
title: "Mock Standard Document"
info-type: standard-definition
standard_id: ST-MOCK-001
version: 1.0.0
kb-id: MOCK-STANDARD-20250617-1200
---'''
                else:
                    return '''---
title: "Mock General Document"
info-type: general-document
kb-id: MOCK-GENERAL-20250617-1200
---'''
        
        return MockLLMClient()
    
    def _initialize_field_defaults(self) -> Dict[str, Any]:
        """Initialize field default values."""
        return {
            'title': 'Auto-Generated Document Title',
            'version': '1.0.0',
            'kb-id': 'AUTO-GENERATED-KB-ID',
            'info-type': 'general-document',
            'criticality': 'P2-Standard',
            'lifecycle_gatekeeper': 'Technical-Review'
        }
    
    def _initialize_validation_patterns(self) -> Dict[str, str]:
        """Initialize validation patterns for common fields."""
        return {
            'iso_datetime': r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z$',
            'semantic_version': r'^\d+\.\d+\.\d+$',
            'standard_id': r'^[A-Z]{2}-[A-Z]{2,6}-[A-Z0-9\-]+$',
            'kb_id': r'^[A-Z0-9\-]+$',
            'criticality': r'^P[0-4]-[A-Za-z\-]+$'
        }
    
    def get_validation_statistics(self, validation_history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Get statistics about validation performance."""
        if not validation_history:
            return {'total_validations': 0}
        
        total_validations = len(validation_history)
        successful_validations = sum(1 for v in validation_history if v.get('success', False))
        fallback_used = sum(1 for v in validation_history if v.get('fallback_used', False))
        avg_attempts = sum(v.get('attempts_used', 1) for v in validation_history) / total_validations
        
        return {
            'total_validations': total_validations,
            'success_rate': successful_validations / total_validations * 100,
            'fallback_rate': fallback_used / total_validations * 100,
            'average_attempts': avg_attempts,
            'max_attempts_needed': max(v.get('attempts_used', 1) for v in validation_history)
        }


if __name__ == "__main__":
    # Example usage for testing
    validator = LLMSHACLValidator()
    
    # Test prompt
    test_prompt = """
CRITICAL: Generate YAML frontmatter for info-type: standard-definition

MANDATORY FIELDS:
- title: Document title
- info-type: Must be 'standard-definition'
- standard_id: Must match pattern ST-*
- version: Semantic version

Generate ONLY the YAML frontmatter block:
"""
    
    # Test validation loop
    result = validator.validate_with_retry_loop(test_prompt, 'standard-definition')
    
    print("Validation Result:")
    print("=" * 50)
    print(f"Success: {result['success']}")
    print(f"Attempts: {result.get('attempts_used', 'N/A')}")
    print(f"Fallback Used: {result.get('fallback_used', False)}")
    print(f"Method: {result.get('validation_method', 'unknown')}")
    print("\nGenerated Frontmatter:")
    print(yaml.dump(result['frontmatter'], default_flow_style=False))
```
  ```
  --- END OF FILE scribe/validation\llm_shacl_validator.py ---
  --- START OF FILE scribe/watcher.py ---
  ```
  ```py
"""
Scribe Engine - File System Watcher (Producer Thread)

This module implements the L1 Driving Adapter in the HMA architecture.
It monitors file system events and feeds them into the event processing pipeline.
"""

import threading
import queue
import time
import uuid
import asyncio
from pathlib import Path
from typing import List, Optional
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler, FileSystemEvent
import structlog
from tools.scribe.core.logging_config import get_scribe_logger
from tools.scribe.core.ports import IEventSource
from tools.scribe.core.hma_telemetry import HMATelemetry
from tools.scribe.core.boundary_validator import BoundaryValidator

logger = get_scribe_logger(__name__)


class ScribeEventHandler(FileSystemEventHandler):
    """Custom event handler that filters and queues relevant file system events."""
    
    def __init__(self, event_bus_port, file_patterns: Optional[List[str]] = None, 
                 telemetry: Optional[HMATelemetry] = None, 
                 boundary_validator: Optional[BoundaryValidator] = None):
        """
        Initialize the event handler with HMA v2.2 compliance.
        
        Args:
            event_bus_port: EventBusPort to publish events to
            file_patterns: List of file patterns to monitor (e.g., ['*.md', '*.txt'])
            telemetry: HMA telemetry for boundary operations
            boundary_validator: Boundary validator for L1 input validation
        """
        super().__init__()
        self.event_bus_port = event_bus_port
        self.file_patterns = file_patterns or ['*.md']  # Default to markdown files
        self.telemetry = telemetry
        self.boundary_validator = boundary_validator
        
        # Log HMA v2.2 compliant initialization
        logger.info("File system watcher initialized with HMA v2.2 compliance",
                   file_patterns=self.file_patterns,
                   telemetry_enabled=telemetry is not None,
                   boundary_validation_enabled=boundary_validator is not None)
        
    def on_modified(self, event: FileSystemEvent) -> None:
        """Handle file modification events with HMA v2.2 boundary telemetry."""
        # HMA v2.2 mandatory OTEL boundary telemetry
        if self.telemetry:
            with self.telemetry.trace_boundary_operation(
                "file_system_event", "l1_driving_adapter", "file_system", "scribe_core"
            ) as span:
                if hasattr(span, 'set_attribute'):
                    span.set_attribute("hma.boundary.type", "l1_file_system_input")
                    span.set_attribute("hma.operation", "file_modified")
                    span.set_attribute("hma.file.path", event.src_path)
                
                if not event.is_directory and self._should_process_file(event.src_path):
                    self._publish_event('modified', event.src_path)
        else:
            if not event.is_directory and self._should_process_file(event.src_path):
                self._publish_event('modified', event.src_path)
    
    def on_created(self, event: FileSystemEvent) -> None:
        """Handle file creation events with HMA v2.2 boundary telemetry."""
        # HMA v2.2 mandatory OTEL boundary telemetry
        if self.telemetry:
            with self.telemetry.trace_boundary_operation(
                "file_system_event", "l1_driving_adapter", "file_system", "scribe_core"
            ) as span:
                if hasattr(span, 'set_attribute'):
                    span.set_attribute("hma.boundary.type", "l1_file_system_input")
                    span.set_attribute("hma.operation", "file_created")
                    span.set_attribute("hma.file.path", event.src_path)
                
                if not event.is_directory and self._should_process_file(event.src_path):
                    self._publish_event('created', event.src_path)
        else:
            if not event.is_directory and self._should_process_file(event.src_path):
                self._publish_event('created', event.src_path)
    
    def on_moved(self, event: FileSystemEvent) -> None:
        """Handle file move/rename events."""
        if not event.is_directory and self._should_process_file(event.dest_path):
            self._publish_event('moved', event.dest_path, event.src_path)
    
    def _should_process_file(self, file_path: str) -> bool:
        """Check if file matches our monitoring patterns."""
        path = Path(file_path)
        return any(path.match(pattern) for pattern in self.file_patterns)
    
    def _publish_event(self, event_type: str, file_path: str, old_path: Optional[str] = None) -> None:
        """Publish a processed event through the EventBusPort with HMA v2.2 boundary validation."""
        # Generate unique event_id for traceability
        event_id = str(uuid.uuid4())
        
        event_data = {
            'event_id': event_id,
            'type': event_type,
            'file_path': file_path,
            'old_path': old_path,
            'timestamp': time.time()
        }
        
        # HMA v2.2 mandatory boundary validation
        if self.boundary_validator:
            try:
                validation_result = self.boundary_validator.validate_input(
                    event_data, "l1_file_system_input"
                )
                if not validation_result.valid:
                    logger.error("Boundary validation failed for file system event",
                               event_id=event_id,
                               file_path=file_path,
                               validation_errors=validation_result.errors)
                    return
                    
            except Exception as e:
                logger.error("Boundary validation error",
                           event_id=event_id,
                           file_path=file_path,
                           error=str(e))
                return
        
        # Use EventBusPort interface through async wrapper
        try:
            # Create an event loop if one doesn't exist
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            # Publish via EventBusPort
            loop.run_until_complete(
                self.event_bus_port.publish_event('file_event', event_data, correlation_id=event_id)
            )
            
            logger.debug("Published event", 
                        event_id=event_id,
                        event_type=event_type, 
                        file_path=file_path)
        except Exception as e:
            logger.error("Failed to publish event", 
                        event_id=event_id,
                        error=str(e))


class Watcher(threading.Thread, IEventSource):
    """
    File system watcher thread (Producer in producer-consumer pattern).
    
    This is the L1 Driving Adapter that observes file system events
    and feeds them into the Scribe processing pipeline.
    """
    
    def __init__(self, 
                 watch_paths: List[str],
                 file_patterns: Optional[List[str]] = None,
                 event_bus_port = None,
                 shutdown_event: threading.Event = None,
                 debounce_seconds: float = 0.5):
        """
        Initialize the watcher thread.
        
        Args:
            watch_paths: List of directory paths to monitor
            file_patterns: List of file patterns to monitor
            event_bus_port: EventBusPort to publish events to
            shutdown_event: Event to signal graceful shutdown
            debounce_seconds: Debounce delay for file events
        """
        super().__init__(name="ScribeWatcher", daemon=True)
            
        self.watch_paths = watch_paths
        self.file_patterns = file_patterns or ['*.md']
        self.event_bus_port = event_bus_port
        self.shutdown_event = shutdown_event or threading.Event()
        self.debounce_seconds = debounce_seconds
        
        # Initialize observer
        self.observer = Observer()
        
        # Create event handler
        self.event_handler = ScribeEventHandler(
            event_bus_port=self.event_bus_port, 
            file_patterns=self.file_patterns
        )
        
        logger.info("Watcher initialized", 
                   watch_paths=watch_paths, 
                   file_patterns=file_patterns)
    
    def start(self):
        """Start the watcher (start the thread and setup observers)"""
        # Start the thread
        super().start()
        
        # Setup file system monitoring
        try:
            for watch_path in self.watch_paths:
                path = Path(watch_path)
                if path.exists():
                    self.observer.schedule(self.event_handler, str(path), recursive=True)
                    logger.info("Watching path", path=str(path))
                else:
                    logger.warning("Watch path does not exist", path=str(path))
            
            # Start the observer
            self.observer.start()
            logger.info("File system watcher started")
        except Exception as e:
            logger.error("Failed to start watcher", error=str(e))
    
    def run(self) -> None:
        """Main thread execution - wait for shutdown signal"""
        try:
            # Main loop - check for shutdown signal
            while not self.shutdown_event.is_set():
                time.sleep(0.1)  # Check shutdown every 100ms
            
            logger.info("Shutdown signal received, stopping watcher")
            
        except Exception as e:
            logger.error("Watcher thread error", error=str(e), exc_info=True)
        finally:
            self.stop()
    
    def stop(self) -> None:
        """Stop the watcher and clean up resources"""
        try:
            # Stop the observer if it's running
            if hasattr(self, 'observer') and self.observer:
                if self.observer.is_alive():
                    self.observer.stop()
                    self.observer.join(timeout=5.0)  # Wait up to 5 seconds
                    
                if self.observer.is_alive():
                    logger.warning("Observer did not stop gracefully")
                else:
                    logger.info("File system watcher stopped cleanly")
            
            # Set shutdown event
            if self.shutdown_event:
                self.shutdown_event.set()
                
        except Exception as e:
            logger.error("Error during watcher cleanup", error=str(e)) 
```
  ```
  --- END OF FILE scribe/watcher.py ---

## 2. The Mission: Bug Analysis & Remediation Guide
---
Your task is twofold:
1.  **Analyze:** Systematically identify potential bugs within the provided code.
2.  **Instruct:** Create a clear, actionable, step-by-step guide that another developer can follow to understand, reproduce, and fix each identified bug.

## 3. Known Bug Categories (Areas of Suspicion)
---
Pay special attention to the following common sources of bugs. I suspect the issues may be related to:
- Incorrect handling of edge cases (e.g., empty arrays, null inputs, zero values).
- Off-by-one errors in loops or array indexing.
- Unexpected data types or type coercion issues.
- Uncaught or improperly handled exceptions.
- Concurrency problems (e.g., race conditions, deadlocks).
- Improper or missing configuration settings.

## 4. Your Analysis Process (Systematic Approach)
---
Please follow this structured process for your analysis:
1.  **Review Systematically:** Read the code to understand its purpose, control flow, and data structures.
2.  **Trace Code Paths:** Mentally trace the execution flow for critical functions, paying close attention to the suspicious areas listed above.
3.  **Consider Boundaries:** Analyze how the code handles boundary conditions, invalid inputs, and potential error states.
4.  **Identify Anti-Patterns:** Look for common coding mistakes or anti-patterns that often lead to bugs.
5.  **Assess Impact:** For each potential bug, determine its severity and the potential impact on the application's functionality, security, or performance.

## 5. Required Output Format: The Step-by-Step Remediation Guide
---
Please structure your entire response as a single, comprehensive "Bug Remediation Guide" in Markdown. The guide must contain the following sections:

### Bug Remediation Guide

#### Executive Summary
Provide a high-level overview of the findings in a Markdown table. This allows for quick prioritization.

| Bug ID | Severity | Title | Location |
| :--- | :--- | :--- | :--- |
| BUG-001 | `Critical` | [Short, descriptive title of the bug] | `[file_path]:[line_number]` |
| BUG-002 | `High` | [Short, descriptive title of the bug] | `[file_path]:[line_number]` |
| BUG-003 | `Medium` | [Short, descriptive title of the bug] | `[file_path]:[line_number]` |

---

#### Detailed Bug Reports & Fix Guides
For each bug identified in the summary table, provide a detailed report with the following structure:

**BUG-001: [Bug Title from Summary Table]**
- **Severity:** `Critical` / `High` / `Medium` / `Low`
- **Location:** `[file_path]:[line_number]`
- **Problem Description:** A clear and concise explanation of the bug. What is happening that shouldn't be?
- **Impact Analysis:** What is the consequence of this bug? (e.g., "Causes incorrect financial calculations," "Leads to application crash on invalid input," "Creates a security vulnerability.")
- **How to Reproduce:** Provide a specific example of input or a sequence of actions that will reliably trigger the bug.

- **Step-by-Step Fix Instructions:**
  1.  **Navigate:** Open the file `[file_path]`.
  2.  **Locate:** Go to line `[line_number]`.
  3.  **Action:** Describe the specific code change required. Be explicit. (e.g., "Change the loop condition from `i <= items.length` to `i < items.length`.")
  4.  **Reasoning:** Briefly explain *why* this change fixes the bug. (e.g., "This corrects the off-by-one error and prevents an out-of-bounds array access.")

- **Suggested Regression Test:**
  - Describe a new unit test, integration test, or manual test case that should be created. This test should fail before the fix and pass after the fix, ensuring the bug does not reappear in the future.
  (e.g., "Add a unit test for the `calculateTotal` function that passes an empty array and asserts that the result is 0.")

---
**(Repeat the detailed report structure for BUG-002, BUG-003, etc.)**